{
  "best_global_step": 12219,
  "best_metric": 61.22141012913486,
  "best_model_checkpoint": "./results/checkpoint-12219",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 12219,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0024551927326295114,
      "grad_norm": 19.97618293762207,
      "learning_rate": 9e-07,
      "loss": 1.5397,
      "step": 10
    },
    {
      "epoch": 0.004910385465259023,
      "grad_norm": 22.24783706665039,
      "learning_rate": 1.9e-06,
      "loss": 1.5983,
      "step": 20
    },
    {
      "epoch": 0.007365578197888534,
      "grad_norm": 18.670942306518555,
      "learning_rate": 2.9e-06,
      "loss": 1.5126,
      "step": 30
    },
    {
      "epoch": 0.009820770930518046,
      "grad_norm": 17.060291290283203,
      "learning_rate": 3.9e-06,
      "loss": 1.6249,
      "step": 40
    },
    {
      "epoch": 0.012275963663147557,
      "grad_norm": 16.3482608795166,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 1.5013,
      "step": 50
    },
    {
      "epoch": 0.014731156395777068,
      "grad_norm": 16.30838966369629,
      "learning_rate": 5.9e-06,
      "loss": 1.5803,
      "step": 60
    },
    {
      "epoch": 0.01718634912840658,
      "grad_norm": 20.236494064331055,
      "learning_rate": 6.800000000000001e-06,
      "loss": 1.6997,
      "step": 70
    },
    {
      "epoch": 0.01964154186103609,
      "grad_norm": 18.321622848510742,
      "learning_rate": 7.8e-06,
      "loss": 1.6351,
      "step": 80
    },
    {
      "epoch": 0.022096734593665603,
      "grad_norm": 18.594520568847656,
      "learning_rate": 8.8e-06,
      "loss": 1.6112,
      "step": 90
    },
    {
      "epoch": 0.024551927326295114,
      "grad_norm": 15.502220153808594,
      "learning_rate": 9.800000000000001e-06,
      "loss": 1.4823,
      "step": 100
    },
    {
      "epoch": 0.027007120058924625,
      "grad_norm": 21.465469360351562,
      "learning_rate": 1.08e-05,
      "loss": 1.6024,
      "step": 110
    },
    {
      "epoch": 0.029462312791554136,
      "grad_norm": 17.96316146850586,
      "learning_rate": 1.18e-05,
      "loss": 1.4446,
      "step": 120
    },
    {
      "epoch": 0.03191750552418365,
      "grad_norm": 14.52453327178955,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 1.5857,
      "step": 130
    },
    {
      "epoch": 0.03437269825681316,
      "grad_norm": 18.965478897094727,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 1.6204,
      "step": 140
    },
    {
      "epoch": 0.03682789098944267,
      "grad_norm": 18.24601173400879,
      "learning_rate": 1.48e-05,
      "loss": 1.5163,
      "step": 150
    },
    {
      "epoch": 0.03928308372207218,
      "grad_norm": 16.014202117919922,
      "learning_rate": 1.58e-05,
      "loss": 1.5703,
      "step": 160
    },
    {
      "epoch": 0.04173827645470169,
      "grad_norm": 15.459785461425781,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 1.5491,
      "step": 170
    },
    {
      "epoch": 0.044193469187331205,
      "grad_norm": 16.271894454956055,
      "learning_rate": 1.78e-05,
      "loss": 1.5438,
      "step": 180
    },
    {
      "epoch": 0.04664866191996072,
      "grad_norm": 15.312271118164062,
      "learning_rate": 1.88e-05,
      "loss": 1.5578,
      "step": 190
    },
    {
      "epoch": 0.04910385465259023,
      "grad_norm": 17.250749588012695,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 1.653,
      "step": 200
    },
    {
      "epoch": 0.05155904738521974,
      "grad_norm": 15.060528755187988,
      "learning_rate": 2.08e-05,
      "loss": 1.6081,
      "step": 210
    },
    {
      "epoch": 0.05401424011784925,
      "grad_norm": 19.068511962890625,
      "learning_rate": 2.18e-05,
      "loss": 1.6942,
      "step": 220
    },
    {
      "epoch": 0.056469432850478764,
      "grad_norm": 22.951526641845703,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 1.5721,
      "step": 230
    },
    {
      "epoch": 0.05892462558310827,
      "grad_norm": 16.47252082824707,
      "learning_rate": 2.38e-05,
      "loss": 1.622,
      "step": 240
    },
    {
      "epoch": 0.061379818315737786,
      "grad_norm": 16.932310104370117,
      "learning_rate": 2.48e-05,
      "loss": 1.4895,
      "step": 250
    },
    {
      "epoch": 0.0638350110483673,
      "grad_norm": 19.161991119384766,
      "learning_rate": 2.58e-05,
      "loss": 1.6059,
      "step": 260
    },
    {
      "epoch": 0.06629020378099681,
      "grad_norm": 14.104909896850586,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 1.5888,
      "step": 270
    },
    {
      "epoch": 0.06874539651362632,
      "grad_norm": 19.10868263244629,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 1.7767,
      "step": 280
    },
    {
      "epoch": 0.07120058924625583,
      "grad_norm": 22.373577117919922,
      "learning_rate": 2.88e-05,
      "loss": 1.5333,
      "step": 290
    },
    {
      "epoch": 0.07365578197888534,
      "grad_norm": 21.15407943725586,
      "learning_rate": 2.98e-05,
      "loss": 1.526,
      "step": 300
    },
    {
      "epoch": 0.07611097471151486,
      "grad_norm": 18.374366760253906,
      "learning_rate": 3.08e-05,
      "loss": 1.5858,
      "step": 310
    },
    {
      "epoch": 0.07856616744414437,
      "grad_norm": 17.041961669921875,
      "learning_rate": 3.18e-05,
      "loss": 1.3992,
      "step": 320
    },
    {
      "epoch": 0.08102136017677387,
      "grad_norm": 17.05830192565918,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 1.5407,
      "step": 330
    },
    {
      "epoch": 0.08347655290940338,
      "grad_norm": 19.098648071289062,
      "learning_rate": 3.38e-05,
      "loss": 1.4166,
      "step": 340
    },
    {
      "epoch": 0.0859317456420329,
      "grad_norm": 18.834457397460938,
      "learning_rate": 3.48e-05,
      "loss": 1.5952,
      "step": 350
    },
    {
      "epoch": 0.08838693837466241,
      "grad_norm": 17.97854995727539,
      "learning_rate": 3.58e-05,
      "loss": 1.5954,
      "step": 360
    },
    {
      "epoch": 0.09084213110729192,
      "grad_norm": 18.253549575805664,
      "learning_rate": 3.68e-05,
      "loss": 1.5254,
      "step": 370
    },
    {
      "epoch": 0.09329732383992144,
      "grad_norm": 17.78670883178711,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 1.5778,
      "step": 380
    },
    {
      "epoch": 0.09575251657255095,
      "grad_norm": 17.82236099243164,
      "learning_rate": 3.88e-05,
      "loss": 1.6245,
      "step": 390
    },
    {
      "epoch": 0.09820770930518045,
      "grad_norm": 23.968530654907227,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 1.6303,
      "step": 400
    },
    {
      "epoch": 0.10066290203780996,
      "grad_norm": 22.66817283630371,
      "learning_rate": 4.08e-05,
      "loss": 1.6082,
      "step": 410
    },
    {
      "epoch": 0.10311809477043948,
      "grad_norm": 24.986648559570312,
      "learning_rate": 4.18e-05,
      "loss": 1.4037,
      "step": 420
    },
    {
      "epoch": 0.10557328750306899,
      "grad_norm": 26.86842155456543,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 1.4229,
      "step": 430
    },
    {
      "epoch": 0.1080284802356985,
      "grad_norm": 15.91030502319336,
      "learning_rate": 4.38e-05,
      "loss": 1.4741,
      "step": 440
    },
    {
      "epoch": 0.11048367296832802,
      "grad_norm": 21.408540725708008,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 1.5652,
      "step": 450
    },
    {
      "epoch": 0.11293886570095753,
      "grad_norm": 14.61571979522705,
      "learning_rate": 4.58e-05,
      "loss": 1.5094,
      "step": 460
    },
    {
      "epoch": 0.11539405843358704,
      "grad_norm": 13.186049461364746,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 1.5637,
      "step": 470
    },
    {
      "epoch": 0.11784925116621654,
      "grad_norm": 21.24006462097168,
      "learning_rate": 4.78e-05,
      "loss": 1.6966,
      "step": 480
    },
    {
      "epoch": 0.12030444389884606,
      "grad_norm": 14.689993858337402,
      "learning_rate": 4.88e-05,
      "loss": 1.382,
      "step": 490
    },
    {
      "epoch": 0.12275963663147557,
      "grad_norm": 29.012718200683594,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 1.4643,
      "step": 500
    },
    {
      "epoch": 0.12521482936410508,
      "grad_norm": 15.076292037963867,
      "learning_rate": 4.9965867394828916e-05,
      "loss": 1.4439,
      "step": 510
    },
    {
      "epoch": 0.1276700220967346,
      "grad_norm": 19.176681518554688,
      "learning_rate": 4.992320163836505e-05,
      "loss": 1.3686,
      "step": 520
    },
    {
      "epoch": 0.1301252148293641,
      "grad_norm": 27.367084503173828,
      "learning_rate": 4.988053588190119e-05,
      "loss": 1.4904,
      "step": 530
    },
    {
      "epoch": 0.13258040756199363,
      "grad_norm": 18.14013671875,
      "learning_rate": 4.983787012543732e-05,
      "loss": 1.4832,
      "step": 540
    },
    {
      "epoch": 0.13503560029462314,
      "grad_norm": 17.614826202392578,
      "learning_rate": 4.9795204368973466e-05,
      "loss": 1.636,
      "step": 550
    },
    {
      "epoch": 0.13749079302725264,
      "grad_norm": 17.418216705322266,
      "learning_rate": 4.97525386125096e-05,
      "loss": 1.6464,
      "step": 560
    },
    {
      "epoch": 0.13994598575988215,
      "grad_norm": 14.82652473449707,
      "learning_rate": 4.9709872856045744e-05,
      "loss": 1.4897,
      "step": 570
    },
    {
      "epoch": 0.14240117849251166,
      "grad_norm": 20.7703800201416,
      "learning_rate": 4.966720709958188e-05,
      "loss": 1.6286,
      "step": 580
    },
    {
      "epoch": 0.14485637122514117,
      "grad_norm": 24.016849517822266,
      "learning_rate": 4.9624541343118015e-05,
      "loss": 1.5125,
      "step": 590
    },
    {
      "epoch": 0.14731156395777067,
      "grad_norm": 17.95581817626953,
      "learning_rate": 4.958187558665415e-05,
      "loss": 1.5062,
      "step": 600
    },
    {
      "epoch": 0.1497667566904002,
      "grad_norm": 18.070302963256836,
      "learning_rate": 4.953920983019029e-05,
      "loss": 1.424,
      "step": 610
    },
    {
      "epoch": 0.15222194942302972,
      "grad_norm": 26.80727767944336,
      "learning_rate": 4.949654407372643e-05,
      "loss": 1.7561,
      "step": 620
    },
    {
      "epoch": 0.15467714215565923,
      "grad_norm": 18.551136016845703,
      "learning_rate": 4.945387831726257e-05,
      "loss": 1.4952,
      "step": 630
    },
    {
      "epoch": 0.15713233488828873,
      "grad_norm": 15.354226112365723,
      "learning_rate": 4.94112125607987e-05,
      "loss": 1.5739,
      "step": 640
    },
    {
      "epoch": 0.15958752762091824,
      "grad_norm": 17.41464614868164,
      "learning_rate": 4.936854680433484e-05,
      "loss": 1.6146,
      "step": 650
    },
    {
      "epoch": 0.16204272035354775,
      "grad_norm": 16.297393798828125,
      "learning_rate": 4.932588104787098e-05,
      "loss": 1.4627,
      "step": 660
    },
    {
      "epoch": 0.16449791308617726,
      "grad_norm": 18.345224380493164,
      "learning_rate": 4.928321529140712e-05,
      "loss": 1.5333,
      "step": 670
    },
    {
      "epoch": 0.16695310581880676,
      "grad_norm": 20.849361419677734,
      "learning_rate": 4.9240549534943256e-05,
      "loss": 1.4853,
      "step": 680
    },
    {
      "epoch": 0.1694082985514363,
      "grad_norm": 18.089143753051758,
      "learning_rate": 4.919788377847939e-05,
      "loss": 1.5358,
      "step": 690
    },
    {
      "epoch": 0.1718634912840658,
      "grad_norm": 26.998701095581055,
      "learning_rate": 4.9155218022015534e-05,
      "loss": 1.6171,
      "step": 700
    },
    {
      "epoch": 0.1743186840166953,
      "grad_norm": 16.907474517822266,
      "learning_rate": 4.911255226555167e-05,
      "loss": 1.5577,
      "step": 710
    },
    {
      "epoch": 0.17677387674932482,
      "grad_norm": 13.285305976867676,
      "learning_rate": 4.906988650908781e-05,
      "loss": 1.6319,
      "step": 720
    },
    {
      "epoch": 0.17922906948195433,
      "grad_norm": 13.451045989990234,
      "learning_rate": 4.902722075262395e-05,
      "loss": 1.5584,
      "step": 730
    },
    {
      "epoch": 0.18168426221458384,
      "grad_norm": 20.880834579467773,
      "learning_rate": 4.898455499616009e-05,
      "loss": 1.5637,
      "step": 740
    },
    {
      "epoch": 0.18413945494721334,
      "grad_norm": 16.639873504638672,
      "learning_rate": 4.894188923969622e-05,
      "loss": 1.3921,
      "step": 750
    },
    {
      "epoch": 0.18659464767984288,
      "grad_norm": 20.29060173034668,
      "learning_rate": 4.889922348323236e-05,
      "loss": 1.6367,
      "step": 760
    },
    {
      "epoch": 0.1890498404124724,
      "grad_norm": 16.301490783691406,
      "learning_rate": 4.88565577267685e-05,
      "loss": 1.5556,
      "step": 770
    },
    {
      "epoch": 0.1915050331451019,
      "grad_norm": 16.70210838317871,
      "learning_rate": 4.881389197030464e-05,
      "loss": 1.5784,
      "step": 780
    },
    {
      "epoch": 0.1939602258777314,
      "grad_norm": 17.88554573059082,
      "learning_rate": 4.8771226213840775e-05,
      "loss": 1.6242,
      "step": 790
    },
    {
      "epoch": 0.1964154186103609,
      "grad_norm": 15.364139556884766,
      "learning_rate": 4.872856045737691e-05,
      "loss": 1.4812,
      "step": 800
    },
    {
      "epoch": 0.19887061134299042,
      "grad_norm": 20.654830932617188,
      "learning_rate": 4.8685894700913046e-05,
      "loss": 1.7278,
      "step": 810
    },
    {
      "epoch": 0.20132580407561992,
      "grad_norm": 11.303154945373535,
      "learning_rate": 4.864322894444919e-05,
      "loss": 1.5462,
      "step": 820
    },
    {
      "epoch": 0.20378099680824946,
      "grad_norm": 17.06078338623047,
      "learning_rate": 4.8600563187985324e-05,
      "loss": 1.465,
      "step": 830
    },
    {
      "epoch": 0.20623618954087897,
      "grad_norm": 19.951017379760742,
      "learning_rate": 4.855789743152147e-05,
      "loss": 1.4621,
      "step": 840
    },
    {
      "epoch": 0.20869138227350847,
      "grad_norm": 30.52486801147461,
      "learning_rate": 4.8515231675057596e-05,
      "loss": 1.6147,
      "step": 850
    },
    {
      "epoch": 0.21114657500613798,
      "grad_norm": 16.59217643737793,
      "learning_rate": 4.847256591859374e-05,
      "loss": 1.3452,
      "step": 860
    },
    {
      "epoch": 0.2136017677387675,
      "grad_norm": 22.854339599609375,
      "learning_rate": 4.8429900162129874e-05,
      "loss": 1.4688,
      "step": 870
    },
    {
      "epoch": 0.216056960471397,
      "grad_norm": 17.209896087646484,
      "learning_rate": 4.8387234405666016e-05,
      "loss": 1.5804,
      "step": 880
    },
    {
      "epoch": 0.2185121532040265,
      "grad_norm": 11.832113265991211,
      "learning_rate": 4.834456864920215e-05,
      "loss": 1.4142,
      "step": 890
    },
    {
      "epoch": 0.22096734593665604,
      "grad_norm": 18.54062271118164,
      "learning_rate": 4.8301902892738294e-05,
      "loss": 1.4661,
      "step": 900
    },
    {
      "epoch": 0.22342253866928555,
      "grad_norm": 21.487491607666016,
      "learning_rate": 4.825923713627443e-05,
      "loss": 1.5484,
      "step": 910
    },
    {
      "epoch": 0.22587773140191506,
      "grad_norm": 14.700235366821289,
      "learning_rate": 4.8216571379810565e-05,
      "loss": 1.395,
      "step": 920
    },
    {
      "epoch": 0.22833292413454456,
      "grad_norm": 17.82758331298828,
      "learning_rate": 4.817390562334671e-05,
      "loss": 1.3271,
      "step": 930
    },
    {
      "epoch": 0.23078811686717407,
      "grad_norm": 13.758143424987793,
      "learning_rate": 4.8131239866882843e-05,
      "loss": 1.5899,
      "step": 940
    },
    {
      "epoch": 0.23324330959980358,
      "grad_norm": 14.82072639465332,
      "learning_rate": 4.808857411041898e-05,
      "loss": 1.5571,
      "step": 950
    },
    {
      "epoch": 0.23569850233243309,
      "grad_norm": 18.233890533447266,
      "learning_rate": 4.8045908353955115e-05,
      "loss": 1.3932,
      "step": 960
    },
    {
      "epoch": 0.23815369506506262,
      "grad_norm": 20.85445785522461,
      "learning_rate": 4.800324259749126e-05,
      "loss": 1.6689,
      "step": 970
    },
    {
      "epoch": 0.24060888779769213,
      "grad_norm": 13.05193042755127,
      "learning_rate": 4.796057684102739e-05,
      "loss": 1.5671,
      "step": 980
    },
    {
      "epoch": 0.24306408053032164,
      "grad_norm": 19.52005958557129,
      "learning_rate": 4.7917911084563535e-05,
      "loss": 1.4293,
      "step": 990
    },
    {
      "epoch": 0.24551927326295114,
      "grad_norm": 17.843761444091797,
      "learning_rate": 4.787524532809967e-05,
      "loss": 1.4174,
      "step": 1000
    },
    {
      "epoch": 0.24797446599558065,
      "grad_norm": 21.01186752319336,
      "learning_rate": 4.7832579571635806e-05,
      "loss": 1.5459,
      "step": 1010
    },
    {
      "epoch": 0.25042965872821016,
      "grad_norm": 17.235424041748047,
      "learning_rate": 4.778991381517194e-05,
      "loss": 1.2932,
      "step": 1020
    },
    {
      "epoch": 0.25288485146083967,
      "grad_norm": 20.89609718322754,
      "learning_rate": 4.7747248058708084e-05,
      "loss": 1.523,
      "step": 1030
    },
    {
      "epoch": 0.2553400441934692,
      "grad_norm": 18.30866050720215,
      "learning_rate": 4.770458230224422e-05,
      "loss": 1.5461,
      "step": 1040
    },
    {
      "epoch": 0.2577952369260987,
      "grad_norm": 20.318571090698242,
      "learning_rate": 4.766191654578036e-05,
      "loss": 1.5523,
      "step": 1050
    },
    {
      "epoch": 0.2602504296587282,
      "grad_norm": 17.151338577270508,
      "learning_rate": 4.76192507893165e-05,
      "loss": 1.6153,
      "step": 1060
    },
    {
      "epoch": 0.2627056223913577,
      "grad_norm": 20.824016571044922,
      "learning_rate": 4.7576585032852634e-05,
      "loss": 1.3819,
      "step": 1070
    },
    {
      "epoch": 0.26516081512398726,
      "grad_norm": 21.996286392211914,
      "learning_rate": 4.753391927638877e-05,
      "loss": 1.4721,
      "step": 1080
    },
    {
      "epoch": 0.26761600785661677,
      "grad_norm": 15.923877716064453,
      "learning_rate": 4.749125351992491e-05,
      "loss": 1.6025,
      "step": 1090
    },
    {
      "epoch": 0.2700712005892463,
      "grad_norm": 20.72901153564453,
      "learning_rate": 4.744858776346105e-05,
      "loss": 1.5057,
      "step": 1100
    },
    {
      "epoch": 0.2725263933218758,
      "grad_norm": 15.019124031066895,
      "learning_rate": 4.740592200699719e-05,
      "loss": 1.5866,
      "step": 1110
    },
    {
      "epoch": 0.2749815860545053,
      "grad_norm": 16.025596618652344,
      "learning_rate": 4.7363256250533325e-05,
      "loss": 1.5558,
      "step": 1120
    },
    {
      "epoch": 0.2774367787871348,
      "grad_norm": 17.33819007873535,
      "learning_rate": 4.732059049406946e-05,
      "loss": 1.6158,
      "step": 1130
    },
    {
      "epoch": 0.2798919715197643,
      "grad_norm": 15.18689250946045,
      "learning_rate": 4.72779247376056e-05,
      "loss": 1.4941,
      "step": 1140
    },
    {
      "epoch": 0.2823471642523938,
      "grad_norm": 13.17165470123291,
      "learning_rate": 4.723525898114174e-05,
      "loss": 1.6795,
      "step": 1150
    },
    {
      "epoch": 0.2848023569850233,
      "grad_norm": 15.571815490722656,
      "learning_rate": 4.7192593224677875e-05,
      "loss": 1.4235,
      "step": 1160
    },
    {
      "epoch": 0.2872575497176528,
      "grad_norm": 22.943025588989258,
      "learning_rate": 4.714992746821401e-05,
      "loss": 1.4734,
      "step": 1170
    },
    {
      "epoch": 0.28971274245028233,
      "grad_norm": 16.89956283569336,
      "learning_rate": 4.710726171175015e-05,
      "loss": 1.5219,
      "step": 1180
    },
    {
      "epoch": 0.29216793518291184,
      "grad_norm": 16.696962356567383,
      "learning_rate": 4.706459595528629e-05,
      "loss": 1.5145,
      "step": 1190
    },
    {
      "epoch": 0.29462312791554135,
      "grad_norm": 18.16014289855957,
      "learning_rate": 4.702193019882243e-05,
      "loss": 1.406,
      "step": 1200
    },
    {
      "epoch": 0.29707832064817086,
      "grad_norm": 12.99226188659668,
      "learning_rate": 4.6979264442358567e-05,
      "loss": 1.6196,
      "step": 1210
    },
    {
      "epoch": 0.2995335133808004,
      "grad_norm": 19.850727081298828,
      "learning_rate": 4.69365986858947e-05,
      "loss": 1.4101,
      "step": 1220
    },
    {
      "epoch": 0.30198870611342993,
      "grad_norm": 16.86110496520996,
      "learning_rate": 4.689393292943084e-05,
      "loss": 1.4159,
      "step": 1230
    },
    {
      "epoch": 0.30444389884605944,
      "grad_norm": 17.03792953491211,
      "learning_rate": 4.685126717296698e-05,
      "loss": 1.4147,
      "step": 1240
    },
    {
      "epoch": 0.30689909157868894,
      "grad_norm": 16.710134506225586,
      "learning_rate": 4.6808601416503116e-05,
      "loss": 1.4591,
      "step": 1250
    },
    {
      "epoch": 0.30935428431131845,
      "grad_norm": 13.001916885375977,
      "learning_rate": 4.676593566003926e-05,
      "loss": 1.4054,
      "step": 1260
    },
    {
      "epoch": 0.31180947704394796,
      "grad_norm": 17.567415237426758,
      "learning_rate": 4.6723269903575394e-05,
      "loss": 1.551,
      "step": 1270
    },
    {
      "epoch": 0.31426466977657747,
      "grad_norm": 16.654951095581055,
      "learning_rate": 4.668060414711153e-05,
      "loss": 1.4407,
      "step": 1280
    },
    {
      "epoch": 0.316719862509207,
      "grad_norm": 12.024564743041992,
      "learning_rate": 4.6637938390647665e-05,
      "loss": 1.4694,
      "step": 1290
    },
    {
      "epoch": 0.3191750552418365,
      "grad_norm": 24.635839462280273,
      "learning_rate": 4.659527263418381e-05,
      "loss": 1.691,
      "step": 1300
    },
    {
      "epoch": 0.321630247974466,
      "grad_norm": 20.632951736450195,
      "learning_rate": 4.655260687771994e-05,
      "loss": 1.4858,
      "step": 1310
    },
    {
      "epoch": 0.3240854407070955,
      "grad_norm": 18.815563201904297,
      "learning_rate": 4.6509941121256086e-05,
      "loss": 1.4771,
      "step": 1320
    },
    {
      "epoch": 0.326540633439725,
      "grad_norm": 16.385610580444336,
      "learning_rate": 4.6467275364792214e-05,
      "loss": 1.5007,
      "step": 1330
    },
    {
      "epoch": 0.3289958261723545,
      "grad_norm": 16.985719680786133,
      "learning_rate": 4.642460960832836e-05,
      "loss": 1.3832,
      "step": 1340
    },
    {
      "epoch": 0.331451018904984,
      "grad_norm": 23.050825119018555,
      "learning_rate": 4.638194385186449e-05,
      "loss": 1.5212,
      "step": 1350
    },
    {
      "epoch": 0.3339062116376135,
      "grad_norm": 18.97663116455078,
      "learning_rate": 4.6339278095400635e-05,
      "loss": 1.4744,
      "step": 1360
    },
    {
      "epoch": 0.3363614043702431,
      "grad_norm": 28.08919906616211,
      "learning_rate": 4.629661233893677e-05,
      "loss": 1.4431,
      "step": 1370
    },
    {
      "epoch": 0.3388165971028726,
      "grad_norm": 19.38307762145996,
      "learning_rate": 4.6253946582472906e-05,
      "loss": 1.4925,
      "step": 1380
    },
    {
      "epoch": 0.3412717898355021,
      "grad_norm": 14.97477912902832,
      "learning_rate": 4.621128082600905e-05,
      "loss": 1.4688,
      "step": 1390
    },
    {
      "epoch": 0.3437269825681316,
      "grad_norm": 17.70899200439453,
      "learning_rate": 4.6168615069545184e-05,
      "loss": 1.5893,
      "step": 1400
    },
    {
      "epoch": 0.3461821753007611,
      "grad_norm": 18.955608367919922,
      "learning_rate": 4.612594931308133e-05,
      "loss": 1.5269,
      "step": 1410
    },
    {
      "epoch": 0.3486373680333906,
      "grad_norm": 17.403520584106445,
      "learning_rate": 4.608328355661746e-05,
      "loss": 1.5376,
      "step": 1420
    },
    {
      "epoch": 0.35109256076602013,
      "grad_norm": 13.470829963684082,
      "learning_rate": 4.6040617800153605e-05,
      "loss": 1.6508,
      "step": 1430
    },
    {
      "epoch": 0.35354775349864964,
      "grad_norm": 20.344255447387695,
      "learning_rate": 4.5997952043689734e-05,
      "loss": 1.5847,
      "step": 1440
    },
    {
      "epoch": 0.35600294623127915,
      "grad_norm": 14.638472557067871,
      "learning_rate": 4.5955286287225876e-05,
      "loss": 1.3616,
      "step": 1450
    },
    {
      "epoch": 0.35845813896390866,
      "grad_norm": 17.328102111816406,
      "learning_rate": 4.591262053076201e-05,
      "loss": 1.6392,
      "step": 1460
    },
    {
      "epoch": 0.36091333169653816,
      "grad_norm": 15.282246589660645,
      "learning_rate": 4.5869954774298154e-05,
      "loss": 1.3749,
      "step": 1470
    },
    {
      "epoch": 0.36336852442916767,
      "grad_norm": 19.71572494506836,
      "learning_rate": 4.582728901783429e-05,
      "loss": 1.5517,
      "step": 1480
    },
    {
      "epoch": 0.3658237171617972,
      "grad_norm": 14.968583106994629,
      "learning_rate": 4.5784623261370425e-05,
      "loss": 1.3909,
      "step": 1490
    },
    {
      "epoch": 0.3682789098944267,
      "grad_norm": 14.489666938781738,
      "learning_rate": 4.574195750490656e-05,
      "loss": 1.4691,
      "step": 1500
    },
    {
      "epoch": 0.37073410262705625,
      "grad_norm": 16.79570960998535,
      "learning_rate": 4.56992917484427e-05,
      "loss": 1.4453,
      "step": 1510
    },
    {
      "epoch": 0.37318929535968576,
      "grad_norm": 18.040794372558594,
      "learning_rate": 4.565662599197884e-05,
      "loss": 1.4147,
      "step": 1520
    },
    {
      "epoch": 0.37564448809231527,
      "grad_norm": 18.137901306152344,
      "learning_rate": 4.561396023551498e-05,
      "loss": 1.4348,
      "step": 1530
    },
    {
      "epoch": 0.3780996808249448,
      "grad_norm": 17.094587326049805,
      "learning_rate": 4.557129447905111e-05,
      "loss": 1.5563,
      "step": 1540
    },
    {
      "epoch": 0.3805548735575743,
      "grad_norm": 19.015365600585938,
      "learning_rate": 4.552862872258725e-05,
      "loss": 1.4222,
      "step": 1550
    },
    {
      "epoch": 0.3830100662902038,
      "grad_norm": 15.720551490783691,
      "learning_rate": 4.548596296612339e-05,
      "loss": 1.4578,
      "step": 1560
    },
    {
      "epoch": 0.3854652590228333,
      "grad_norm": 22.43003273010254,
      "learning_rate": 4.544329720965953e-05,
      "loss": 1.3819,
      "step": 1570
    },
    {
      "epoch": 0.3879204517554628,
      "grad_norm": 26.70953941345215,
      "learning_rate": 4.5400631453195666e-05,
      "loss": 1.4325,
      "step": 1580
    },
    {
      "epoch": 0.3903756444880923,
      "grad_norm": 24.830394744873047,
      "learning_rate": 4.535796569673181e-05,
      "loss": 1.478,
      "step": 1590
    },
    {
      "epoch": 0.3928308372207218,
      "grad_norm": 15.822714805603027,
      "learning_rate": 4.5315299940267944e-05,
      "loss": 1.4675,
      "step": 1600
    },
    {
      "epoch": 0.3952860299533513,
      "grad_norm": 17.49456787109375,
      "learning_rate": 4.527263418380408e-05,
      "loss": 1.3753,
      "step": 1610
    },
    {
      "epoch": 0.39774122268598083,
      "grad_norm": 13.944931030273438,
      "learning_rate": 4.522996842734022e-05,
      "loss": 1.3101,
      "step": 1620
    },
    {
      "epoch": 0.40019641541861034,
      "grad_norm": 17.308998107910156,
      "learning_rate": 4.518730267087636e-05,
      "loss": 1.4541,
      "step": 1630
    },
    {
      "epoch": 0.40265160815123985,
      "grad_norm": 20.83806800842285,
      "learning_rate": 4.5144636914412494e-05,
      "loss": 1.4763,
      "step": 1640
    },
    {
      "epoch": 0.4051068008838694,
      "grad_norm": 17.77863121032715,
      "learning_rate": 4.510197115794863e-05,
      "loss": 1.3978,
      "step": 1650
    },
    {
      "epoch": 0.4075619936164989,
      "grad_norm": 23.742942810058594,
      "learning_rate": 4.505930540148477e-05,
      "loss": 1.5154,
      "step": 1660
    },
    {
      "epoch": 0.4100171863491284,
      "grad_norm": 17.249296188354492,
      "learning_rate": 4.501663964502091e-05,
      "loss": 1.5058,
      "step": 1670
    },
    {
      "epoch": 0.41247237908175793,
      "grad_norm": 21.119489669799805,
      "learning_rate": 4.497397388855705e-05,
      "loss": 1.5653,
      "step": 1680
    },
    {
      "epoch": 0.41492757181438744,
      "grad_norm": 16.704681396484375,
      "learning_rate": 4.4931308132093185e-05,
      "loss": 1.4657,
      "step": 1690
    },
    {
      "epoch": 0.41738276454701695,
      "grad_norm": 15.709576606750488,
      "learning_rate": 4.488864237562932e-05,
      "loss": 1.5234,
      "step": 1700
    },
    {
      "epoch": 0.41983795727964646,
      "grad_norm": 17.195682525634766,
      "learning_rate": 4.4845976619165457e-05,
      "loss": 1.6241,
      "step": 1710
    },
    {
      "epoch": 0.42229315001227596,
      "grad_norm": 16.096670150756836,
      "learning_rate": 4.48033108627016e-05,
      "loss": 1.4219,
      "step": 1720
    },
    {
      "epoch": 0.42474834274490547,
      "grad_norm": 23.46505355834961,
      "learning_rate": 4.4760645106237735e-05,
      "loss": 1.4842,
      "step": 1730
    },
    {
      "epoch": 0.427203535477535,
      "grad_norm": 13.290119171142578,
      "learning_rate": 4.471797934977388e-05,
      "loss": 1.5783,
      "step": 1740
    },
    {
      "epoch": 0.4296587282101645,
      "grad_norm": 15.338871955871582,
      "learning_rate": 4.467531359331001e-05,
      "loss": 1.4225,
      "step": 1750
    },
    {
      "epoch": 0.432113920942794,
      "grad_norm": 22.225317001342773,
      "learning_rate": 4.463264783684615e-05,
      "loss": 1.4824,
      "step": 1760
    },
    {
      "epoch": 0.4345691136754235,
      "grad_norm": 14.592628479003906,
      "learning_rate": 4.4589982080382284e-05,
      "loss": 1.3504,
      "step": 1770
    },
    {
      "epoch": 0.437024306408053,
      "grad_norm": 15.131017684936523,
      "learning_rate": 4.4547316323918426e-05,
      "loss": 1.4576,
      "step": 1780
    },
    {
      "epoch": 0.4394794991406825,
      "grad_norm": 14.001687049865723,
      "learning_rate": 4.450465056745456e-05,
      "loss": 1.4093,
      "step": 1790
    },
    {
      "epoch": 0.4419346918733121,
      "grad_norm": 20.28236961364746,
      "learning_rate": 4.4461984810990704e-05,
      "loss": 1.4303,
      "step": 1800
    },
    {
      "epoch": 0.4443898846059416,
      "grad_norm": 18.779081344604492,
      "learning_rate": 4.441931905452684e-05,
      "loss": 1.3682,
      "step": 1810
    },
    {
      "epoch": 0.4468450773385711,
      "grad_norm": 20.267845153808594,
      "learning_rate": 4.4376653298062976e-05,
      "loss": 1.5316,
      "step": 1820
    },
    {
      "epoch": 0.4493002700712006,
      "grad_norm": 17.048446655273438,
      "learning_rate": 4.433398754159911e-05,
      "loss": 1.4146,
      "step": 1830
    },
    {
      "epoch": 0.4517554628038301,
      "grad_norm": 21.892135620117188,
      "learning_rate": 4.4291321785135254e-05,
      "loss": 1.4729,
      "step": 1840
    },
    {
      "epoch": 0.4542106555364596,
      "grad_norm": 15.49747085571289,
      "learning_rate": 4.424865602867139e-05,
      "loss": 1.468,
      "step": 1850
    },
    {
      "epoch": 0.4566658482690891,
      "grad_norm": 17.785354614257812,
      "learning_rate": 4.4205990272207525e-05,
      "loss": 1.4255,
      "step": 1860
    },
    {
      "epoch": 0.45912104100171863,
      "grad_norm": 31.079988479614258,
      "learning_rate": 4.416332451574367e-05,
      "loss": 1.5352,
      "step": 1870
    },
    {
      "epoch": 0.46157623373434814,
      "grad_norm": 29.223899841308594,
      "learning_rate": 4.41206587592798e-05,
      "loss": 1.4675,
      "step": 1880
    },
    {
      "epoch": 0.46403142646697765,
      "grad_norm": 16.97996711730957,
      "learning_rate": 4.4077993002815945e-05,
      "loss": 1.5354,
      "step": 1890
    },
    {
      "epoch": 0.46648661919960716,
      "grad_norm": 13.823070526123047,
      "learning_rate": 4.403532724635208e-05,
      "loss": 1.4362,
      "step": 1900
    },
    {
      "epoch": 0.46894181193223666,
      "grad_norm": 18.673524856567383,
      "learning_rate": 4.3992661489888223e-05,
      "loss": 1.5205,
      "step": 1910
    },
    {
      "epoch": 0.47139700466486617,
      "grad_norm": 17.761417388916016,
      "learning_rate": 4.394999573342435e-05,
      "loss": 1.4097,
      "step": 1920
    },
    {
      "epoch": 0.4738521973974957,
      "grad_norm": 21.741365432739258,
      "learning_rate": 4.3907329976960495e-05,
      "loss": 1.4279,
      "step": 1930
    },
    {
      "epoch": 0.47630739013012524,
      "grad_norm": 16.949386596679688,
      "learning_rate": 4.386466422049663e-05,
      "loss": 1.6038,
      "step": 1940
    },
    {
      "epoch": 0.47876258286275475,
      "grad_norm": 16.70180320739746,
      "learning_rate": 4.382199846403277e-05,
      "loss": 1.4841,
      "step": 1950
    },
    {
      "epoch": 0.48121777559538426,
      "grad_norm": 16.346847534179688,
      "learning_rate": 4.377933270756891e-05,
      "loss": 1.3048,
      "step": 1960
    },
    {
      "epoch": 0.48367296832801376,
      "grad_norm": 18.46590232849121,
      "learning_rate": 4.3736666951105044e-05,
      "loss": 1.4369,
      "step": 1970
    },
    {
      "epoch": 0.48612816106064327,
      "grad_norm": 16.91606330871582,
      "learning_rate": 4.369400119464118e-05,
      "loss": 1.4958,
      "step": 1980
    },
    {
      "epoch": 0.4885833537932728,
      "grad_norm": 16.044631958007812,
      "learning_rate": 4.365133543817732e-05,
      "loss": 1.3828,
      "step": 1990
    },
    {
      "epoch": 0.4910385465259023,
      "grad_norm": 18.526227951049805,
      "learning_rate": 4.360866968171346e-05,
      "loss": 1.4266,
      "step": 2000
    },
    {
      "epoch": 0.4934937392585318,
      "grad_norm": 17.16498374938965,
      "learning_rate": 4.35660039252496e-05,
      "loss": 1.5026,
      "step": 2010
    },
    {
      "epoch": 0.4959489319911613,
      "grad_norm": 13.065351486206055,
      "learning_rate": 4.352333816878573e-05,
      "loss": 1.3646,
      "step": 2020
    },
    {
      "epoch": 0.4984041247237908,
      "grad_norm": 23.500505447387695,
      "learning_rate": 4.348067241232187e-05,
      "loss": 1.5044,
      "step": 2030
    },
    {
      "epoch": 0.5008593174564203,
      "grad_norm": 18.72426414489746,
      "learning_rate": 4.343800665585801e-05,
      "loss": 1.542,
      "step": 2040
    },
    {
      "epoch": 0.5033145101890498,
      "grad_norm": 13.814963340759277,
      "learning_rate": 4.339534089939415e-05,
      "loss": 1.5229,
      "step": 2050
    },
    {
      "epoch": 0.5057697029216793,
      "grad_norm": 17.61827850341797,
      "learning_rate": 4.3352675142930285e-05,
      "loss": 1.3409,
      "step": 2060
    },
    {
      "epoch": 0.5082248956543088,
      "grad_norm": 16.700817108154297,
      "learning_rate": 4.331000938646642e-05,
      "loss": 1.3886,
      "step": 2070
    },
    {
      "epoch": 0.5106800883869383,
      "grad_norm": 17.57028579711914,
      "learning_rate": 4.326734363000256e-05,
      "loss": 1.4905,
      "step": 2080
    },
    {
      "epoch": 0.5131352811195679,
      "grad_norm": 18.160280227661133,
      "learning_rate": 4.32246778735387e-05,
      "loss": 1.5121,
      "step": 2090
    },
    {
      "epoch": 0.5155904738521974,
      "grad_norm": 35.03056716918945,
      "learning_rate": 4.318201211707484e-05,
      "loss": 1.5908,
      "step": 2100
    },
    {
      "epoch": 0.5180456665848269,
      "grad_norm": 14.927021026611328,
      "learning_rate": 4.313934636061098e-05,
      "loss": 1.4951,
      "step": 2110
    },
    {
      "epoch": 0.5205008593174564,
      "grad_norm": 16.705181121826172,
      "learning_rate": 4.309668060414712e-05,
      "loss": 1.4093,
      "step": 2120
    },
    {
      "epoch": 0.5229560520500859,
      "grad_norm": 16.373506546020508,
      "learning_rate": 4.305401484768325e-05,
      "loss": 1.5615,
      "step": 2130
    },
    {
      "epoch": 0.5254112447827154,
      "grad_norm": 14.932629585266113,
      "learning_rate": 4.301134909121939e-05,
      "loss": 1.4085,
      "step": 2140
    },
    {
      "epoch": 0.5278664375153449,
      "grad_norm": 12.139214515686035,
      "learning_rate": 4.2968683334755526e-05,
      "loss": 1.5105,
      "step": 2150
    },
    {
      "epoch": 0.5303216302479745,
      "grad_norm": 16.004430770874023,
      "learning_rate": 4.292601757829167e-05,
      "loss": 1.3465,
      "step": 2160
    },
    {
      "epoch": 0.532776822980604,
      "grad_norm": 16.647985458374023,
      "learning_rate": 4.2883351821827804e-05,
      "loss": 1.4173,
      "step": 2170
    },
    {
      "epoch": 0.5352320157132335,
      "grad_norm": 14.7952299118042,
      "learning_rate": 4.284068606536394e-05,
      "loss": 1.427,
      "step": 2180
    },
    {
      "epoch": 0.537687208445863,
      "grad_norm": 16.22884178161621,
      "learning_rate": 4.2798020308900075e-05,
      "loss": 1.2621,
      "step": 2190
    },
    {
      "epoch": 0.5401424011784925,
      "grad_norm": 16.415592193603516,
      "learning_rate": 4.275535455243622e-05,
      "loss": 1.3982,
      "step": 2200
    },
    {
      "epoch": 0.5425975939111221,
      "grad_norm": 15.134458541870117,
      "learning_rate": 4.2712688795972353e-05,
      "loss": 1.5696,
      "step": 2210
    },
    {
      "epoch": 0.5450527866437516,
      "grad_norm": 17.287240982055664,
      "learning_rate": 4.2670023039508496e-05,
      "loss": 1.4709,
      "step": 2220
    },
    {
      "epoch": 0.5475079793763811,
      "grad_norm": 17.97455406188965,
      "learning_rate": 4.2627357283044625e-05,
      "loss": 1.4132,
      "step": 2230
    },
    {
      "epoch": 0.5499631721090106,
      "grad_norm": 16.573801040649414,
      "learning_rate": 4.258469152658077e-05,
      "loss": 1.3849,
      "step": 2240
    },
    {
      "epoch": 0.5524183648416401,
      "grad_norm": 19.99306297302246,
      "learning_rate": 4.25420257701169e-05,
      "loss": 1.4937,
      "step": 2250
    },
    {
      "epoch": 0.5548735575742696,
      "grad_norm": 15.429673194885254,
      "learning_rate": 4.2499360013653045e-05,
      "loss": 1.5123,
      "step": 2260
    },
    {
      "epoch": 0.5573287503068991,
      "grad_norm": 19.24848747253418,
      "learning_rate": 4.245669425718918e-05,
      "loss": 1.4044,
      "step": 2270
    },
    {
      "epoch": 0.5597839430395286,
      "grad_norm": 16.417409896850586,
      "learning_rate": 4.241402850072532e-05,
      "loss": 1.4529,
      "step": 2280
    },
    {
      "epoch": 0.5622391357721581,
      "grad_norm": 15.979519844055176,
      "learning_rate": 4.237136274426146e-05,
      "loss": 1.3694,
      "step": 2290
    },
    {
      "epoch": 0.5646943285047876,
      "grad_norm": 14.13403606414795,
      "learning_rate": 4.2328696987797594e-05,
      "loss": 1.3714,
      "step": 2300
    },
    {
      "epoch": 0.5671495212374171,
      "grad_norm": 20.40810203552246,
      "learning_rate": 4.228603123133374e-05,
      "loss": 1.4997,
      "step": 2310
    },
    {
      "epoch": 0.5696047139700466,
      "grad_norm": 22.561237335205078,
      "learning_rate": 4.224336547486987e-05,
      "loss": 1.4359,
      "step": 2320
    },
    {
      "epoch": 0.5720599067026761,
      "grad_norm": 17.77558135986328,
      "learning_rate": 4.220069971840601e-05,
      "loss": 1.4527,
      "step": 2330
    },
    {
      "epoch": 0.5745150994353057,
      "grad_norm": 20.80369758605957,
      "learning_rate": 4.2158033961942144e-05,
      "loss": 1.4418,
      "step": 2340
    },
    {
      "epoch": 0.5769702921679352,
      "grad_norm": 13.88691520690918,
      "learning_rate": 4.2115368205478286e-05,
      "loss": 1.4019,
      "step": 2350
    },
    {
      "epoch": 0.5794254849005647,
      "grad_norm": 26.633609771728516,
      "learning_rate": 4.207270244901442e-05,
      "loss": 1.5326,
      "step": 2360
    },
    {
      "epoch": 0.5818806776331942,
      "grad_norm": 14.496254920959473,
      "learning_rate": 4.2030036692550564e-05,
      "loss": 1.4567,
      "step": 2370
    },
    {
      "epoch": 0.5843358703658237,
      "grad_norm": 15.545933723449707,
      "learning_rate": 4.19873709360867e-05,
      "loss": 1.3777,
      "step": 2380
    },
    {
      "epoch": 0.5867910630984532,
      "grad_norm": 18.755813598632812,
      "learning_rate": 4.1944705179622836e-05,
      "loss": 1.5439,
      "step": 2390
    },
    {
      "epoch": 0.5892462558310827,
      "grad_norm": 17.264755249023438,
      "learning_rate": 4.190203942315897e-05,
      "loss": 1.4229,
      "step": 2400
    },
    {
      "epoch": 0.5917014485637122,
      "grad_norm": 16.540359497070312,
      "learning_rate": 4.1859373666695114e-05,
      "loss": 1.5644,
      "step": 2410
    },
    {
      "epoch": 0.5941566412963417,
      "grad_norm": 15.716084480285645,
      "learning_rate": 4.181670791023125e-05,
      "loss": 1.4109,
      "step": 2420
    },
    {
      "epoch": 0.5966118340289712,
      "grad_norm": 15.65455436706543,
      "learning_rate": 4.177404215376739e-05,
      "loss": 1.4069,
      "step": 2430
    },
    {
      "epoch": 0.5990670267616008,
      "grad_norm": 18.618257522583008,
      "learning_rate": 4.173137639730353e-05,
      "loss": 1.5073,
      "step": 2440
    },
    {
      "epoch": 0.6015222194942303,
      "grad_norm": 23.19086456298828,
      "learning_rate": 4.168871064083966e-05,
      "loss": 1.6468,
      "step": 2450
    },
    {
      "epoch": 0.6039774122268599,
      "grad_norm": 20.663360595703125,
      "learning_rate": 4.16460448843758e-05,
      "loss": 1.6288,
      "step": 2460
    },
    {
      "epoch": 0.6064326049594894,
      "grad_norm": 16.569705963134766,
      "learning_rate": 4.160337912791194e-05,
      "loss": 1.3942,
      "step": 2470
    },
    {
      "epoch": 0.6088877976921189,
      "grad_norm": 14.142617225646973,
      "learning_rate": 4.1560713371448077e-05,
      "loss": 1.3407,
      "step": 2480
    },
    {
      "epoch": 0.6113429904247484,
      "grad_norm": 16.254920959472656,
      "learning_rate": 4.151804761498422e-05,
      "loss": 1.3614,
      "step": 2490
    },
    {
      "epoch": 0.6137981831573779,
      "grad_norm": 22.47358512878418,
      "learning_rate": 4.1475381858520355e-05,
      "loss": 1.4774,
      "step": 2500
    },
    {
      "epoch": 0.6162533758900074,
      "grad_norm": 17.22732162475586,
      "learning_rate": 4.143271610205649e-05,
      "loss": 1.4303,
      "step": 2510
    },
    {
      "epoch": 0.6187085686226369,
      "grad_norm": 16.51824378967285,
      "learning_rate": 4.1390050345592626e-05,
      "loss": 1.699,
      "step": 2520
    },
    {
      "epoch": 0.6211637613552664,
      "grad_norm": 19.266218185424805,
      "learning_rate": 4.134738458912877e-05,
      "loss": 1.3931,
      "step": 2530
    },
    {
      "epoch": 0.6236189540878959,
      "grad_norm": 12.435629844665527,
      "learning_rate": 4.1304718832664904e-05,
      "loss": 1.5579,
      "step": 2540
    },
    {
      "epoch": 0.6260741468205254,
      "grad_norm": 17.468416213989258,
      "learning_rate": 4.126205307620104e-05,
      "loss": 1.3873,
      "step": 2550
    },
    {
      "epoch": 0.6285293395531549,
      "grad_norm": 17.368202209472656,
      "learning_rate": 4.121938731973718e-05,
      "loss": 1.2849,
      "step": 2560
    },
    {
      "epoch": 0.6309845322857844,
      "grad_norm": 18.743301391601562,
      "learning_rate": 4.117672156327332e-05,
      "loss": 1.4225,
      "step": 2570
    },
    {
      "epoch": 0.633439725018414,
      "grad_norm": 17.942235946655273,
      "learning_rate": 4.113405580680946e-05,
      "loss": 1.4224,
      "step": 2580
    },
    {
      "epoch": 0.6358949177510435,
      "grad_norm": 17.086471557617188,
      "learning_rate": 4.1091390050345596e-05,
      "loss": 1.5726,
      "step": 2590
    },
    {
      "epoch": 0.638350110483673,
      "grad_norm": 17.431852340698242,
      "learning_rate": 4.104872429388174e-05,
      "loss": 1.43,
      "step": 2600
    },
    {
      "epoch": 0.6408053032163025,
      "grad_norm": 17.47749137878418,
      "learning_rate": 4.100605853741787e-05,
      "loss": 1.4398,
      "step": 2610
    },
    {
      "epoch": 0.643260495948932,
      "grad_norm": 12.976348876953125,
      "learning_rate": 4.096339278095401e-05,
      "loss": 1.4721,
      "step": 2620
    },
    {
      "epoch": 0.6457156886815615,
      "grad_norm": 14.549762725830078,
      "learning_rate": 4.0920727024490145e-05,
      "loss": 1.3825,
      "step": 2630
    },
    {
      "epoch": 0.648170881414191,
      "grad_norm": 13.342370986938477,
      "learning_rate": 4.087806126802629e-05,
      "loss": 1.3869,
      "step": 2640
    },
    {
      "epoch": 0.6506260741468205,
      "grad_norm": 19.714115142822266,
      "learning_rate": 4.083539551156242e-05,
      "loss": 1.4425,
      "step": 2650
    },
    {
      "epoch": 0.65308126687945,
      "grad_norm": 21.226633071899414,
      "learning_rate": 4.079272975509856e-05,
      "loss": 1.395,
      "step": 2660
    },
    {
      "epoch": 0.6555364596120795,
      "grad_norm": 17.75282096862793,
      "learning_rate": 4.0750063998634694e-05,
      "loss": 1.5896,
      "step": 2670
    },
    {
      "epoch": 0.657991652344709,
      "grad_norm": 14.397348403930664,
      "learning_rate": 4.070739824217084e-05,
      "loss": 1.3985,
      "step": 2680
    },
    {
      "epoch": 0.6604468450773385,
      "grad_norm": 20.01553726196289,
      "learning_rate": 4.066473248570697e-05,
      "loss": 1.3229,
      "step": 2690
    },
    {
      "epoch": 0.662902037809968,
      "grad_norm": 17.824188232421875,
      "learning_rate": 4.0622066729243115e-05,
      "loss": 1.321,
      "step": 2700
    },
    {
      "epoch": 0.6653572305425975,
      "grad_norm": 17.887250900268555,
      "learning_rate": 4.0579400972779244e-05,
      "loss": 1.4648,
      "step": 2710
    },
    {
      "epoch": 0.667812423275227,
      "grad_norm": 17.616044998168945,
      "learning_rate": 4.0536735216315386e-05,
      "loss": 1.4527,
      "step": 2720
    },
    {
      "epoch": 0.6702676160078567,
      "grad_norm": 13.734485626220703,
      "learning_rate": 4.049406945985152e-05,
      "loss": 1.6114,
      "step": 2730
    },
    {
      "epoch": 0.6727228087404862,
      "grad_norm": 18.10129165649414,
      "learning_rate": 4.0451403703387664e-05,
      "loss": 1.6432,
      "step": 2740
    },
    {
      "epoch": 0.6751780014731157,
      "grad_norm": 12.48859977722168,
      "learning_rate": 4.04087379469238e-05,
      "loss": 1.4153,
      "step": 2750
    },
    {
      "epoch": 0.6776331942057452,
      "grad_norm": 16.799060821533203,
      "learning_rate": 4.036607219045994e-05,
      "loss": 1.3253,
      "step": 2760
    },
    {
      "epoch": 0.6800883869383747,
      "grad_norm": 19.725326538085938,
      "learning_rate": 4.032340643399608e-05,
      "loss": 1.3897,
      "step": 2770
    },
    {
      "epoch": 0.6825435796710042,
      "grad_norm": 13.148829460144043,
      "learning_rate": 4.028074067753221e-05,
      "loss": 1.3634,
      "step": 2780
    },
    {
      "epoch": 0.6849987724036337,
      "grad_norm": 18.663387298583984,
      "learning_rate": 4.0238074921068356e-05,
      "loss": 1.2721,
      "step": 2790
    },
    {
      "epoch": 0.6874539651362632,
      "grad_norm": 22.79940414428711,
      "learning_rate": 4.019540916460449e-05,
      "loss": 1.5053,
      "step": 2800
    },
    {
      "epoch": 0.6899091578688927,
      "grad_norm": 18.667068481445312,
      "learning_rate": 4.0152743408140634e-05,
      "loss": 1.5129,
      "step": 2810
    },
    {
      "epoch": 0.6923643506015222,
      "grad_norm": 17.799528121948242,
      "learning_rate": 4.011007765167676e-05,
      "loss": 1.3682,
      "step": 2820
    },
    {
      "epoch": 0.6948195433341517,
      "grad_norm": 16.9714412689209,
      "learning_rate": 4.0067411895212905e-05,
      "loss": 1.3562,
      "step": 2830
    },
    {
      "epoch": 0.6972747360667813,
      "grad_norm": 18.654939651489258,
      "learning_rate": 4.002474613874904e-05,
      "loss": 1.5099,
      "step": 2840
    },
    {
      "epoch": 0.6997299287994108,
      "grad_norm": 14.359925270080566,
      "learning_rate": 3.998208038228518e-05,
      "loss": 1.4923,
      "step": 2850
    },
    {
      "epoch": 0.7021851215320403,
      "grad_norm": 16.127031326293945,
      "learning_rate": 3.993941462582132e-05,
      "loss": 1.362,
      "step": 2860
    },
    {
      "epoch": 0.7046403142646698,
      "grad_norm": 19.179546356201172,
      "learning_rate": 3.9896748869357454e-05,
      "loss": 1.4716,
      "step": 2870
    },
    {
      "epoch": 0.7070955069972993,
      "grad_norm": 22.109254837036133,
      "learning_rate": 3.985408311289359e-05,
      "loss": 1.5201,
      "step": 2880
    },
    {
      "epoch": 0.7095506997299288,
      "grad_norm": 16.203014373779297,
      "learning_rate": 3.981141735642973e-05,
      "loss": 1.5702,
      "step": 2890
    },
    {
      "epoch": 0.7120058924625583,
      "grad_norm": 16.150514602661133,
      "learning_rate": 3.976875159996587e-05,
      "loss": 1.6373,
      "step": 2900
    },
    {
      "epoch": 0.7144610851951878,
      "grad_norm": 21.49725914001465,
      "learning_rate": 3.972608584350201e-05,
      "loss": 1.5674,
      "step": 2910
    },
    {
      "epoch": 0.7169162779278173,
      "grad_norm": 17.157785415649414,
      "learning_rate": 3.9683420087038146e-05,
      "loss": 1.3893,
      "step": 2920
    },
    {
      "epoch": 0.7193714706604468,
      "grad_norm": 15.205799102783203,
      "learning_rate": 3.964075433057428e-05,
      "loss": 1.556,
      "step": 2930
    },
    {
      "epoch": 0.7218266633930763,
      "grad_norm": 14.906254768371582,
      "learning_rate": 3.959808857411042e-05,
      "loss": 1.4066,
      "step": 2940
    },
    {
      "epoch": 0.7242818561257058,
      "grad_norm": 12.180951118469238,
      "learning_rate": 3.955542281764656e-05,
      "loss": 1.5384,
      "step": 2950
    },
    {
      "epoch": 0.7267370488583353,
      "grad_norm": 14.348186492919922,
      "learning_rate": 3.9512757061182695e-05,
      "loss": 1.3946,
      "step": 2960
    },
    {
      "epoch": 0.7291922415909649,
      "grad_norm": 16.37265968322754,
      "learning_rate": 3.947009130471884e-05,
      "loss": 1.2856,
      "step": 2970
    },
    {
      "epoch": 0.7316474343235944,
      "grad_norm": 27.934913635253906,
      "learning_rate": 3.9427425548254973e-05,
      "loss": 1.4345,
      "step": 2980
    },
    {
      "epoch": 0.7341026270562239,
      "grad_norm": 19.458572387695312,
      "learning_rate": 3.938475979179111e-05,
      "loss": 1.5031,
      "step": 2990
    },
    {
      "epoch": 0.7365578197888534,
      "grad_norm": 14.596637725830078,
      "learning_rate": 3.934209403532725e-05,
      "loss": 1.4201,
      "step": 3000
    },
    {
      "epoch": 0.7390130125214829,
      "grad_norm": 27.661907196044922,
      "learning_rate": 3.929942827886339e-05,
      "loss": 1.3778,
      "step": 3010
    },
    {
      "epoch": 0.7414682052541125,
      "grad_norm": 14.581459999084473,
      "learning_rate": 3.925676252239953e-05,
      "loss": 1.4029,
      "step": 3020
    },
    {
      "epoch": 0.743923397986742,
      "grad_norm": 15.15911865234375,
      "learning_rate": 3.921409676593566e-05,
      "loss": 1.4885,
      "step": 3030
    },
    {
      "epoch": 0.7463785907193715,
      "grad_norm": 17.699533462524414,
      "learning_rate": 3.91714310094718e-05,
      "loss": 1.2761,
      "step": 3040
    },
    {
      "epoch": 0.748833783452001,
      "grad_norm": 13.953875541687012,
      "learning_rate": 3.9128765253007936e-05,
      "loss": 1.4315,
      "step": 3050
    },
    {
      "epoch": 0.7512889761846305,
      "grad_norm": 17.96657943725586,
      "learning_rate": 3.908609949654408e-05,
      "loss": 1.4788,
      "step": 3060
    },
    {
      "epoch": 0.75374416891726,
      "grad_norm": 21.356395721435547,
      "learning_rate": 3.9043433740080214e-05,
      "loss": 1.3906,
      "step": 3070
    },
    {
      "epoch": 0.7561993616498895,
      "grad_norm": 15.4409818649292,
      "learning_rate": 3.900076798361635e-05,
      "loss": 1.3106,
      "step": 3080
    },
    {
      "epoch": 0.758654554382519,
      "grad_norm": 18.430950164794922,
      "learning_rate": 3.8958102227152486e-05,
      "loss": 1.4387,
      "step": 3090
    },
    {
      "epoch": 0.7611097471151486,
      "grad_norm": 17.283601760864258,
      "learning_rate": 3.891543647068863e-05,
      "loss": 1.4258,
      "step": 3100
    },
    {
      "epoch": 0.7635649398477781,
      "grad_norm": 19.527610778808594,
      "learning_rate": 3.8872770714224764e-05,
      "loss": 1.5272,
      "step": 3110
    },
    {
      "epoch": 0.7660201325804076,
      "grad_norm": 14.973008155822754,
      "learning_rate": 3.8830104957760906e-05,
      "loss": 1.459,
      "step": 3120
    },
    {
      "epoch": 0.7684753253130371,
      "grad_norm": 17.693025588989258,
      "learning_rate": 3.878743920129704e-05,
      "loss": 1.5337,
      "step": 3130
    },
    {
      "epoch": 0.7709305180456666,
      "grad_norm": 15.033425331115723,
      "learning_rate": 3.874477344483318e-05,
      "loss": 1.3375,
      "step": 3140
    },
    {
      "epoch": 0.7733857107782961,
      "grad_norm": 16.499704360961914,
      "learning_rate": 3.870210768836931e-05,
      "loss": 1.3908,
      "step": 3150
    },
    {
      "epoch": 0.7758409035109256,
      "grad_norm": 13.358887672424316,
      "learning_rate": 3.8659441931905455e-05,
      "loss": 1.5422,
      "step": 3160
    },
    {
      "epoch": 0.7782960962435551,
      "grad_norm": 19.67469596862793,
      "learning_rate": 3.861677617544159e-05,
      "loss": 1.3974,
      "step": 3170
    },
    {
      "epoch": 0.7807512889761846,
      "grad_norm": 20.731773376464844,
      "learning_rate": 3.8574110418977734e-05,
      "loss": 1.4827,
      "step": 3180
    },
    {
      "epoch": 0.7832064817088141,
      "grad_norm": 18.68391227722168,
      "learning_rate": 3.853144466251387e-05,
      "loss": 1.3908,
      "step": 3190
    },
    {
      "epoch": 0.7856616744414436,
      "grad_norm": 17.550626754760742,
      "learning_rate": 3.8488778906050005e-05,
      "loss": 1.4456,
      "step": 3200
    },
    {
      "epoch": 0.7881168671740731,
      "grad_norm": 18.832115173339844,
      "learning_rate": 3.844611314958614e-05,
      "loss": 1.3114,
      "step": 3210
    },
    {
      "epoch": 0.7905720599067027,
      "grad_norm": 16.100078582763672,
      "learning_rate": 3.840344739312228e-05,
      "loss": 1.3499,
      "step": 3220
    },
    {
      "epoch": 0.7930272526393322,
      "grad_norm": 15.976258277893066,
      "learning_rate": 3.836078163665842e-05,
      "loss": 1.6532,
      "step": 3230
    },
    {
      "epoch": 0.7954824453719617,
      "grad_norm": 19.011140823364258,
      "learning_rate": 3.8318115880194554e-05,
      "loss": 1.5129,
      "step": 3240
    },
    {
      "epoch": 0.7979376381045912,
      "grad_norm": 21.279712677001953,
      "learning_rate": 3.8275450123730697e-05,
      "loss": 1.3748,
      "step": 3250
    },
    {
      "epoch": 0.8003928308372207,
      "grad_norm": 18.111595153808594,
      "learning_rate": 3.823278436726683e-05,
      "loss": 1.2565,
      "step": 3260
    },
    {
      "epoch": 0.8028480235698502,
      "grad_norm": 30.748437881469727,
      "learning_rate": 3.8190118610802975e-05,
      "loss": 1.4339,
      "step": 3270
    },
    {
      "epoch": 0.8053032163024797,
      "grad_norm": 19.230167388916016,
      "learning_rate": 3.814745285433911e-05,
      "loss": 1.4611,
      "step": 3280
    },
    {
      "epoch": 0.8077584090351092,
      "grad_norm": 21.915754318237305,
      "learning_rate": 3.810478709787525e-05,
      "loss": 1.4547,
      "step": 3290
    },
    {
      "epoch": 0.8102136017677388,
      "grad_norm": 16.525386810302734,
      "learning_rate": 3.806212134141138e-05,
      "loss": 1.2907,
      "step": 3300
    },
    {
      "epoch": 0.8126687945003683,
      "grad_norm": 14.715694427490234,
      "learning_rate": 3.8019455584947524e-05,
      "loss": 1.5302,
      "step": 3310
    },
    {
      "epoch": 0.8151239872329978,
      "grad_norm": 12.278244018554688,
      "learning_rate": 3.797678982848366e-05,
      "loss": 1.4958,
      "step": 3320
    },
    {
      "epoch": 0.8175791799656273,
      "grad_norm": 13.848058700561523,
      "learning_rate": 3.79341240720198e-05,
      "loss": 1.3988,
      "step": 3330
    },
    {
      "epoch": 0.8200343726982569,
      "grad_norm": 19.435157775878906,
      "learning_rate": 3.789145831555594e-05,
      "loss": 1.3103,
      "step": 3340
    },
    {
      "epoch": 0.8224895654308864,
      "grad_norm": 18.215097427368164,
      "learning_rate": 3.784879255909207e-05,
      "loss": 1.4423,
      "step": 3350
    },
    {
      "epoch": 0.8249447581635159,
      "grad_norm": 21.538728713989258,
      "learning_rate": 3.780612680262821e-05,
      "loss": 1.4674,
      "step": 3360
    },
    {
      "epoch": 0.8273999508961454,
      "grad_norm": 20.059070587158203,
      "learning_rate": 3.776346104616435e-05,
      "loss": 1.6018,
      "step": 3370
    },
    {
      "epoch": 0.8298551436287749,
      "grad_norm": 21.538768768310547,
      "learning_rate": 3.772079528970049e-05,
      "loss": 1.349,
      "step": 3380
    },
    {
      "epoch": 0.8323103363614044,
      "grad_norm": 16.195768356323242,
      "learning_rate": 3.767812953323663e-05,
      "loss": 1.4604,
      "step": 3390
    },
    {
      "epoch": 0.8347655290940339,
      "grad_norm": 17.596511840820312,
      "learning_rate": 3.763546377677276e-05,
      "loss": 1.3417,
      "step": 3400
    },
    {
      "epoch": 0.8372207218266634,
      "grad_norm": 19.674142837524414,
      "learning_rate": 3.75927980203089e-05,
      "loss": 1.4176,
      "step": 3410
    },
    {
      "epoch": 0.8396759145592929,
      "grad_norm": 19.02620506286621,
      "learning_rate": 3.7550132263845036e-05,
      "loss": 1.4687,
      "step": 3420
    },
    {
      "epoch": 0.8421311072919224,
      "grad_norm": 14.587723731994629,
      "learning_rate": 3.750746650738118e-05,
      "loss": 1.4754,
      "step": 3430
    },
    {
      "epoch": 0.8445863000245519,
      "grad_norm": 14.039074897766113,
      "learning_rate": 3.7464800750917314e-05,
      "loss": 1.3586,
      "step": 3440
    },
    {
      "epoch": 0.8470414927571814,
      "grad_norm": 18.676910400390625,
      "learning_rate": 3.742213499445346e-05,
      "loss": 1.4197,
      "step": 3450
    },
    {
      "epoch": 0.8494966854898109,
      "grad_norm": 17.950693130493164,
      "learning_rate": 3.737946923798959e-05,
      "loss": 1.3752,
      "step": 3460
    },
    {
      "epoch": 0.8519518782224405,
      "grad_norm": 17.076566696166992,
      "learning_rate": 3.733680348152573e-05,
      "loss": 1.5025,
      "step": 3470
    },
    {
      "epoch": 0.85440707095507,
      "grad_norm": 15.132450103759766,
      "learning_rate": 3.729413772506187e-05,
      "loss": 1.4302,
      "step": 3480
    },
    {
      "epoch": 0.8568622636876995,
      "grad_norm": 17.978593826293945,
      "learning_rate": 3.7251471968598006e-05,
      "loss": 1.3674,
      "step": 3490
    },
    {
      "epoch": 0.859317456420329,
      "grad_norm": 12.266763687133789,
      "learning_rate": 3.720880621213415e-05,
      "loss": 1.3781,
      "step": 3500
    },
    {
      "epoch": 0.8617726491529585,
      "grad_norm": 17.189220428466797,
      "learning_rate": 3.716614045567028e-05,
      "loss": 1.2705,
      "step": 3510
    },
    {
      "epoch": 0.864227841885588,
      "grad_norm": 18.755123138427734,
      "learning_rate": 3.712347469920642e-05,
      "loss": 1.3904,
      "step": 3520
    },
    {
      "epoch": 0.8666830346182175,
      "grad_norm": 17.547069549560547,
      "learning_rate": 3.7080808942742555e-05,
      "loss": 1.3124,
      "step": 3530
    },
    {
      "epoch": 0.869138227350847,
      "grad_norm": 17.31968116760254,
      "learning_rate": 3.70381431862787e-05,
      "loss": 1.5273,
      "step": 3540
    },
    {
      "epoch": 0.8715934200834765,
      "grad_norm": 19.748464584350586,
      "learning_rate": 3.699547742981483e-05,
      "loss": 1.4143,
      "step": 3550
    },
    {
      "epoch": 0.874048612816106,
      "grad_norm": 19.155284881591797,
      "learning_rate": 3.695281167335097e-05,
      "loss": 1.5095,
      "step": 3560
    },
    {
      "epoch": 0.8765038055487355,
      "grad_norm": 18.978918075561523,
      "learning_rate": 3.6910145916887105e-05,
      "loss": 1.4809,
      "step": 3570
    },
    {
      "epoch": 0.878958998281365,
      "grad_norm": 22.339454650878906,
      "learning_rate": 3.686748016042325e-05,
      "loss": 1.6543,
      "step": 3580
    },
    {
      "epoch": 0.8814141910139947,
      "grad_norm": 14.792805671691895,
      "learning_rate": 3.682481440395938e-05,
      "loss": 1.2955,
      "step": 3590
    },
    {
      "epoch": 0.8838693837466242,
      "grad_norm": 16.361894607543945,
      "learning_rate": 3.6782148647495525e-05,
      "loss": 1.2583,
      "step": 3600
    },
    {
      "epoch": 0.8863245764792537,
      "grad_norm": 11.937975883483887,
      "learning_rate": 3.673948289103166e-05,
      "loss": 1.3422,
      "step": 3610
    },
    {
      "epoch": 0.8887797692118832,
      "grad_norm": 14.186751365661621,
      "learning_rate": 3.6696817134567796e-05,
      "loss": 1.4503,
      "step": 3620
    },
    {
      "epoch": 0.8912349619445127,
      "grad_norm": 20.70210075378418,
      "learning_rate": 3.665415137810393e-05,
      "loss": 1.3971,
      "step": 3630
    },
    {
      "epoch": 0.8936901546771422,
      "grad_norm": 18.73181915283203,
      "learning_rate": 3.6611485621640074e-05,
      "loss": 1.503,
      "step": 3640
    },
    {
      "epoch": 0.8961453474097717,
      "grad_norm": 15.94405746459961,
      "learning_rate": 3.656881986517621e-05,
      "loss": 1.4353,
      "step": 3650
    },
    {
      "epoch": 0.8986005401424012,
      "grad_norm": 12.238526344299316,
      "learning_rate": 3.652615410871235e-05,
      "loss": 1.3945,
      "step": 3660
    },
    {
      "epoch": 0.9010557328750307,
      "grad_norm": 16.299972534179688,
      "learning_rate": 3.648348835224849e-05,
      "loss": 1.3123,
      "step": 3670
    },
    {
      "epoch": 0.9035109256076602,
      "grad_norm": 17.802783966064453,
      "learning_rate": 3.6440822595784624e-05,
      "loss": 1.3668,
      "step": 3680
    },
    {
      "epoch": 0.9059661183402897,
      "grad_norm": 17.662872314453125,
      "learning_rate": 3.6398156839320766e-05,
      "loss": 1.5052,
      "step": 3690
    },
    {
      "epoch": 0.9084213110729192,
      "grad_norm": 18.821720123291016,
      "learning_rate": 3.63554910828569e-05,
      "loss": 1.3157,
      "step": 3700
    },
    {
      "epoch": 0.9108765038055487,
      "grad_norm": 12.884428977966309,
      "learning_rate": 3.6312825326393044e-05,
      "loss": 1.388,
      "step": 3710
    },
    {
      "epoch": 0.9133316965381783,
      "grad_norm": 21.178285598754883,
      "learning_rate": 3.627015956992917e-05,
      "loss": 1.4173,
      "step": 3720
    },
    {
      "epoch": 0.9157868892708078,
      "grad_norm": 14.970173835754395,
      "learning_rate": 3.6227493813465315e-05,
      "loss": 1.3121,
      "step": 3730
    },
    {
      "epoch": 0.9182420820034373,
      "grad_norm": 20.6700382232666,
      "learning_rate": 3.618482805700145e-05,
      "loss": 1.4208,
      "step": 3740
    },
    {
      "epoch": 0.9206972747360668,
      "grad_norm": 21.212387084960938,
      "learning_rate": 3.614216230053759e-05,
      "loss": 1.2776,
      "step": 3750
    },
    {
      "epoch": 0.9231524674686963,
      "grad_norm": 25.205507278442383,
      "learning_rate": 3.609949654407373e-05,
      "loss": 1.4615,
      "step": 3760
    },
    {
      "epoch": 0.9256076602013258,
      "grad_norm": 20.24227523803711,
      "learning_rate": 3.605683078760987e-05,
      "loss": 1.4258,
      "step": 3770
    },
    {
      "epoch": 0.9280628529339553,
      "grad_norm": 15.712874412536621,
      "learning_rate": 3.6014165031146e-05,
      "loss": 1.5204,
      "step": 3780
    },
    {
      "epoch": 0.9305180456665848,
      "grad_norm": 18.097076416015625,
      "learning_rate": 3.597149927468214e-05,
      "loss": 1.4445,
      "step": 3790
    },
    {
      "epoch": 0.9329732383992143,
      "grad_norm": 16.647544860839844,
      "learning_rate": 3.592883351821828e-05,
      "loss": 1.4346,
      "step": 3800
    },
    {
      "epoch": 0.9354284311318438,
      "grad_norm": 13.150869369506836,
      "learning_rate": 3.588616776175442e-05,
      "loss": 1.5801,
      "step": 3810
    },
    {
      "epoch": 0.9378836238644733,
      "grad_norm": 21.167810440063477,
      "learning_rate": 3.5843502005290556e-05,
      "loss": 1.484,
      "step": 3820
    },
    {
      "epoch": 0.9403388165971028,
      "grad_norm": 20.463638305664062,
      "learning_rate": 3.580083624882669e-05,
      "loss": 1.6775,
      "step": 3830
    },
    {
      "epoch": 0.9427940093297323,
      "grad_norm": 14.504389762878418,
      "learning_rate": 3.575817049236283e-05,
      "loss": 1.5866,
      "step": 3840
    },
    {
      "epoch": 0.9452492020623618,
      "grad_norm": 12.642082214355469,
      "learning_rate": 3.571550473589897e-05,
      "loss": 1.3754,
      "step": 3850
    },
    {
      "epoch": 0.9477043947949914,
      "grad_norm": 18.940505981445312,
      "learning_rate": 3.5672838979435106e-05,
      "loss": 1.4168,
      "step": 3860
    },
    {
      "epoch": 0.9501595875276209,
      "grad_norm": 16.15582275390625,
      "learning_rate": 3.563017322297125e-05,
      "loss": 1.4073,
      "step": 3870
    },
    {
      "epoch": 0.9526147802602505,
      "grad_norm": 19.160362243652344,
      "learning_rate": 3.5587507466507384e-05,
      "loss": 1.5735,
      "step": 3880
    },
    {
      "epoch": 0.95506997299288,
      "grad_norm": 14.603316307067871,
      "learning_rate": 3.554484171004352e-05,
      "loss": 1.3051,
      "step": 3890
    },
    {
      "epoch": 0.9575251657255095,
      "grad_norm": 15.67354679107666,
      "learning_rate": 3.5502175953579655e-05,
      "loss": 1.3858,
      "step": 3900
    },
    {
      "epoch": 0.959980358458139,
      "grad_norm": 13.853238105773926,
      "learning_rate": 3.54595101971158e-05,
      "loss": 1.3754,
      "step": 3910
    },
    {
      "epoch": 0.9624355511907685,
      "grad_norm": 15.47938346862793,
      "learning_rate": 3.541684444065193e-05,
      "loss": 1.3991,
      "step": 3920
    },
    {
      "epoch": 0.964890743923398,
      "grad_norm": 18.40792465209961,
      "learning_rate": 3.5374178684188075e-05,
      "loss": 1.5644,
      "step": 3930
    },
    {
      "epoch": 0.9673459366560275,
      "grad_norm": 11.258827209472656,
      "learning_rate": 3.533151292772421e-05,
      "loss": 1.3955,
      "step": 3940
    },
    {
      "epoch": 0.969801129388657,
      "grad_norm": 20.005298614501953,
      "learning_rate": 3.528884717126035e-05,
      "loss": 1.3336,
      "step": 3950
    },
    {
      "epoch": 0.9722563221212865,
      "grad_norm": 18.969148635864258,
      "learning_rate": 3.524618141479649e-05,
      "loss": 1.3601,
      "step": 3960
    },
    {
      "epoch": 0.974711514853916,
      "grad_norm": 23.964031219482422,
      "learning_rate": 3.5203515658332625e-05,
      "loss": 1.4944,
      "step": 3970
    },
    {
      "epoch": 0.9771667075865456,
      "grad_norm": 13.570852279663086,
      "learning_rate": 3.516084990186877e-05,
      "loss": 1.3505,
      "step": 3980
    },
    {
      "epoch": 0.9796219003191751,
      "grad_norm": 17.09914779663086,
      "learning_rate": 3.5118184145404896e-05,
      "loss": 1.455,
      "step": 3990
    },
    {
      "epoch": 0.9820770930518046,
      "grad_norm": 23.882143020629883,
      "learning_rate": 3.507551838894104e-05,
      "loss": 1.4732,
      "step": 4000
    },
    {
      "epoch": 0.9845322857844341,
      "grad_norm": 22.119741439819336,
      "learning_rate": 3.5032852632477174e-05,
      "loss": 1.4546,
      "step": 4010
    },
    {
      "epoch": 0.9869874785170636,
      "grad_norm": 20.42696189880371,
      "learning_rate": 3.4990186876013316e-05,
      "loss": 1.2872,
      "step": 4020
    },
    {
      "epoch": 0.9894426712496931,
      "grad_norm": 20.417972564697266,
      "learning_rate": 3.494752111954945e-05,
      "loss": 1.4954,
      "step": 4030
    },
    {
      "epoch": 0.9918978639823226,
      "grad_norm": 14.989978790283203,
      "learning_rate": 3.490485536308559e-05,
      "loss": 1.4242,
      "step": 4040
    },
    {
      "epoch": 0.9943530567149521,
      "grad_norm": 16.999420166015625,
      "learning_rate": 3.486218960662172e-05,
      "loss": 1.3468,
      "step": 4050
    },
    {
      "epoch": 0.9968082494475816,
      "grad_norm": 21.98698616027832,
      "learning_rate": 3.4819523850157866e-05,
      "loss": 1.2797,
      "step": 4060
    },
    {
      "epoch": 0.9992634421802111,
      "grad_norm": 15.296296119689941,
      "learning_rate": 3.4776858093694e-05,
      "loss": 1.3384,
      "step": 4070
    },
    {
      "epoch": 1.0,
      "eval_exact": 56.076812936915694,
      "eval_f1": 60.25295142816102,
      "eval_hasAns_f1": 50.04778885063334,
      "eval_loss": 1.5530803203582764,
      "eval_runtime": 42.9593,
      "eval_samples_per_second": 276.378,
      "eval_steps_per_second": 4.33,
      "step": 4073
    },
    {
      "epoch": 1.0017186349128406,
      "grad_norm": 13.440691947937012,
      "learning_rate": 3.4734192337230144e-05,
      "loss": 1.2043,
      "step": 4080
    },
    {
      "epoch": 1.0041738276454701,
      "grad_norm": 31.7971134185791,
      "learning_rate": 3.4695793156412665e-05,
      "loss": 1.3303,
      "step": 4090
    },
    {
      "epoch": 1.0066290203780996,
      "grad_norm": 17.71030616760254,
      "learning_rate": 3.46531273999488e-05,
      "loss": 1.2737,
      "step": 4100
    },
    {
      "epoch": 1.0090842131107292,
      "grad_norm": 17.51396942138672,
      "learning_rate": 3.461046164348494e-05,
      "loss": 1.1703,
      "step": 4110
    },
    {
      "epoch": 1.0115394058433587,
      "grad_norm": 15.266483306884766,
      "learning_rate": 3.456779588702108e-05,
      "loss": 1.205,
      "step": 4120
    },
    {
      "epoch": 1.0139945985759882,
      "grad_norm": 14.894940376281738,
      "learning_rate": 3.4525130130557215e-05,
      "loss": 1.1108,
      "step": 4130
    },
    {
      "epoch": 1.0164497913086177,
      "grad_norm": 21.734342575073242,
      "learning_rate": 3.448246437409336e-05,
      "loss": 1.1565,
      "step": 4140
    },
    {
      "epoch": 1.0189049840412472,
      "grad_norm": 16.676477432250977,
      "learning_rate": 3.443979861762949e-05,
      "loss": 1.1265,
      "step": 4150
    },
    {
      "epoch": 1.0213601767738767,
      "grad_norm": 17.039270401000977,
      "learning_rate": 3.4397132861165635e-05,
      "loss": 1.2762,
      "step": 4160
    },
    {
      "epoch": 1.0238153695065062,
      "grad_norm": 19.902572631835938,
      "learning_rate": 3.435446710470177e-05,
      "loss": 1.1554,
      "step": 4170
    },
    {
      "epoch": 1.0262705622391357,
      "grad_norm": 20.9327335357666,
      "learning_rate": 3.4311801348237906e-05,
      "loss": 1.2731,
      "step": 4180
    },
    {
      "epoch": 1.0287257549717652,
      "grad_norm": 16.538467407226562,
      "learning_rate": 3.426913559177404e-05,
      "loss": 1.304,
      "step": 4190
    },
    {
      "epoch": 1.0311809477043947,
      "grad_norm": 13.430063247680664,
      "learning_rate": 3.4226469835310184e-05,
      "loss": 1.2183,
      "step": 4200
    },
    {
      "epoch": 1.0336361404370242,
      "grad_norm": 17.34056282043457,
      "learning_rate": 3.418380407884632e-05,
      "loss": 1.3373,
      "step": 4210
    },
    {
      "epoch": 1.0360913331696537,
      "grad_norm": 16.11901092529297,
      "learning_rate": 3.414113832238246e-05,
      "loss": 1.2174,
      "step": 4220
    },
    {
      "epoch": 1.0385465259022832,
      "grad_norm": 12.624663352966309,
      "learning_rate": 3.409847256591859e-05,
      "loss": 1.1884,
      "step": 4230
    },
    {
      "epoch": 1.0410017186349128,
      "grad_norm": 22.357179641723633,
      "learning_rate": 3.4055806809454734e-05,
      "loss": 1.2651,
      "step": 4240
    },
    {
      "epoch": 1.0434569113675423,
      "grad_norm": 16.567241668701172,
      "learning_rate": 3.401314105299087e-05,
      "loss": 1.1664,
      "step": 4250
    },
    {
      "epoch": 1.0459121041001718,
      "grad_norm": 14.323908805847168,
      "learning_rate": 3.397047529652701e-05,
      "loss": 1.1616,
      "step": 4260
    },
    {
      "epoch": 1.0483672968328013,
      "grad_norm": 17.585485458374023,
      "learning_rate": 3.392780954006315e-05,
      "loss": 1.3943,
      "step": 4270
    },
    {
      "epoch": 1.0508224895654308,
      "grad_norm": 14.620322227478027,
      "learning_rate": 3.388514378359928e-05,
      "loss": 1.2655,
      "step": 4280
    },
    {
      "epoch": 1.0532776822980603,
      "grad_norm": 16.63777732849121,
      "learning_rate": 3.384247802713542e-05,
      "loss": 1.2849,
      "step": 4290
    },
    {
      "epoch": 1.05573287503069,
      "grad_norm": 16.890796661376953,
      "learning_rate": 3.379981227067156e-05,
      "loss": 1.0946,
      "step": 4300
    },
    {
      "epoch": 1.0581880677633193,
      "grad_norm": 26.337757110595703,
      "learning_rate": 3.3757146514207697e-05,
      "loss": 1.33,
      "step": 4310
    },
    {
      "epoch": 1.060643260495949,
      "grad_norm": 17.83364486694336,
      "learning_rate": 3.371448075774384e-05,
      "loss": 1.4898,
      "step": 4320
    },
    {
      "epoch": 1.0630984532285785,
      "grad_norm": 25.650270462036133,
      "learning_rate": 3.3671815001279975e-05,
      "loss": 1.2333,
      "step": 4330
    },
    {
      "epoch": 1.065553645961208,
      "grad_norm": 18.61801528930664,
      "learning_rate": 3.362914924481611e-05,
      "loss": 1.3607,
      "step": 4340
    },
    {
      "epoch": 1.0680088386938376,
      "grad_norm": 14.939010620117188,
      "learning_rate": 3.358648348835225e-05,
      "loss": 1.1884,
      "step": 4350
    },
    {
      "epoch": 1.070464031426467,
      "grad_norm": 15.181191444396973,
      "learning_rate": 3.354381773188839e-05,
      "loss": 1.2618,
      "step": 4360
    },
    {
      "epoch": 1.0729192241590966,
      "grad_norm": 19.44411849975586,
      "learning_rate": 3.350115197542453e-05,
      "loss": 1.1443,
      "step": 4370
    },
    {
      "epoch": 1.075374416891726,
      "grad_norm": 16.477991104125977,
      "learning_rate": 3.3458486218960666e-05,
      "loss": 1.2214,
      "step": 4380
    },
    {
      "epoch": 1.0778296096243556,
      "grad_norm": 19.594146728515625,
      "learning_rate": 3.34158204624968e-05,
      "loss": 1.3223,
      "step": 4390
    },
    {
      "epoch": 1.080284802356985,
      "grad_norm": 23.349462509155273,
      "learning_rate": 3.337315470603294e-05,
      "loss": 1.2374,
      "step": 4400
    },
    {
      "epoch": 1.0827399950896146,
      "grad_norm": 16.668453216552734,
      "learning_rate": 3.333048894956908e-05,
      "loss": 1.1543,
      "step": 4410
    },
    {
      "epoch": 1.0851951878222441,
      "grad_norm": 18.387048721313477,
      "learning_rate": 3.3287823193105216e-05,
      "loss": 1.2051,
      "step": 4420
    },
    {
      "epoch": 1.0876503805548736,
      "grad_norm": 16.614625930786133,
      "learning_rate": 3.324515743664136e-05,
      "loss": 1.1601,
      "step": 4430
    },
    {
      "epoch": 1.0901055732875031,
      "grad_norm": 31.0953311920166,
      "learning_rate": 3.320249168017749e-05,
      "loss": 1.224,
      "step": 4440
    },
    {
      "epoch": 1.0925607660201326,
      "grad_norm": 19.968175888061523,
      "learning_rate": 3.315982592371363e-05,
      "loss": 1.3695,
      "step": 4450
    },
    {
      "epoch": 1.0950159587527621,
      "grad_norm": 16.687204360961914,
      "learning_rate": 3.3117160167249765e-05,
      "loss": 1.3202,
      "step": 4460
    },
    {
      "epoch": 1.0974711514853916,
      "grad_norm": 15.093268394470215,
      "learning_rate": 3.307449441078591e-05,
      "loss": 1.1998,
      "step": 4470
    },
    {
      "epoch": 1.0999263442180212,
      "grad_norm": 17.116275787353516,
      "learning_rate": 3.303182865432204e-05,
      "loss": 1.2588,
      "step": 4480
    },
    {
      "epoch": 1.1023815369506507,
      "grad_norm": 15.48996639251709,
      "learning_rate": 3.298916289785818e-05,
      "loss": 1.235,
      "step": 4490
    },
    {
      "epoch": 1.1048367296832802,
      "grad_norm": 17.106525421142578,
      "learning_rate": 3.2946497141394314e-05,
      "loss": 1.2045,
      "step": 4500
    },
    {
      "epoch": 1.1072919224159097,
      "grad_norm": 18.670249938964844,
      "learning_rate": 3.290383138493046e-05,
      "loss": 1.2376,
      "step": 4510
    },
    {
      "epoch": 1.1097471151485392,
      "grad_norm": 19.125490188598633,
      "learning_rate": 3.286116562846659e-05,
      "loss": 1.1583,
      "step": 4520
    },
    {
      "epoch": 1.1122023078811687,
      "grad_norm": 16.55862045288086,
      "learning_rate": 3.2818499872002735e-05,
      "loss": 1.1788,
      "step": 4530
    },
    {
      "epoch": 1.1146575006137982,
      "grad_norm": 23.70785140991211,
      "learning_rate": 3.277583411553887e-05,
      "loss": 1.2531,
      "step": 4540
    },
    {
      "epoch": 1.1171126933464277,
      "grad_norm": 21.115304946899414,
      "learning_rate": 3.2733168359075006e-05,
      "loss": 1.3013,
      "step": 4550
    },
    {
      "epoch": 1.1195678860790572,
      "grad_norm": 16.760543823242188,
      "learning_rate": 3.269050260261114e-05,
      "loss": 1.2,
      "step": 4560
    },
    {
      "epoch": 1.1220230788116867,
      "grad_norm": 20.46354866027832,
      "learning_rate": 3.2647836846147284e-05,
      "loss": 1.2223,
      "step": 4570
    },
    {
      "epoch": 1.1244782715443162,
      "grad_norm": 16.637300491333008,
      "learning_rate": 3.260517108968342e-05,
      "loss": 1.1245,
      "step": 4580
    },
    {
      "epoch": 1.1269334642769457,
      "grad_norm": 20.33194923400879,
      "learning_rate": 3.256250533321956e-05,
      "loss": 1.4851,
      "step": 4590
    },
    {
      "epoch": 1.1293886570095752,
      "grad_norm": 19.91619300842285,
      "learning_rate": 3.25198395767557e-05,
      "loss": 1.3068,
      "step": 4600
    },
    {
      "epoch": 1.1318438497422048,
      "grad_norm": 20.327987670898438,
      "learning_rate": 3.247717382029183e-05,
      "loss": 1.298,
      "step": 4610
    },
    {
      "epoch": 1.1342990424748343,
      "grad_norm": 20.791990280151367,
      "learning_rate": 3.2434508063827976e-05,
      "loss": 1.0852,
      "step": 4620
    },
    {
      "epoch": 1.1367542352074638,
      "grad_norm": 16.936479568481445,
      "learning_rate": 3.239184230736411e-05,
      "loss": 1.1655,
      "step": 4630
    },
    {
      "epoch": 1.1392094279400933,
      "grad_norm": 21.342060089111328,
      "learning_rate": 3.2349176550900254e-05,
      "loss": 1.237,
      "step": 4640
    },
    {
      "epoch": 1.1416646206727228,
      "grad_norm": 13.984390258789062,
      "learning_rate": 3.230651079443638e-05,
      "loss": 1.2492,
      "step": 4650
    },
    {
      "epoch": 1.1441198134053523,
      "grad_norm": 22.966386795043945,
      "learning_rate": 3.2263845037972525e-05,
      "loss": 1.3159,
      "step": 4660
    },
    {
      "epoch": 1.1465750061379818,
      "grad_norm": 24.29118537902832,
      "learning_rate": 3.222117928150866e-05,
      "loss": 1.348,
      "step": 4670
    },
    {
      "epoch": 1.1490301988706113,
      "grad_norm": 15.037768363952637,
      "learning_rate": 3.21785135250448e-05,
      "loss": 1.2139,
      "step": 4680
    },
    {
      "epoch": 1.1514853916032408,
      "grad_norm": 24.278268814086914,
      "learning_rate": 3.213584776858094e-05,
      "loss": 1.2032,
      "step": 4690
    },
    {
      "epoch": 1.1539405843358703,
      "grad_norm": 13.886629104614258,
      "learning_rate": 3.209318201211708e-05,
      "loss": 1.1923,
      "step": 4700
    },
    {
      "epoch": 1.1563957770684998,
      "grad_norm": 25.21141815185547,
      "learning_rate": 3.205051625565321e-05,
      "loss": 1.2451,
      "step": 4710
    },
    {
      "epoch": 1.1588509698011293,
      "grad_norm": 16.749420166015625,
      "learning_rate": 3.200785049918935e-05,
      "loss": 1.3248,
      "step": 4720
    },
    {
      "epoch": 1.1613061625337588,
      "grad_norm": 14.088061332702637,
      "learning_rate": 3.196518474272549e-05,
      "loss": 1.3553,
      "step": 4730
    },
    {
      "epoch": 1.1637613552663884,
      "grad_norm": 23.768959045410156,
      "learning_rate": 3.192251898626163e-05,
      "loss": 1.2868,
      "step": 4740
    },
    {
      "epoch": 1.1662165479990179,
      "grad_norm": 15.013630867004395,
      "learning_rate": 3.1879853229797766e-05,
      "loss": 1.0598,
      "step": 4750
    },
    {
      "epoch": 1.1686717407316474,
      "grad_norm": 19.023738861083984,
      "learning_rate": 3.18371874733339e-05,
      "loss": 1.1344,
      "step": 4760
    },
    {
      "epoch": 1.1711269334642769,
      "grad_norm": 21.114978790283203,
      "learning_rate": 3.179452171687004e-05,
      "loss": 1.4984,
      "step": 4770
    },
    {
      "epoch": 1.1735821261969064,
      "grad_norm": 17.519676208496094,
      "learning_rate": 3.175185596040618e-05,
      "loss": 1.0202,
      "step": 4780
    },
    {
      "epoch": 1.176037318929536,
      "grad_norm": 18.802677154541016,
      "learning_rate": 3.1709190203942315e-05,
      "loss": 1.1502,
      "step": 4790
    },
    {
      "epoch": 1.1784925116621654,
      "grad_norm": 29.639192581176758,
      "learning_rate": 3.166652444747846e-05,
      "loss": 1.1952,
      "step": 4800
    },
    {
      "epoch": 1.180947704394795,
      "grad_norm": 14.95936107635498,
      "learning_rate": 3.1623858691014593e-05,
      "loss": 1.2262,
      "step": 4810
    },
    {
      "epoch": 1.1834028971274244,
      "grad_norm": 25.420671463012695,
      "learning_rate": 3.158119293455073e-05,
      "loss": 1.1253,
      "step": 4820
    },
    {
      "epoch": 1.185858089860054,
      "grad_norm": 26.51723861694336,
      "learning_rate": 3.153852717808687e-05,
      "loss": 1.1929,
      "step": 4830
    },
    {
      "epoch": 1.1883132825926834,
      "grad_norm": 22.317150115966797,
      "learning_rate": 3.149586142162301e-05,
      "loss": 1.3802,
      "step": 4840
    },
    {
      "epoch": 1.1907684753253132,
      "grad_norm": 21.174013137817383,
      "learning_rate": 3.145319566515915e-05,
      "loss": 1.2151,
      "step": 4850
    },
    {
      "epoch": 1.1932236680579424,
      "grad_norm": 21.02806282043457,
      "learning_rate": 3.1410529908695285e-05,
      "loss": 1.2618,
      "step": 4860
    },
    {
      "epoch": 1.1956788607905722,
      "grad_norm": 18.415332794189453,
      "learning_rate": 3.136786415223142e-05,
      "loss": 1.171,
      "step": 4870
    },
    {
      "epoch": 1.1981340535232015,
      "grad_norm": 22.151044845581055,
      "learning_rate": 3.1325198395767556e-05,
      "loss": 1.331,
      "step": 4880
    },
    {
      "epoch": 1.2005892462558312,
      "grad_norm": 17.93316650390625,
      "learning_rate": 3.12825326393037e-05,
      "loss": 1.3499,
      "step": 4890
    },
    {
      "epoch": 1.2030444389884605,
      "grad_norm": 16.974990844726562,
      "learning_rate": 3.1239866882839834e-05,
      "loss": 1.2607,
      "step": 4900
    },
    {
      "epoch": 1.2054996317210902,
      "grad_norm": 14.325870513916016,
      "learning_rate": 3.119720112637598e-05,
      "loss": 1.1478,
      "step": 4910
    },
    {
      "epoch": 1.2079548244537197,
      "grad_norm": 21.942264556884766,
      "learning_rate": 3.1154535369912106e-05,
      "loss": 1.1729,
      "step": 4920
    },
    {
      "epoch": 1.2104100171863492,
      "grad_norm": 19.942724227905273,
      "learning_rate": 3.111186961344825e-05,
      "loss": 1.2147,
      "step": 4930
    },
    {
      "epoch": 1.2128652099189787,
      "grad_norm": 21.342836380004883,
      "learning_rate": 3.1069203856984384e-05,
      "loss": 1.1549,
      "step": 4940
    },
    {
      "epoch": 1.2153204026516082,
      "grad_norm": 15.293014526367188,
      "learning_rate": 3.1026538100520526e-05,
      "loss": 1.2669,
      "step": 4950
    },
    {
      "epoch": 1.2177755953842377,
      "grad_norm": 21.699262619018555,
      "learning_rate": 3.098387234405666e-05,
      "loss": 1.3946,
      "step": 4960
    },
    {
      "epoch": 1.2202307881168672,
      "grad_norm": 18.434093475341797,
      "learning_rate": 3.09412065875928e-05,
      "loss": 1.2822,
      "step": 4970
    },
    {
      "epoch": 1.2226859808494968,
      "grad_norm": 18.418376922607422,
      "learning_rate": 3.089854083112893e-05,
      "loss": 1.2522,
      "step": 4980
    },
    {
      "epoch": 1.2251411735821263,
      "grad_norm": 16.68257713317871,
      "learning_rate": 3.0855875074665076e-05,
      "loss": 1.1159,
      "step": 4990
    },
    {
      "epoch": 1.2275963663147558,
      "grad_norm": 22.293989181518555,
      "learning_rate": 3.081320931820121e-05,
      "loss": 1.1523,
      "step": 5000
    },
    {
      "epoch": 1.2300515590473853,
      "grad_norm": 13.928030967712402,
      "learning_rate": 3.0770543561737354e-05,
      "loss": 1.0878,
      "step": 5010
    },
    {
      "epoch": 1.2325067517800148,
      "grad_norm": 12.774408340454102,
      "learning_rate": 3.072787780527349e-05,
      "loss": 1.2531,
      "step": 5020
    },
    {
      "epoch": 1.2349619445126443,
      "grad_norm": 19.96636199951172,
      "learning_rate": 3.0685212048809625e-05,
      "loss": 1.1752,
      "step": 5030
    },
    {
      "epoch": 1.2374171372452738,
      "grad_norm": 16.07071304321289,
      "learning_rate": 3.064254629234577e-05,
      "loss": 1.2438,
      "step": 5040
    },
    {
      "epoch": 1.2398723299779033,
      "grad_norm": 15.680068016052246,
      "learning_rate": 3.05998805358819e-05,
      "loss": 1.2755,
      "step": 5050
    },
    {
      "epoch": 1.2423275227105328,
      "grad_norm": 19.36787223815918,
      "learning_rate": 3.0557214779418045e-05,
      "loss": 1.2723,
      "step": 5060
    },
    {
      "epoch": 1.2447827154431623,
      "grad_norm": 12.79963207244873,
      "learning_rate": 3.051454902295418e-05,
      "loss": 1.1318,
      "step": 5070
    },
    {
      "epoch": 1.2472379081757918,
      "grad_norm": 25.065963745117188,
      "learning_rate": 3.0471883266490313e-05,
      "loss": 1.2038,
      "step": 5080
    },
    {
      "epoch": 1.2496931009084213,
      "grad_norm": 23.30496597290039,
      "learning_rate": 3.0429217510026452e-05,
      "loss": 1.192,
      "step": 5090
    },
    {
      "epoch": 1.2521482936410508,
      "grad_norm": 16.917137145996094,
      "learning_rate": 3.038655175356259e-05,
      "loss": 1.2719,
      "step": 5100
    },
    {
      "epoch": 1.2546034863736804,
      "grad_norm": 14.331111907958984,
      "learning_rate": 3.034388599709873e-05,
      "loss": 1.1492,
      "step": 5110
    },
    {
      "epoch": 1.2570586791063099,
      "grad_norm": 17.46131134033203,
      "learning_rate": 3.030122024063487e-05,
      "loss": 1.2393,
      "step": 5120
    },
    {
      "epoch": 1.2595138718389394,
      "grad_norm": 20.005380630493164,
      "learning_rate": 3.0258554484171005e-05,
      "loss": 1.3923,
      "step": 5130
    },
    {
      "epoch": 1.2619690645715689,
      "grad_norm": 19.153762817382812,
      "learning_rate": 3.0215888727707144e-05,
      "loss": 1.25,
      "step": 5140
    },
    {
      "epoch": 1.2644242573041984,
      "grad_norm": 24.345243453979492,
      "learning_rate": 3.0173222971243283e-05,
      "loss": 1.3139,
      "step": 5150
    },
    {
      "epoch": 1.266879450036828,
      "grad_norm": 16.486072540283203,
      "learning_rate": 3.013055721477942e-05,
      "loss": 1.1094,
      "step": 5160
    },
    {
      "epoch": 1.2693346427694574,
      "grad_norm": 20.314300537109375,
      "learning_rate": 3.0087891458315558e-05,
      "loss": 1.139,
      "step": 5170
    },
    {
      "epoch": 1.271789835502087,
      "grad_norm": 21.40546989440918,
      "learning_rate": 3.0045225701851693e-05,
      "loss": 1.2269,
      "step": 5180
    },
    {
      "epoch": 1.2742450282347164,
      "grad_norm": 15.908699989318848,
      "learning_rate": 3.0002559945387832e-05,
      "loss": 1.1143,
      "step": 5190
    },
    {
      "epoch": 1.276700220967346,
      "grad_norm": 22.111787796020508,
      "learning_rate": 2.995989418892397e-05,
      "loss": 1.2201,
      "step": 5200
    },
    {
      "epoch": 1.2791554136999754,
      "grad_norm": 24.03329849243164,
      "learning_rate": 2.991722843246011e-05,
      "loss": 1.2677,
      "step": 5210
    },
    {
      "epoch": 1.281610606432605,
      "grad_norm": 16.76061248779297,
      "learning_rate": 2.987456267599625e-05,
      "loss": 1.223,
      "step": 5220
    },
    {
      "epoch": 1.2840657991652344,
      "grad_norm": 20.982839584350586,
      "learning_rate": 2.9831896919532388e-05,
      "loss": 1.0723,
      "step": 5230
    },
    {
      "epoch": 1.286520991897864,
      "grad_norm": 17.414812088012695,
      "learning_rate": 2.978923116306852e-05,
      "loss": 1.2693,
      "step": 5240
    },
    {
      "epoch": 1.2889761846304935,
      "grad_norm": 15.318415641784668,
      "learning_rate": 2.974656540660466e-05,
      "loss": 1.3132,
      "step": 5250
    },
    {
      "epoch": 1.291431377363123,
      "grad_norm": 15.024585723876953,
      "learning_rate": 2.97038996501408e-05,
      "loss": 1.2841,
      "step": 5260
    },
    {
      "epoch": 1.2938865700957525,
      "grad_norm": 15.634686470031738,
      "learning_rate": 2.9661233893676938e-05,
      "loss": 1.2674,
      "step": 5270
    },
    {
      "epoch": 1.296341762828382,
      "grad_norm": 22.0808048248291,
      "learning_rate": 2.9618568137213077e-05,
      "loss": 1.1624,
      "step": 5280
    },
    {
      "epoch": 1.2987969555610115,
      "grad_norm": 15.332256317138672,
      "learning_rate": 2.957590238074921e-05,
      "loss": 1.2056,
      "step": 5290
    },
    {
      "epoch": 1.301252148293641,
      "grad_norm": 15.256704330444336,
      "learning_rate": 2.9533236624285348e-05,
      "loss": 1.243,
      "step": 5300
    },
    {
      "epoch": 1.3037073410262705,
      "grad_norm": 22.8935489654541,
      "learning_rate": 2.9490570867821487e-05,
      "loss": 1.3489,
      "step": 5310
    },
    {
      "epoch": 1.3061625337589,
      "grad_norm": 15.81343936920166,
      "learning_rate": 2.9447905111357626e-05,
      "loss": 1.2463,
      "step": 5320
    },
    {
      "epoch": 1.3086177264915295,
      "grad_norm": 21.274412155151367,
      "learning_rate": 2.9405239354893765e-05,
      "loss": 1.2222,
      "step": 5330
    },
    {
      "epoch": 1.311072919224159,
      "grad_norm": 18.35671615600586,
      "learning_rate": 2.9362573598429897e-05,
      "loss": 1.3262,
      "step": 5340
    },
    {
      "epoch": 1.3135281119567885,
      "grad_norm": 20.555082321166992,
      "learning_rate": 2.9319907841966036e-05,
      "loss": 1.1814,
      "step": 5350
    },
    {
      "epoch": 1.315983304689418,
      "grad_norm": 14.081962585449219,
      "learning_rate": 2.9277242085502175e-05,
      "loss": 1.121,
      "step": 5360
    },
    {
      "epoch": 1.3184384974220476,
      "grad_norm": 14.54621410369873,
      "learning_rate": 2.9234576329038314e-05,
      "loss": 1.3031,
      "step": 5370
    },
    {
      "epoch": 1.3208936901546773,
      "grad_norm": 23.73992156982422,
      "learning_rate": 2.9191910572574453e-05,
      "loss": 1.1424,
      "step": 5380
    },
    {
      "epoch": 1.3233488828873066,
      "grad_norm": 25.370208740234375,
      "learning_rate": 2.9149244816110592e-05,
      "loss": 1.3649,
      "step": 5390
    },
    {
      "epoch": 1.3258040756199363,
      "grad_norm": 19.588876724243164,
      "learning_rate": 2.9106579059646728e-05,
      "loss": 1.2364,
      "step": 5400
    },
    {
      "epoch": 1.3282592683525656,
      "grad_norm": 17.875722885131836,
      "learning_rate": 2.9063913303182867e-05,
      "loss": 1.3374,
      "step": 5410
    },
    {
      "epoch": 1.3307144610851953,
      "grad_norm": 20.367813110351562,
      "learning_rate": 2.9021247546719006e-05,
      "loss": 1.2614,
      "step": 5420
    },
    {
      "epoch": 1.3331696538178246,
      "grad_norm": 16.94515037536621,
      "learning_rate": 2.8978581790255145e-05,
      "loss": 1.183,
      "step": 5430
    },
    {
      "epoch": 1.3356248465504543,
      "grad_norm": 21.00985336303711,
      "learning_rate": 2.8935916033791284e-05,
      "loss": 1.294,
      "step": 5440
    },
    {
      "epoch": 1.3380800392830836,
      "grad_norm": 12.761407852172852,
      "learning_rate": 2.8893250277327416e-05,
      "loss": 1.3108,
      "step": 5450
    },
    {
      "epoch": 1.3405352320157133,
      "grad_norm": 16.102203369140625,
      "learning_rate": 2.8850584520863555e-05,
      "loss": 1.2384,
      "step": 5460
    },
    {
      "epoch": 1.3429904247483426,
      "grad_norm": 19.841384887695312,
      "learning_rate": 2.8807918764399694e-05,
      "loss": 1.3942,
      "step": 5470
    },
    {
      "epoch": 1.3454456174809724,
      "grad_norm": 18.78371810913086,
      "learning_rate": 2.8765253007935833e-05,
      "loss": 1.2493,
      "step": 5480
    },
    {
      "epoch": 1.3479008102136016,
      "grad_norm": 20.785730361938477,
      "learning_rate": 2.8722587251471972e-05,
      "loss": 1.2772,
      "step": 5490
    },
    {
      "epoch": 1.3503560029462314,
      "grad_norm": 18.789737701416016,
      "learning_rate": 2.8679921495008105e-05,
      "loss": 1.2961,
      "step": 5500
    },
    {
      "epoch": 1.3528111956788607,
      "grad_norm": 22.050901412963867,
      "learning_rate": 2.8637255738544244e-05,
      "loss": 1.2751,
      "step": 5510
    },
    {
      "epoch": 1.3552663884114904,
      "grad_norm": 17.50777244567871,
      "learning_rate": 2.8594589982080383e-05,
      "loss": 1.1951,
      "step": 5520
    },
    {
      "epoch": 1.35772158114412,
      "grad_norm": 17.511178970336914,
      "learning_rate": 2.855192422561652e-05,
      "loss": 1.0271,
      "step": 5530
    },
    {
      "epoch": 1.3601767738767494,
      "grad_norm": 20.018720626831055,
      "learning_rate": 2.850925846915266e-05,
      "loss": 1.2902,
      "step": 5540
    },
    {
      "epoch": 1.362631966609379,
      "grad_norm": 17.419326782226562,
      "learning_rate": 2.84665927126888e-05,
      "loss": 1.2216,
      "step": 5550
    },
    {
      "epoch": 1.3650871593420084,
      "grad_norm": 17.945415496826172,
      "learning_rate": 2.8423926956224932e-05,
      "loss": 1.2479,
      "step": 5560
    },
    {
      "epoch": 1.367542352074638,
      "grad_norm": 15.247157096862793,
      "learning_rate": 2.838126119976107e-05,
      "loss": 1.2501,
      "step": 5570
    },
    {
      "epoch": 1.3699975448072674,
      "grad_norm": 12.50341796875,
      "learning_rate": 2.833859544329721e-05,
      "loss": 1.3211,
      "step": 5580
    },
    {
      "epoch": 1.372452737539897,
      "grad_norm": 15.89215087890625,
      "learning_rate": 2.829592968683335e-05,
      "loss": 1.2429,
      "step": 5590
    },
    {
      "epoch": 1.3749079302725264,
      "grad_norm": 22.214197158813477,
      "learning_rate": 2.8253263930369488e-05,
      "loss": 1.2882,
      "step": 5600
    },
    {
      "epoch": 1.377363123005156,
      "grad_norm": 12.595281600952148,
      "learning_rate": 2.8210598173905624e-05,
      "loss": 1.2039,
      "step": 5610
    },
    {
      "epoch": 1.3798183157377855,
      "grad_norm": 16.69274139404297,
      "learning_rate": 2.8167932417441763e-05,
      "loss": 1.0958,
      "step": 5620
    },
    {
      "epoch": 1.382273508470415,
      "grad_norm": 15.002397537231445,
      "learning_rate": 2.8125266660977902e-05,
      "loss": 1.2685,
      "step": 5630
    },
    {
      "epoch": 1.3847287012030445,
      "grad_norm": 17.07223892211914,
      "learning_rate": 2.808260090451404e-05,
      "loss": 1.305,
      "step": 5640
    },
    {
      "epoch": 1.387183893935674,
      "grad_norm": 19.754215240478516,
      "learning_rate": 2.803993514805018e-05,
      "loss": 1.3612,
      "step": 5650
    },
    {
      "epoch": 1.3896390866683035,
      "grad_norm": 13.081413269042969,
      "learning_rate": 2.7997269391586312e-05,
      "loss": 1.3814,
      "step": 5660
    },
    {
      "epoch": 1.392094279400933,
      "grad_norm": 24.225244522094727,
      "learning_rate": 2.795460363512245e-05,
      "loss": 1.2398,
      "step": 5670
    },
    {
      "epoch": 1.3945494721335625,
      "grad_norm": 17.460100173950195,
      "learning_rate": 2.791193787865859e-05,
      "loss": 1.2142,
      "step": 5680
    },
    {
      "epoch": 1.397004664866192,
      "grad_norm": 28.870269775390625,
      "learning_rate": 2.786927212219473e-05,
      "loss": 1.3386,
      "step": 5690
    },
    {
      "epoch": 1.3994598575988215,
      "grad_norm": 11.991888046264648,
      "learning_rate": 2.7826606365730868e-05,
      "loss": 1.2707,
      "step": 5700
    },
    {
      "epoch": 1.401915050331451,
      "grad_norm": 20.07191276550293,
      "learning_rate": 2.7783940609267007e-05,
      "loss": 1.2587,
      "step": 5710
    },
    {
      "epoch": 1.4043702430640805,
      "grad_norm": 21.745872497558594,
      "learning_rate": 2.774127485280314e-05,
      "loss": 1.207,
      "step": 5720
    },
    {
      "epoch": 1.40682543579671,
      "grad_norm": 16.246938705444336,
      "learning_rate": 2.769860909633928e-05,
      "loss": 1.1606,
      "step": 5730
    },
    {
      "epoch": 1.4092806285293396,
      "grad_norm": 24.910903930664062,
      "learning_rate": 2.7655943339875417e-05,
      "loss": 1.1319,
      "step": 5740
    },
    {
      "epoch": 1.411735821261969,
      "grad_norm": 20.6267147064209,
      "learning_rate": 2.7613277583411556e-05,
      "loss": 1.2128,
      "step": 5750
    },
    {
      "epoch": 1.4141910139945986,
      "grad_norm": 21.433780670166016,
      "learning_rate": 2.7570611826947695e-05,
      "loss": 1.2662,
      "step": 5760
    },
    {
      "epoch": 1.416646206727228,
      "grad_norm": 17.325119018554688,
      "learning_rate": 2.7527946070483828e-05,
      "loss": 1.2408,
      "step": 5770
    },
    {
      "epoch": 1.4191013994598576,
      "grad_norm": 19.21756935119629,
      "learning_rate": 2.7485280314019967e-05,
      "loss": 1.2154,
      "step": 5780
    },
    {
      "epoch": 1.421556592192487,
      "grad_norm": 22.905738830566406,
      "learning_rate": 2.7442614557556106e-05,
      "loss": 1.2813,
      "step": 5790
    },
    {
      "epoch": 1.4240117849251166,
      "grad_norm": 14.062265396118164,
      "learning_rate": 2.7399948801092245e-05,
      "loss": 1.2557,
      "step": 5800
    },
    {
      "epoch": 1.426466977657746,
      "grad_norm": 20.79661750793457,
      "learning_rate": 2.7357283044628384e-05,
      "loss": 1.2797,
      "step": 5810
    },
    {
      "epoch": 1.4289221703903756,
      "grad_norm": 22.486574172973633,
      "learning_rate": 2.731461728816452e-05,
      "loss": 1.1361,
      "step": 5820
    },
    {
      "epoch": 1.4313773631230051,
      "grad_norm": 14.716591835021973,
      "learning_rate": 2.727195153170066e-05,
      "loss": 1.1527,
      "step": 5830
    },
    {
      "epoch": 1.4338325558556346,
      "grad_norm": 19.993873596191406,
      "learning_rate": 2.7229285775236797e-05,
      "loss": 1.2544,
      "step": 5840
    },
    {
      "epoch": 1.4362877485882641,
      "grad_norm": 17.40132713317871,
      "learning_rate": 2.7186620018772933e-05,
      "loss": 1.3129,
      "step": 5850
    },
    {
      "epoch": 1.4387429413208936,
      "grad_norm": 21.144628524780273,
      "learning_rate": 2.7143954262309072e-05,
      "loss": 1.3449,
      "step": 5860
    },
    {
      "epoch": 1.4411981340535231,
      "grad_norm": 18.5234432220459,
      "learning_rate": 2.710128850584521e-05,
      "loss": 1.3859,
      "step": 5870
    },
    {
      "epoch": 1.4436533267861527,
      "grad_norm": 13.545533180236816,
      "learning_rate": 2.7058622749381347e-05,
      "loss": 1.3936,
      "step": 5880
    },
    {
      "epoch": 1.4461085195187822,
      "grad_norm": 15.254669189453125,
      "learning_rate": 2.7015956992917486e-05,
      "loss": 1.2242,
      "step": 5890
    },
    {
      "epoch": 1.4485637122514117,
      "grad_norm": 13.843425750732422,
      "learning_rate": 2.6973291236453625e-05,
      "loss": 1.2215,
      "step": 5900
    },
    {
      "epoch": 1.4510189049840412,
      "grad_norm": 18.311565399169922,
      "learning_rate": 2.6930625479989764e-05,
      "loss": 1.2221,
      "step": 5910
    },
    {
      "epoch": 1.4534740977166707,
      "grad_norm": 17.51752281188965,
      "learning_rate": 2.6887959723525903e-05,
      "loss": 1.273,
      "step": 5920
    },
    {
      "epoch": 1.4559292904493002,
      "grad_norm": 20.550201416015625,
      "learning_rate": 2.6845293967062035e-05,
      "loss": 1.2037,
      "step": 5930
    },
    {
      "epoch": 1.4583844831819297,
      "grad_norm": 22.914108276367188,
      "learning_rate": 2.6802628210598174e-05,
      "loss": 1.2954,
      "step": 5940
    },
    {
      "epoch": 1.4608396759145592,
      "grad_norm": 17.386775970458984,
      "learning_rate": 2.6759962454134313e-05,
      "loss": 1.2396,
      "step": 5950
    },
    {
      "epoch": 1.4632948686471887,
      "grad_norm": 19.3266658782959,
      "learning_rate": 2.6717296697670452e-05,
      "loss": 1.4048,
      "step": 5960
    },
    {
      "epoch": 1.4657500613798184,
      "grad_norm": 15.085927963256836,
      "learning_rate": 2.667463094120659e-05,
      "loss": 1.3134,
      "step": 5970
    },
    {
      "epoch": 1.4682052541124477,
      "grad_norm": 17.469024658203125,
      "learning_rate": 2.6631965184742723e-05,
      "loss": 1.2819,
      "step": 5980
    },
    {
      "epoch": 1.4706604468450775,
      "grad_norm": 16.921186447143555,
      "learning_rate": 2.6589299428278862e-05,
      "loss": 1.2929,
      "step": 5990
    },
    {
      "epoch": 1.4731156395777067,
      "grad_norm": 20.57211685180664,
      "learning_rate": 2.6546633671815e-05,
      "loss": 1.1685,
      "step": 6000
    },
    {
      "epoch": 1.4755708323103365,
      "grad_norm": 15.207544326782227,
      "learning_rate": 2.650396791535114e-05,
      "loss": 1.2948,
      "step": 6010
    },
    {
      "epoch": 1.4780260250429658,
      "grad_norm": 15.864477157592773,
      "learning_rate": 2.646130215888728e-05,
      "loss": 1.2322,
      "step": 6020
    },
    {
      "epoch": 1.4804812177755955,
      "grad_norm": 19.397411346435547,
      "learning_rate": 2.641863640242342e-05,
      "loss": 1.2121,
      "step": 6030
    },
    {
      "epoch": 1.4829364105082248,
      "grad_norm": 26.272281646728516,
      "learning_rate": 2.637597064595955e-05,
      "loss": 1.198,
      "step": 6040
    },
    {
      "epoch": 1.4853916032408545,
      "grad_norm": 17.55596923828125,
      "learning_rate": 2.633330488949569e-05,
      "loss": 1.2516,
      "step": 6050
    },
    {
      "epoch": 1.4878467959734838,
      "grad_norm": 16.078960418701172,
      "learning_rate": 2.629063913303183e-05,
      "loss": 1.2738,
      "step": 6060
    },
    {
      "epoch": 1.4903019887061135,
      "grad_norm": 17.377073287963867,
      "learning_rate": 2.6247973376567968e-05,
      "loss": 1.1131,
      "step": 6070
    },
    {
      "epoch": 1.4927571814387428,
      "grad_norm": 24.83179473876953,
      "learning_rate": 2.6205307620104107e-05,
      "loss": 1.3241,
      "step": 6080
    },
    {
      "epoch": 1.4952123741713725,
      "grad_norm": 19.42167854309082,
      "learning_rate": 2.6162641863640243e-05,
      "loss": 1.4155,
      "step": 6090
    },
    {
      "epoch": 1.497667566904002,
      "grad_norm": 16.220556259155273,
      "learning_rate": 2.611997610717638e-05,
      "loss": 1.2195,
      "step": 6100
    },
    {
      "epoch": 1.5001227596366316,
      "grad_norm": 14.435432434082031,
      "learning_rate": 2.607731035071252e-05,
      "loss": 1.1958,
      "step": 6110
    },
    {
      "epoch": 1.5025779523692608,
      "grad_norm": 18.149003982543945,
      "learning_rate": 2.6038911169895042e-05,
      "loss": 1.3384,
      "step": 6120
    },
    {
      "epoch": 1.5050331451018906,
      "grad_norm": 29.2570743560791,
      "learning_rate": 2.599624541343118e-05,
      "loss": 1.2298,
      "step": 6130
    },
    {
      "epoch": 1.5074883378345199,
      "grad_norm": 19.022619247436523,
      "learning_rate": 2.595357965696732e-05,
      "loss": 1.497,
      "step": 6140
    },
    {
      "epoch": 1.5099435305671496,
      "grad_norm": 17.386577606201172,
      "learning_rate": 2.591091390050346e-05,
      "loss": 1.2289,
      "step": 6150
    },
    {
      "epoch": 1.5123987232997789,
      "grad_norm": 20.144397735595703,
      "learning_rate": 2.5868248144039598e-05,
      "loss": 1.2485,
      "step": 6160
    },
    {
      "epoch": 1.5148539160324086,
      "grad_norm": 14.320547103881836,
      "learning_rate": 2.582558238757573e-05,
      "loss": 1.2773,
      "step": 6170
    },
    {
      "epoch": 1.517309108765038,
      "grad_norm": 17.933963775634766,
      "learning_rate": 2.578291663111187e-05,
      "loss": 1.2281,
      "step": 6180
    },
    {
      "epoch": 1.5197643014976676,
      "grad_norm": 17.706071853637695,
      "learning_rate": 2.574025087464801e-05,
      "loss": 1.2809,
      "step": 6190
    },
    {
      "epoch": 1.5222194942302971,
      "grad_norm": 14.357200622558594,
      "learning_rate": 2.5697585118184147e-05,
      "loss": 1.1958,
      "step": 6200
    },
    {
      "epoch": 1.5246746869629266,
      "grad_norm": 16.00898551940918,
      "learning_rate": 2.5654919361720286e-05,
      "loss": 1.2557,
      "step": 6210
    },
    {
      "epoch": 1.5271298796955561,
      "grad_norm": 14.67321491241455,
      "learning_rate": 2.561225360525642e-05,
      "loss": 1.2344,
      "step": 6220
    },
    {
      "epoch": 1.5295850724281856,
      "grad_norm": 22.905744552612305,
      "learning_rate": 2.5569587848792558e-05,
      "loss": 1.126,
      "step": 6230
    },
    {
      "epoch": 1.5320402651608152,
      "grad_norm": 15.542016983032227,
      "learning_rate": 2.5526922092328697e-05,
      "loss": 1.3079,
      "step": 6240
    },
    {
      "epoch": 1.5344954578934447,
      "grad_norm": 14.898211479187012,
      "learning_rate": 2.5484256335864836e-05,
      "loss": 1.2013,
      "step": 6250
    },
    {
      "epoch": 1.5369506506260742,
      "grad_norm": 21.140209197998047,
      "learning_rate": 2.5441590579400975e-05,
      "loss": 1.2886,
      "step": 6260
    },
    {
      "epoch": 1.5394058433587037,
      "grad_norm": 19.486705780029297,
      "learning_rate": 2.5398924822937114e-05,
      "loss": 1.243,
      "step": 6270
    },
    {
      "epoch": 1.5418610360913332,
      "grad_norm": 15.68089771270752,
      "learning_rate": 2.535625906647325e-05,
      "loss": 1.1477,
      "step": 6280
    },
    {
      "epoch": 1.5443162288239627,
      "grad_norm": 19.375139236450195,
      "learning_rate": 2.531359331000939e-05,
      "loss": 1.3847,
      "step": 6290
    },
    {
      "epoch": 1.5467714215565922,
      "grad_norm": 17.79928207397461,
      "learning_rate": 2.5270927553545527e-05,
      "loss": 1.2217,
      "step": 6300
    },
    {
      "epoch": 1.5492266142892217,
      "grad_norm": 16.510900497436523,
      "learning_rate": 2.5228261797081666e-05,
      "loss": 1.3542,
      "step": 6310
    },
    {
      "epoch": 1.5516818070218512,
      "grad_norm": 11.3043851852417,
      "learning_rate": 2.5185596040617805e-05,
      "loss": 1.1722,
      "step": 6320
    },
    {
      "epoch": 1.5541369997544807,
      "grad_norm": 15.523384094238281,
      "learning_rate": 2.5142930284153938e-05,
      "loss": 1.3568,
      "step": 6330
    },
    {
      "epoch": 1.5565921924871102,
      "grad_norm": 24.2136287689209,
      "learning_rate": 2.5100264527690077e-05,
      "loss": 1.2028,
      "step": 6340
    },
    {
      "epoch": 1.5590473852197397,
      "grad_norm": 15.727526664733887,
      "learning_rate": 2.5057598771226216e-05,
      "loss": 1.3862,
      "step": 6350
    },
    {
      "epoch": 1.5615025779523692,
      "grad_norm": 14.769006729125977,
      "learning_rate": 2.5014933014762355e-05,
      "loss": 1.2098,
      "step": 6360
    },
    {
      "epoch": 1.5639577706849987,
      "grad_norm": 26.229990005493164,
      "learning_rate": 2.497226725829849e-05,
      "loss": 1.286,
      "step": 6370
    },
    {
      "epoch": 1.5664129634176283,
      "grad_norm": 19.96038055419922,
      "learning_rate": 2.492960150183463e-05,
      "loss": 1.2155,
      "step": 6380
    },
    {
      "epoch": 1.5688681561502578,
      "grad_norm": 18.34994125366211,
      "learning_rate": 2.488693574537077e-05,
      "loss": 1.3079,
      "step": 6390
    },
    {
      "epoch": 1.5713233488828873,
      "grad_norm": 14.3428955078125,
      "learning_rate": 2.4844269988906904e-05,
      "loss": 1.3795,
      "step": 6400
    },
    {
      "epoch": 1.5737785416155168,
      "grad_norm": 16.177141189575195,
      "learning_rate": 2.4801604232443043e-05,
      "loss": 1.2894,
      "step": 6410
    },
    {
      "epoch": 1.5762337343481463,
      "grad_norm": 16.143190383911133,
      "learning_rate": 2.475893847597918e-05,
      "loss": 1.2362,
      "step": 6420
    },
    {
      "epoch": 1.5786889270807758,
      "grad_norm": 19.6120662689209,
      "learning_rate": 2.4716272719515318e-05,
      "loss": 1.1793,
      "step": 6430
    },
    {
      "epoch": 1.5811441198134053,
      "grad_norm": 19.055953979492188,
      "learning_rate": 2.4673606963051457e-05,
      "loss": 1.2437,
      "step": 6440
    },
    {
      "epoch": 1.5835993125460348,
      "grad_norm": 22.184223175048828,
      "learning_rate": 2.4630941206587592e-05,
      "loss": 1.2056,
      "step": 6450
    },
    {
      "epoch": 1.5860545052786645,
      "grad_norm": 20.021135330200195,
      "learning_rate": 2.458827545012373e-05,
      "loss": 1.2109,
      "step": 6460
    },
    {
      "epoch": 1.5885096980112938,
      "grad_norm": 17.529706954956055,
      "learning_rate": 2.4545609693659867e-05,
      "loss": 1.2824,
      "step": 6470
    },
    {
      "epoch": 1.5909648907439236,
      "grad_norm": 15.706022262573242,
      "learning_rate": 2.4502943937196006e-05,
      "loss": 1.3728,
      "step": 6480
    },
    {
      "epoch": 1.5934200834765528,
      "grad_norm": 14.072994232177734,
      "learning_rate": 2.4460278180732145e-05,
      "loss": 1.204,
      "step": 6490
    },
    {
      "epoch": 1.5958752762091826,
      "grad_norm": 18.481121063232422,
      "learning_rate": 2.4417612424268284e-05,
      "loss": 1.2674,
      "step": 6500
    },
    {
      "epoch": 1.5983304689418119,
      "grad_norm": 18.80643081665039,
      "learning_rate": 2.437494666780442e-05,
      "loss": 1.2863,
      "step": 6510
    },
    {
      "epoch": 1.6007856616744416,
      "grad_norm": 24.068023681640625,
      "learning_rate": 2.433228091134056e-05,
      "loss": 1.3172,
      "step": 6520
    },
    {
      "epoch": 1.6032408544070709,
      "grad_norm": 16.845048904418945,
      "learning_rate": 2.4289615154876698e-05,
      "loss": 1.1714,
      "step": 6530
    },
    {
      "epoch": 1.6056960471397006,
      "grad_norm": 27.580297470092773,
      "learning_rate": 2.4246949398412837e-05,
      "loss": 1.1701,
      "step": 6540
    },
    {
      "epoch": 1.6081512398723299,
      "grad_norm": 16.15776824951172,
      "learning_rate": 2.4204283641948972e-05,
      "loss": 1.2864,
      "step": 6550
    },
    {
      "epoch": 1.6106064326049596,
      "grad_norm": 14.244383811950684,
      "learning_rate": 2.416161788548511e-05,
      "loss": 1.2464,
      "step": 6560
    },
    {
      "epoch": 1.613061625337589,
      "grad_norm": 13.06449031829834,
      "learning_rate": 2.411895212902125e-05,
      "loss": 1.1015,
      "step": 6570
    },
    {
      "epoch": 1.6155168180702186,
      "grad_norm": 18.221223831176758,
      "learning_rate": 2.4076286372557386e-05,
      "loss": 1.2082,
      "step": 6580
    },
    {
      "epoch": 1.617972010802848,
      "grad_norm": 20.489591598510742,
      "learning_rate": 2.4033620616093525e-05,
      "loss": 1.2211,
      "step": 6590
    },
    {
      "epoch": 1.6204272035354776,
      "grad_norm": 17.221670150756836,
      "learning_rate": 2.3990954859629664e-05,
      "loss": 1.1796,
      "step": 6600
    },
    {
      "epoch": 1.622882396268107,
      "grad_norm": 16.27416229248047,
      "learning_rate": 2.39482891031658e-05,
      "loss": 1.1872,
      "step": 6610
    },
    {
      "epoch": 1.6253375890007367,
      "grad_norm": 17.744495391845703,
      "learning_rate": 2.390562334670194e-05,
      "loss": 1.2081,
      "step": 6620
    },
    {
      "epoch": 1.627792781733366,
      "grad_norm": 22.17230224609375,
      "learning_rate": 2.3862957590238074e-05,
      "loss": 1.2875,
      "step": 6630
    },
    {
      "epoch": 1.6302479744659957,
      "grad_norm": 18.127731323242188,
      "learning_rate": 2.3820291833774213e-05,
      "loss": 1.2127,
      "step": 6640
    },
    {
      "epoch": 1.632703167198625,
      "grad_norm": 20.45987319946289,
      "learning_rate": 2.3777626077310352e-05,
      "loss": 1.1608,
      "step": 6650
    },
    {
      "epoch": 1.6351583599312547,
      "grad_norm": 14.944735527038574,
      "learning_rate": 2.3734960320846488e-05,
      "loss": 1.1274,
      "step": 6660
    },
    {
      "epoch": 1.637613552663884,
      "grad_norm": 20.150781631469727,
      "learning_rate": 2.3692294564382627e-05,
      "loss": 1.2077,
      "step": 6670
    },
    {
      "epoch": 1.6400687453965137,
      "grad_norm": 18.932781219482422,
      "learning_rate": 2.3649628807918766e-05,
      "loss": 1.1542,
      "step": 6680
    },
    {
      "epoch": 1.642523938129143,
      "grad_norm": 24.09284782409668,
      "learning_rate": 2.3606963051454902e-05,
      "loss": 1.0948,
      "step": 6690
    },
    {
      "epoch": 1.6449791308617727,
      "grad_norm": 19.65772819519043,
      "learning_rate": 2.356429729499104e-05,
      "loss": 1.2186,
      "step": 6700
    },
    {
      "epoch": 1.647434323594402,
      "grad_norm": 13.333660125732422,
      "learning_rate": 2.3521631538527176e-05,
      "loss": 1.2362,
      "step": 6710
    },
    {
      "epoch": 1.6498895163270317,
      "grad_norm": 21.243085861206055,
      "learning_rate": 2.3478965782063315e-05,
      "loss": 1.2732,
      "step": 6720
    },
    {
      "epoch": 1.652344709059661,
      "grad_norm": 29.418825149536133,
      "learning_rate": 2.3436300025599454e-05,
      "loss": 1.3936,
      "step": 6730
    },
    {
      "epoch": 1.6547999017922908,
      "grad_norm": 20.556241989135742,
      "learning_rate": 2.3393634269135594e-05,
      "loss": 1.3882,
      "step": 6740
    },
    {
      "epoch": 1.6572550945249203,
      "grad_norm": 14.374712944030762,
      "learning_rate": 2.3350968512671733e-05,
      "loss": 1.1533,
      "step": 6750
    },
    {
      "epoch": 1.6597102872575498,
      "grad_norm": 23.635223388671875,
      "learning_rate": 2.3308302756207868e-05,
      "loss": 1.1812,
      "step": 6760
    },
    {
      "epoch": 1.6621654799901793,
      "grad_norm": 14.780792236328125,
      "learning_rate": 2.3265636999744007e-05,
      "loss": 1.1685,
      "step": 6770
    },
    {
      "epoch": 1.6646206727228088,
      "grad_norm": 20.767927169799805,
      "learning_rate": 2.3222971243280146e-05,
      "loss": 1.1657,
      "step": 6780
    },
    {
      "epoch": 1.6670758654554383,
      "grad_norm": 22.737255096435547,
      "learning_rate": 2.3180305486816282e-05,
      "loss": 1.3513,
      "step": 6790
    },
    {
      "epoch": 1.6695310581880678,
      "grad_norm": 17.84093475341797,
      "learning_rate": 2.313763973035242e-05,
      "loss": 1.3321,
      "step": 6800
    },
    {
      "epoch": 1.6719862509206973,
      "grad_norm": 22.96792984008789,
      "learning_rate": 2.309497397388856e-05,
      "loss": 1.2146,
      "step": 6810
    },
    {
      "epoch": 1.6744414436533268,
      "grad_norm": 14.29028606414795,
      "learning_rate": 2.3052308217424696e-05,
      "loss": 1.2425,
      "step": 6820
    },
    {
      "epoch": 1.6768966363859563,
      "grad_norm": 18.49913787841797,
      "learning_rate": 2.3009642460960835e-05,
      "loss": 1.223,
      "step": 6830
    },
    {
      "epoch": 1.6793518291185858,
      "grad_norm": 12.390497207641602,
      "learning_rate": 2.2966976704496974e-05,
      "loss": 1.1034,
      "step": 6840
    },
    {
      "epoch": 1.6818070218512153,
      "grad_norm": 22.02581787109375,
      "learning_rate": 2.292431094803311e-05,
      "loss": 1.287,
      "step": 6850
    },
    {
      "epoch": 1.6842622145838448,
      "grad_norm": 14.319892883300781,
      "learning_rate": 2.2881645191569248e-05,
      "loss": 1.0882,
      "step": 6860
    },
    {
      "epoch": 1.6867174073164743,
      "grad_norm": 16.309608459472656,
      "learning_rate": 2.2838979435105384e-05,
      "loss": 1.291,
      "step": 6870
    },
    {
      "epoch": 1.6891726000491039,
      "grad_norm": 20.84641456604004,
      "learning_rate": 2.2796313678641523e-05,
      "loss": 1.1974,
      "step": 6880
    },
    {
      "epoch": 1.6916277927817334,
      "grad_norm": 20.120708465576172,
      "learning_rate": 2.2753647922177662e-05,
      "loss": 1.3968,
      "step": 6890
    },
    {
      "epoch": 1.6940829855143629,
      "grad_norm": 15.267833709716797,
      "learning_rate": 2.2710982165713798e-05,
      "loss": 1.2274,
      "step": 6900
    },
    {
      "epoch": 1.6965381782469924,
      "grad_norm": 16.096416473388672,
      "learning_rate": 2.2668316409249937e-05,
      "loss": 1.3078,
      "step": 6910
    },
    {
      "epoch": 1.6989933709796219,
      "grad_norm": 18.219396591186523,
      "learning_rate": 2.2625650652786076e-05,
      "loss": 1.1143,
      "step": 6920
    },
    {
      "epoch": 1.7014485637122514,
      "grad_norm": 20.40663719177246,
      "learning_rate": 2.258298489632221e-05,
      "loss": 1.208,
      "step": 6930
    },
    {
      "epoch": 1.703903756444881,
      "grad_norm": 17.827259063720703,
      "learning_rate": 2.254031913985835e-05,
      "loss": 1.3293,
      "step": 6940
    },
    {
      "epoch": 1.7063589491775104,
      "grad_norm": 15.58791732788086,
      "learning_rate": 2.2497653383394486e-05,
      "loss": 1.2517,
      "step": 6950
    },
    {
      "epoch": 1.70881414191014,
      "grad_norm": 17.671052932739258,
      "learning_rate": 2.2454987626930625e-05,
      "loss": 1.2799,
      "step": 6960
    },
    {
      "epoch": 1.7112693346427694,
      "grad_norm": 20.489778518676758,
      "learning_rate": 2.2412321870466764e-05,
      "loss": 1.2111,
      "step": 6970
    },
    {
      "epoch": 1.713724527375399,
      "grad_norm": 16.95716667175293,
      "learning_rate": 2.2369656114002903e-05,
      "loss": 1.376,
      "step": 6980
    },
    {
      "epoch": 1.7161797201080284,
      "grad_norm": 18.316007614135742,
      "learning_rate": 2.2326990357539042e-05,
      "loss": 1.1902,
      "step": 6990
    },
    {
      "epoch": 1.718634912840658,
      "grad_norm": 19.084671020507812,
      "learning_rate": 2.228432460107518e-05,
      "loss": 1.1394,
      "step": 7000
    },
    {
      "epoch": 1.7210901055732875,
      "grad_norm": 19.26607894897461,
      "learning_rate": 2.2241658844611317e-05,
      "loss": 1.2017,
      "step": 7010
    },
    {
      "epoch": 1.723545298305917,
      "grad_norm": 16.779510498046875,
      "learning_rate": 2.2198993088147456e-05,
      "loss": 1.2621,
      "step": 7020
    },
    {
      "epoch": 1.7260004910385467,
      "grad_norm": 13.13556957244873,
      "learning_rate": 2.215632733168359e-05,
      "loss": 1.2565,
      "step": 7030
    },
    {
      "epoch": 1.728455683771176,
      "grad_norm": 17.449092864990234,
      "learning_rate": 2.211366157521973e-05,
      "loss": 1.3175,
      "step": 7040
    },
    {
      "epoch": 1.7309108765038057,
      "grad_norm": 15.752181053161621,
      "learning_rate": 2.207099581875587e-05,
      "loss": 1.1752,
      "step": 7050
    },
    {
      "epoch": 1.733366069236435,
      "grad_norm": 16.194042205810547,
      "learning_rate": 2.2028330062292005e-05,
      "loss": 1.1827,
      "step": 7060
    },
    {
      "epoch": 1.7358212619690647,
      "grad_norm": 17.612977981567383,
      "learning_rate": 2.1985664305828144e-05,
      "loss": 1.147,
      "step": 7070
    },
    {
      "epoch": 1.738276454701694,
      "grad_norm": 16.280540466308594,
      "learning_rate": 2.1942998549364283e-05,
      "loss": 1.2611,
      "step": 7080
    },
    {
      "epoch": 1.7407316474343237,
      "grad_norm": 17.685596466064453,
      "learning_rate": 2.190033279290042e-05,
      "loss": 1.2878,
      "step": 7090
    },
    {
      "epoch": 1.743186840166953,
      "grad_norm": 17.698497772216797,
      "learning_rate": 2.1857667036436558e-05,
      "loss": 1.2168,
      "step": 7100
    },
    {
      "epoch": 1.7456420328995828,
      "grad_norm": 14.16634464263916,
      "learning_rate": 2.1815001279972693e-05,
      "loss": 1.2858,
      "step": 7110
    },
    {
      "epoch": 1.748097225632212,
      "grad_norm": 12.358854293823242,
      "learning_rate": 2.1772335523508832e-05,
      "loss": 1.2046,
      "step": 7120
    },
    {
      "epoch": 1.7505524183648418,
      "grad_norm": 20.244388580322266,
      "learning_rate": 2.172966976704497e-05,
      "loss": 1.215,
      "step": 7130
    },
    {
      "epoch": 1.753007611097471,
      "grad_norm": 19.43956184387207,
      "learning_rate": 2.1687004010581107e-05,
      "loss": 1.3445,
      "step": 7140
    },
    {
      "epoch": 1.7554628038301008,
      "grad_norm": 12.499817848205566,
      "learning_rate": 2.1644338254117246e-05,
      "loss": 1.2755,
      "step": 7150
    },
    {
      "epoch": 1.75791799656273,
      "grad_norm": 14.352014541625977,
      "learning_rate": 2.1601672497653385e-05,
      "loss": 1.2241,
      "step": 7160
    },
    {
      "epoch": 1.7603731892953598,
      "grad_norm": 18.77902603149414,
      "learning_rate": 2.155900674118952e-05,
      "loss": 1.229,
      "step": 7170
    },
    {
      "epoch": 1.762828382027989,
      "grad_norm": 21.4166316986084,
      "learning_rate": 2.151634098472566e-05,
      "loss": 1.3347,
      "step": 7180
    },
    {
      "epoch": 1.7652835747606188,
      "grad_norm": 23.261436462402344,
      "learning_rate": 2.14736752282618e-05,
      "loss": 1.3242,
      "step": 7190
    },
    {
      "epoch": 1.767738767493248,
      "grad_norm": 15.485529899597168,
      "learning_rate": 2.1431009471797934e-05,
      "loss": 1.0961,
      "step": 7200
    },
    {
      "epoch": 1.7701939602258778,
      "grad_norm": 16.443397521972656,
      "learning_rate": 2.1388343715334073e-05,
      "loss": 1.0938,
      "step": 7210
    },
    {
      "epoch": 1.7726491529585071,
      "grad_norm": 12.694479942321777,
      "learning_rate": 2.1345677958870212e-05,
      "loss": 1.2735,
      "step": 7220
    },
    {
      "epoch": 1.7751043456911368,
      "grad_norm": 19.17702865600586,
      "learning_rate": 2.130301220240635e-05,
      "loss": 1.1925,
      "step": 7230
    },
    {
      "epoch": 1.7775595384237661,
      "grad_norm": 17.703577041625977,
      "learning_rate": 2.126034644594249e-05,
      "loss": 1.157,
      "step": 7240
    },
    {
      "epoch": 1.7800147311563959,
      "grad_norm": 22.12287712097168,
      "learning_rate": 2.1217680689478626e-05,
      "loss": 1.1963,
      "step": 7250
    },
    {
      "epoch": 1.7824699238890251,
      "grad_norm": 13.38161849975586,
      "learning_rate": 2.1175014933014765e-05,
      "loss": 1.0987,
      "step": 7260
    },
    {
      "epoch": 1.7849251166216549,
      "grad_norm": 17.771120071411133,
      "learning_rate": 2.11323491765509e-05,
      "loss": 1.3877,
      "step": 7270
    },
    {
      "epoch": 1.7873803093542842,
      "grad_norm": 22.298526763916016,
      "learning_rate": 2.108968342008704e-05,
      "loss": 1.4234,
      "step": 7280
    },
    {
      "epoch": 1.7898355020869139,
      "grad_norm": 19.08383560180664,
      "learning_rate": 2.104701766362318e-05,
      "loss": 1.2537,
      "step": 7290
    },
    {
      "epoch": 1.7922906948195432,
      "grad_norm": 18.393295288085938,
      "learning_rate": 2.1004351907159314e-05,
      "loss": 1.3032,
      "step": 7300
    },
    {
      "epoch": 1.794745887552173,
      "grad_norm": 18.02573013305664,
      "learning_rate": 2.0961686150695453e-05,
      "loss": 1.3313,
      "step": 7310
    },
    {
      "epoch": 1.7972010802848024,
      "grad_norm": 14.332855224609375,
      "learning_rate": 2.0919020394231592e-05,
      "loss": 1.161,
      "step": 7320
    },
    {
      "epoch": 1.799656273017432,
      "grad_norm": 14.834723472595215,
      "learning_rate": 2.0876354637767728e-05,
      "loss": 1.1104,
      "step": 7330
    },
    {
      "epoch": 1.8021114657500614,
      "grad_norm": 15.8076753616333,
      "learning_rate": 2.0833688881303867e-05,
      "loss": 1.0675,
      "step": 7340
    },
    {
      "epoch": 1.804566658482691,
      "grad_norm": 15.724775314331055,
      "learning_rate": 2.0791023124840003e-05,
      "loss": 1.2838,
      "step": 7350
    },
    {
      "epoch": 1.8070218512153204,
      "grad_norm": 15.706426620483398,
      "learning_rate": 2.0748357368376142e-05,
      "loss": 1.22,
      "step": 7360
    },
    {
      "epoch": 1.80947704394795,
      "grad_norm": 17.00166893005371,
      "learning_rate": 2.070569161191228e-05,
      "loss": 1.2703,
      "step": 7370
    },
    {
      "epoch": 1.8119322366805795,
      "grad_norm": 24.189002990722656,
      "learning_rate": 2.0663025855448416e-05,
      "loss": 1.1736,
      "step": 7380
    },
    {
      "epoch": 1.814387429413209,
      "grad_norm": 18.982755661010742,
      "learning_rate": 2.0620360098984555e-05,
      "loss": 1.2221,
      "step": 7390
    },
    {
      "epoch": 1.8168426221458385,
      "grad_norm": 18.734603881835938,
      "learning_rate": 2.057769434252069e-05,
      "loss": 1.1303,
      "step": 7400
    },
    {
      "epoch": 1.819297814878468,
      "grad_norm": 17.453731536865234,
      "learning_rate": 2.053502858605683e-05,
      "loss": 1.1602,
      "step": 7410
    },
    {
      "epoch": 1.8217530076110975,
      "grad_norm": 18.653589248657227,
      "learning_rate": 2.049236282959297e-05,
      "loss": 1.2536,
      "step": 7420
    },
    {
      "epoch": 1.824208200343727,
      "grad_norm": 20.194190979003906,
      "learning_rate": 2.0449697073129108e-05,
      "loss": 1.159,
      "step": 7430
    },
    {
      "epoch": 1.8266633930763565,
      "grad_norm": 16.93296241760254,
      "learning_rate": 2.0407031316665247e-05,
      "loss": 1.3556,
      "step": 7440
    },
    {
      "epoch": 1.829118585808986,
      "grad_norm": 16.9818115234375,
      "learning_rate": 2.0364365560201383e-05,
      "loss": 1.3617,
      "step": 7450
    },
    {
      "epoch": 1.8315737785416155,
      "grad_norm": 19.83795738220215,
      "learning_rate": 2.0321699803737522e-05,
      "loss": 1.2163,
      "step": 7460
    },
    {
      "epoch": 1.834028971274245,
      "grad_norm": 15.450572967529297,
      "learning_rate": 2.027903404727366e-05,
      "loss": 1.1899,
      "step": 7470
    },
    {
      "epoch": 1.8364841640068745,
      "grad_norm": 28.38197135925293,
      "learning_rate": 2.0236368290809796e-05,
      "loss": 1.2934,
      "step": 7480
    },
    {
      "epoch": 1.838939356739504,
      "grad_norm": 16.51959800720215,
      "learning_rate": 2.0193702534345935e-05,
      "loss": 1.2808,
      "step": 7490
    },
    {
      "epoch": 1.8413945494721335,
      "grad_norm": 18.578025817871094,
      "learning_rate": 2.0151036777882074e-05,
      "loss": 1.3052,
      "step": 7500
    },
    {
      "epoch": 1.843849742204763,
      "grad_norm": 19.507234573364258,
      "learning_rate": 2.010837102141821e-05,
      "loss": 1.1808,
      "step": 7510
    },
    {
      "epoch": 1.8463049349373926,
      "grad_norm": 24.46407699584961,
      "learning_rate": 2.006570526495435e-05,
      "loss": 1.199,
      "step": 7520
    },
    {
      "epoch": 1.848760127670022,
      "grad_norm": 19.151578903198242,
      "learning_rate": 2.0023039508490488e-05,
      "loss": 1.2559,
      "step": 7530
    },
    {
      "epoch": 1.8512153204026516,
      "grad_norm": 23.4188175201416,
      "learning_rate": 1.9980373752026624e-05,
      "loss": 1.0647,
      "step": 7540
    },
    {
      "epoch": 1.853670513135281,
      "grad_norm": 17.298818588256836,
      "learning_rate": 1.9937707995562763e-05,
      "loss": 1.0891,
      "step": 7550
    },
    {
      "epoch": 1.8561257058679106,
      "grad_norm": 18.25591278076172,
      "learning_rate": 1.98950422390989e-05,
      "loss": 1.1013,
      "step": 7560
    },
    {
      "epoch": 1.85858089860054,
      "grad_norm": 15.956210136413574,
      "learning_rate": 1.9852376482635037e-05,
      "loss": 1.2357,
      "step": 7570
    },
    {
      "epoch": 1.8610360913331696,
      "grad_norm": 24.908544540405273,
      "learning_rate": 1.9809710726171176e-05,
      "loss": 1.2783,
      "step": 7580
    },
    {
      "epoch": 1.8634912840657991,
      "grad_norm": 19.26731300354004,
      "learning_rate": 1.9767044969707312e-05,
      "loss": 1.3253,
      "step": 7590
    },
    {
      "epoch": 1.8659464767984286,
      "grad_norm": 17.1734676361084,
      "learning_rate": 1.972437921324345e-05,
      "loss": 1.3319,
      "step": 7600
    },
    {
      "epoch": 1.8684016695310581,
      "grad_norm": 19.282297134399414,
      "learning_rate": 1.968171345677959e-05,
      "loss": 1.2267,
      "step": 7610
    },
    {
      "epoch": 1.8708568622636879,
      "grad_norm": 16.633617401123047,
      "learning_rate": 1.9639047700315726e-05,
      "loss": 1.051,
      "step": 7620
    },
    {
      "epoch": 1.8733120549963171,
      "grad_norm": 14.8544921875,
      "learning_rate": 1.9596381943851865e-05,
      "loss": 1.194,
      "step": 7630
    },
    {
      "epoch": 1.8757672477289469,
      "grad_norm": 17.747432708740234,
      "learning_rate": 1.9553716187388e-05,
      "loss": 1.2886,
      "step": 7640
    },
    {
      "epoch": 1.8782224404615762,
      "grad_norm": 16.71763038635254,
      "learning_rate": 1.951105043092414e-05,
      "loss": 1.1179,
      "step": 7650
    },
    {
      "epoch": 1.8806776331942059,
      "grad_norm": 14.390393257141113,
      "learning_rate": 1.946838467446028e-05,
      "loss": 1.3286,
      "step": 7660
    },
    {
      "epoch": 1.8831328259268352,
      "grad_norm": 17.740861892700195,
      "learning_rate": 1.9425718917996417e-05,
      "loss": 1.2079,
      "step": 7670
    },
    {
      "epoch": 1.885588018659465,
      "grad_norm": 18.109619140625,
      "learning_rate": 1.9383053161532557e-05,
      "loss": 1.3308,
      "step": 7680
    },
    {
      "epoch": 1.8880432113920942,
      "grad_norm": 16.629446029663086,
      "learning_rate": 1.9340387405068696e-05,
      "loss": 1.3574,
      "step": 7690
    },
    {
      "epoch": 1.890498404124724,
      "grad_norm": 16.48978042602539,
      "learning_rate": 1.929772164860483e-05,
      "loss": 1.1201,
      "step": 7700
    },
    {
      "epoch": 1.8929535968573532,
      "grad_norm": 20.187694549560547,
      "learning_rate": 1.925505589214097e-05,
      "loss": 1.2134,
      "step": 7710
    },
    {
      "epoch": 1.895408789589983,
      "grad_norm": 22.823108673095703,
      "learning_rate": 1.9212390135677106e-05,
      "loss": 1.2949,
      "step": 7720
    },
    {
      "epoch": 1.8978639823226122,
      "grad_norm": 16.991931915283203,
      "learning_rate": 1.9169724379213245e-05,
      "loss": 1.159,
      "step": 7730
    },
    {
      "epoch": 1.900319175055242,
      "grad_norm": 25.93939781188965,
      "learning_rate": 1.9127058622749384e-05,
      "loss": 1.26,
      "step": 7740
    },
    {
      "epoch": 1.9027743677878712,
      "grad_norm": 20.1939697265625,
      "learning_rate": 1.908439286628552e-05,
      "loss": 1.0407,
      "step": 7750
    },
    {
      "epoch": 1.905229560520501,
      "grad_norm": 17.909320831298828,
      "learning_rate": 1.904172710982166e-05,
      "loss": 1.2338,
      "step": 7760
    },
    {
      "epoch": 1.9076847532531302,
      "grad_norm": 28.201610565185547,
      "learning_rate": 1.8999061353357798e-05,
      "loss": 1.175,
      "step": 7770
    },
    {
      "epoch": 1.91013994598576,
      "grad_norm": 26.110822677612305,
      "learning_rate": 1.8956395596893933e-05,
      "loss": 1.3502,
      "step": 7780
    },
    {
      "epoch": 1.9125951387183893,
      "grad_norm": 19.79240608215332,
      "learning_rate": 1.8913729840430072e-05,
      "loss": 1.3404,
      "step": 7790
    },
    {
      "epoch": 1.915050331451019,
      "grad_norm": 14.999266624450684,
      "learning_rate": 1.8871064083966208e-05,
      "loss": 1.2515,
      "step": 7800
    },
    {
      "epoch": 1.9175055241836483,
      "grad_norm": 16.45503044128418,
      "learning_rate": 1.8828398327502347e-05,
      "loss": 1.2461,
      "step": 7810
    },
    {
      "epoch": 1.919960716916278,
      "grad_norm": 25.934450149536133,
      "learning_rate": 1.8785732571038486e-05,
      "loss": 1.2969,
      "step": 7820
    },
    {
      "epoch": 1.9224159096489073,
      "grad_norm": 15.856649398803711,
      "learning_rate": 1.874306681457462e-05,
      "loss": 1.2301,
      "step": 7830
    },
    {
      "epoch": 1.924871102381537,
      "grad_norm": 15.514690399169922,
      "learning_rate": 1.870040105811076e-05,
      "loss": 1.0968,
      "step": 7840
    },
    {
      "epoch": 1.9273262951141663,
      "grad_norm": 17.977140426635742,
      "learning_rate": 1.86577353016469e-05,
      "loss": 1.2623,
      "step": 7850
    },
    {
      "epoch": 1.929781487846796,
      "grad_norm": 14.589717864990234,
      "learning_rate": 1.8615069545183035e-05,
      "loss": 1.1579,
      "step": 7860
    },
    {
      "epoch": 1.9322366805794253,
      "grad_norm": 14.540907859802246,
      "learning_rate": 1.8572403788719174e-05,
      "loss": 1.3652,
      "step": 7870
    },
    {
      "epoch": 1.934691873312055,
      "grad_norm": 17.074291229248047,
      "learning_rate": 1.8529738032255313e-05,
      "loss": 1.3237,
      "step": 7880
    },
    {
      "epoch": 1.9371470660446846,
      "grad_norm": 17.386905670166016,
      "learning_rate": 1.848707227579145e-05,
      "loss": 1.2766,
      "step": 7890
    },
    {
      "epoch": 1.939602258777314,
      "grad_norm": 17.134260177612305,
      "learning_rate": 1.8444406519327588e-05,
      "loss": 1.2163,
      "step": 7900
    },
    {
      "epoch": 1.9420574515099436,
      "grad_norm": 17.3872127532959,
      "learning_rate": 1.8401740762863727e-05,
      "loss": 1.1592,
      "step": 7910
    },
    {
      "epoch": 1.944512644242573,
      "grad_norm": 19.877668380737305,
      "learning_rate": 1.8359075006399866e-05,
      "loss": 1.3174,
      "step": 7920
    },
    {
      "epoch": 1.9469678369752026,
      "grad_norm": 24.598543167114258,
      "learning_rate": 1.8316409249936005e-05,
      "loss": 1.3088,
      "step": 7930
    },
    {
      "epoch": 1.949423029707832,
      "grad_norm": 16.704853057861328,
      "learning_rate": 1.827374349347214e-05,
      "loss": 1.308,
      "step": 7940
    },
    {
      "epoch": 1.9518782224404616,
      "grad_norm": 18.306015014648438,
      "learning_rate": 1.823107773700828e-05,
      "loss": 1.2467,
      "step": 7950
    },
    {
      "epoch": 1.9543334151730911,
      "grad_norm": 19.341978073120117,
      "learning_rate": 1.8188411980544415e-05,
      "loss": 1.236,
      "step": 7960
    },
    {
      "epoch": 1.9567886079057206,
      "grad_norm": 15.59162425994873,
      "learning_rate": 1.8145746224080554e-05,
      "loss": 1.2948,
      "step": 7970
    },
    {
      "epoch": 1.9592438006383501,
      "grad_norm": 19.708782196044922,
      "learning_rate": 1.8103080467616693e-05,
      "loss": 1.3577,
      "step": 7980
    },
    {
      "epoch": 1.9616989933709796,
      "grad_norm": 20.705791473388672,
      "learning_rate": 1.806041471115283e-05,
      "loss": 1.2569,
      "step": 7990
    },
    {
      "epoch": 1.9641541861036091,
      "grad_norm": 17.547983169555664,
      "learning_rate": 1.8017748954688968e-05,
      "loss": 1.3128,
      "step": 8000
    },
    {
      "epoch": 1.9666093788362387,
      "grad_norm": 16.06427574157715,
      "learning_rate": 1.7975083198225107e-05,
      "loss": 1.4167,
      "step": 8010
    },
    {
      "epoch": 1.9690645715688682,
      "grad_norm": 14.165163040161133,
      "learning_rate": 1.7932417441761243e-05,
      "loss": 1.3628,
      "step": 8020
    },
    {
      "epoch": 1.9715197643014977,
      "grad_norm": 15.674781799316406,
      "learning_rate": 1.788975168529738e-05,
      "loss": 1.2808,
      "step": 8030
    },
    {
      "epoch": 1.9739749570341272,
      "grad_norm": 16.813392639160156,
      "learning_rate": 1.7847085928833517e-05,
      "loss": 1.258,
      "step": 8040
    },
    {
      "epoch": 1.9764301497667567,
      "grad_norm": 15.5125150680542,
      "learning_rate": 1.7804420172369656e-05,
      "loss": 1.3425,
      "step": 8050
    },
    {
      "epoch": 1.9788853424993862,
      "grad_norm": 13.418585777282715,
      "learning_rate": 1.7761754415905795e-05,
      "loss": 1.3011,
      "step": 8060
    },
    {
      "epoch": 1.9813405352320157,
      "grad_norm": 20.726850509643555,
      "learning_rate": 1.771908865944193e-05,
      "loss": 1.3231,
      "step": 8070
    },
    {
      "epoch": 1.9837957279646452,
      "grad_norm": 19.886911392211914,
      "learning_rate": 1.767642290297807e-05,
      "loss": 1.2833,
      "step": 8080
    },
    {
      "epoch": 1.9862509206972747,
      "grad_norm": 20.750965118408203,
      "learning_rate": 1.763375714651421e-05,
      "loss": 1.3023,
      "step": 8090
    },
    {
      "epoch": 1.9887061134299042,
      "grad_norm": 18.97832489013672,
      "learning_rate": 1.7591091390050345e-05,
      "loss": 1.2341,
      "step": 8100
    },
    {
      "epoch": 1.9911613061625337,
      "grad_norm": 19.33981704711914,
      "learning_rate": 1.7548425633586484e-05,
      "loss": 1.204,
      "step": 8110
    },
    {
      "epoch": 1.9936164988951632,
      "grad_norm": 20.306682586669922,
      "learning_rate": 1.7505759877122623e-05,
      "loss": 1.2896,
      "step": 8120
    },
    {
      "epoch": 1.9960716916277927,
      "grad_norm": 15.079468727111816,
      "learning_rate": 1.746309412065876e-05,
      "loss": 1.3509,
      "step": 8130
    },
    {
      "epoch": 1.9985268843604223,
      "grad_norm": 19.521032333374023,
      "learning_rate": 1.74204283641949e-05,
      "loss": 1.2544,
      "step": 8140
    },
    {
      "epoch": 2.0,
      "eval_exact": 55.88309610039585,
      "eval_f1": 60.448030446961965,
      "eval_hasAns_f1": 54.183445596622335,
      "eval_loss": 1.6107739210128784,
      "eval_runtime": 42.7441,
      "eval_samples_per_second": 277.769,
      "eval_steps_per_second": 4.351,
      "step": 8146
    },
    {
      "epoch": 2.000982077093052,
      "grad_norm": 12.56842041015625,
      "learning_rate": 1.7377762607731036e-05,
      "loss": 1.1982,
      "step": 8150
    },
    {
      "epoch": 2.0034372698256813,
      "grad_norm": 24.23826026916504,
      "learning_rate": 1.7335096851267175e-05,
      "loss": 1.1659,
      "step": 8160
    },
    {
      "epoch": 2.005892462558311,
      "grad_norm": Infinity,
      "learning_rate": 1.72966976704497e-05,
      "loss": 0.9553,
      "step": 8170
    },
    {
      "epoch": 2.0083476552909403,
      "grad_norm": 14.527759552001953,
      "learning_rate": 1.7254031913985836e-05,
      "loss": 1.0381,
      "step": 8180
    },
    {
      "epoch": 2.01080284802357,
      "grad_norm": 18.017681121826172,
      "learning_rate": 1.7211366157521975e-05,
      "loss": 1.0538,
      "step": 8190
    },
    {
      "epoch": 2.0132580407561993,
      "grad_norm": 20.45457649230957,
      "learning_rate": 1.716870040105811e-05,
      "loss": 1.0848,
      "step": 8200
    },
    {
      "epoch": 2.015713233488829,
      "grad_norm": 12.495001792907715,
      "learning_rate": 1.712603464459425e-05,
      "loss": 0.9023,
      "step": 8210
    },
    {
      "epoch": 2.0181684262214583,
      "grad_norm": 21.052478790283203,
      "learning_rate": 1.708336888813039e-05,
      "loss": 1.1585,
      "step": 8220
    },
    {
      "epoch": 2.020623618954088,
      "grad_norm": 13.065629959106445,
      "learning_rate": 1.7040703131666524e-05,
      "loss": 1.1244,
      "step": 8230
    },
    {
      "epoch": 2.0230788116867173,
      "grad_norm": 22.424846649169922,
      "learning_rate": 1.6998037375202663e-05,
      "loss": 1.237,
      "step": 8240
    },
    {
      "epoch": 2.025534004419347,
      "grad_norm": 17.325639724731445,
      "learning_rate": 1.6955371618738802e-05,
      "loss": 1.1232,
      "step": 8250
    },
    {
      "epoch": 2.0279891971519763,
      "grad_norm": 16.027664184570312,
      "learning_rate": 1.6912705862274938e-05,
      "loss": 1.0217,
      "step": 8260
    },
    {
      "epoch": 2.030444389884606,
      "grad_norm": 17.527490615844727,
      "learning_rate": 1.6870040105811077e-05,
      "loss": 1.0189,
      "step": 8270
    },
    {
      "epoch": 2.0328995826172354,
      "grad_norm": 18.969675064086914,
      "learning_rate": 1.6827374349347212e-05,
      "loss": 1.0506,
      "step": 8280
    },
    {
      "epoch": 2.035354775349865,
      "grad_norm": 20.442161560058594,
      "learning_rate": 1.678470859288335e-05,
      "loss": 1.0655,
      "step": 8290
    },
    {
      "epoch": 2.0378099680824944,
      "grad_norm": 16.91111946105957,
      "learning_rate": 1.674204283641949e-05,
      "loss": 1.1169,
      "step": 8300
    },
    {
      "epoch": 2.040265160815124,
      "grad_norm": 17.840587615966797,
      "learning_rate": 1.6699377079955626e-05,
      "loss": 1.0144,
      "step": 8310
    },
    {
      "epoch": 2.0427203535477534,
      "grad_norm": 25.302194595336914,
      "learning_rate": 1.6656711323491765e-05,
      "loss": 1.0939,
      "step": 8320
    },
    {
      "epoch": 2.045175546280383,
      "grad_norm": 18.766071319580078,
      "learning_rate": 1.6614045567027904e-05,
      "loss": 1.0242,
      "step": 8330
    },
    {
      "epoch": 2.0476307390130124,
      "grad_norm": 19.27387046813965,
      "learning_rate": 1.6571379810564043e-05,
      "loss": 1.1571,
      "step": 8340
    },
    {
      "epoch": 2.050085931745642,
      "grad_norm": 19.965360641479492,
      "learning_rate": 1.6528714054100182e-05,
      "loss": 0.9656,
      "step": 8350
    },
    {
      "epoch": 2.0525411244782714,
      "grad_norm": 22.152830123901367,
      "learning_rate": 1.6486048297636318e-05,
      "loss": 1.1352,
      "step": 8360
    },
    {
      "epoch": 2.054996317210901,
      "grad_norm": 19.450481414794922,
      "learning_rate": 1.6443382541172457e-05,
      "loss": 1.1613,
      "step": 8370
    },
    {
      "epoch": 2.0574515099435304,
      "grad_norm": 23.968475341796875,
      "learning_rate": 1.6400716784708596e-05,
      "loss": 1.1013,
      "step": 8380
    },
    {
      "epoch": 2.05990670267616,
      "grad_norm": 26.058176040649414,
      "learning_rate": 1.635805102824473e-05,
      "loss": 1.1944,
      "step": 8390
    },
    {
      "epoch": 2.0623618954087894,
      "grad_norm": 22.035259246826172,
      "learning_rate": 1.631538527178087e-05,
      "loss": 1.2078,
      "step": 8400
    },
    {
      "epoch": 2.064817088141419,
      "grad_norm": 16.706581115722656,
      "learning_rate": 1.627271951531701e-05,
      "loss": 1.0691,
      "step": 8410
    },
    {
      "epoch": 2.0672722808740485,
      "grad_norm": 14.316335678100586,
      "learning_rate": 1.6230053758853145e-05,
      "loss": 1.0867,
      "step": 8420
    },
    {
      "epoch": 2.069727473606678,
      "grad_norm": 18.167192459106445,
      "learning_rate": 1.6187388002389284e-05,
      "loss": 1.2421,
      "step": 8430
    },
    {
      "epoch": 2.0721826663393075,
      "grad_norm": 21.106788635253906,
      "learning_rate": 1.614472224592542e-05,
      "loss": 1.1106,
      "step": 8440
    },
    {
      "epoch": 2.074637859071937,
      "grad_norm": 14.524084091186523,
      "learning_rate": 1.610205648946156e-05,
      "loss": 1.1347,
      "step": 8450
    },
    {
      "epoch": 2.0770930518045665,
      "grad_norm": 17.766878128051758,
      "learning_rate": 1.6059390732997698e-05,
      "loss": 1.0397,
      "step": 8460
    },
    {
      "epoch": 2.0795482445371962,
      "grad_norm": 19.861465454101562,
      "learning_rate": 1.6016724976533833e-05,
      "loss": 1.1644,
      "step": 8470
    },
    {
      "epoch": 2.0820034372698255,
      "grad_norm": 17.594097137451172,
      "learning_rate": 1.5974059220069973e-05,
      "loss": 1.0316,
      "step": 8480
    },
    {
      "epoch": 2.0844586300024552,
      "grad_norm": 17.608762741088867,
      "learning_rate": 1.593139346360611e-05,
      "loss": 1.0977,
      "step": 8490
    },
    {
      "epoch": 2.0869138227350845,
      "grad_norm": 20.947696685791016,
      "learning_rate": 1.5888727707142247e-05,
      "loss": 1.0034,
      "step": 8500
    },
    {
      "epoch": 2.0893690154677143,
      "grad_norm": 10.888569831848145,
      "learning_rate": 1.5846061950678386e-05,
      "loss": 1.0282,
      "step": 8510
    },
    {
      "epoch": 2.0918242082003435,
      "grad_norm": 19.920236587524414,
      "learning_rate": 1.5803396194214522e-05,
      "loss": 1.1659,
      "step": 8520
    },
    {
      "epoch": 2.0942794009329733,
      "grad_norm": 19.03784942626953,
      "learning_rate": 1.576073043775066e-05,
      "loss": 1.1609,
      "step": 8530
    },
    {
      "epoch": 2.0967345936656026,
      "grad_norm": 21.478649139404297,
      "learning_rate": 1.57180646812868e-05,
      "loss": 1.1622,
      "step": 8540
    },
    {
      "epoch": 2.0991897863982323,
      "grad_norm": 16.874696731567383,
      "learning_rate": 1.5675398924822935e-05,
      "loss": 1.1339,
      "step": 8550
    },
    {
      "epoch": 2.1016449791308616,
      "grad_norm": 15.447196960449219,
      "learning_rate": 1.5632733168359075e-05,
      "loss": 0.9585,
      "step": 8560
    },
    {
      "epoch": 2.1041001718634913,
      "grad_norm": 23.850414276123047,
      "learning_rate": 1.5590067411895214e-05,
      "loss": 1.1196,
      "step": 8570
    },
    {
      "epoch": 2.1065553645961206,
      "grad_norm": 19.91905403137207,
      "learning_rate": 1.5547401655431353e-05,
      "loss": 1.1673,
      "step": 8580
    },
    {
      "epoch": 2.1090105573287503,
      "grad_norm": 13.294323921203613,
      "learning_rate": 1.550473589896749e-05,
      "loss": 1.0245,
      "step": 8590
    },
    {
      "epoch": 2.11146575006138,
      "grad_norm": 19.982513427734375,
      "learning_rate": 1.5462070142503627e-05,
      "loss": 1.1177,
      "step": 8600
    },
    {
      "epoch": 2.1139209427940093,
      "grad_norm": 16.7694149017334,
      "learning_rate": 1.5419404386039766e-05,
      "loss": 1.0119,
      "step": 8610
    },
    {
      "epoch": 2.1163761355266386,
      "grad_norm": 21.320159912109375,
      "learning_rate": 1.5376738629575905e-05,
      "loss": 1.1052,
      "step": 8620
    },
    {
      "epoch": 2.1188313282592683,
      "grad_norm": 21.057485580444336,
      "learning_rate": 1.533407287311204e-05,
      "loss": 1.0892,
      "step": 8630
    },
    {
      "epoch": 2.121286520991898,
      "grad_norm": 14.59815502166748,
      "learning_rate": 1.529140711664818e-05,
      "loss": 1.1009,
      "step": 8640
    },
    {
      "epoch": 2.1237417137245274,
      "grad_norm": 17.179777145385742,
      "learning_rate": 1.5248741360184316e-05,
      "loss": 1.2464,
      "step": 8650
    },
    {
      "epoch": 2.126196906457157,
      "grad_norm": 22.460237503051758,
      "learning_rate": 1.5206075603720455e-05,
      "loss": 1.129,
      "step": 8660
    },
    {
      "epoch": 2.1286520991897864,
      "grad_norm": 15.510202407836914,
      "learning_rate": 1.5163409847256594e-05,
      "loss": 1.2103,
      "step": 8670
    },
    {
      "epoch": 2.131107291922416,
      "grad_norm": 28.72451400756836,
      "learning_rate": 1.512074409079273e-05,
      "loss": 1.1389,
      "step": 8680
    },
    {
      "epoch": 2.1335624846550454,
      "grad_norm": 18.051513671875,
      "learning_rate": 1.5078078334328868e-05,
      "loss": 1.0459,
      "step": 8690
    },
    {
      "epoch": 2.136017677387675,
      "grad_norm": 16.414785385131836,
      "learning_rate": 1.5035412577865007e-05,
      "loss": 1.0352,
      "step": 8700
    },
    {
      "epoch": 2.1384728701203044,
      "grad_norm": 14.615889549255371,
      "learning_rate": 1.4992746821401143e-05,
      "loss": 1.1018,
      "step": 8710
    },
    {
      "epoch": 2.140928062852934,
      "grad_norm": 17.825075149536133,
      "learning_rate": 1.4950081064937282e-05,
      "loss": 1.0481,
      "step": 8720
    },
    {
      "epoch": 2.1433832555855634,
      "grad_norm": 16.822635650634766,
      "learning_rate": 1.490741530847342e-05,
      "loss": 1.1418,
      "step": 8730
    },
    {
      "epoch": 2.145838448318193,
      "grad_norm": 21.56620979309082,
      "learning_rate": 1.4864749552009558e-05,
      "loss": 1.0505,
      "step": 8740
    },
    {
      "epoch": 2.1482936410508224,
      "grad_norm": 19.928712844848633,
      "learning_rate": 1.4822083795545697e-05,
      "loss": 1.0871,
      "step": 8750
    },
    {
      "epoch": 2.150748833783452,
      "grad_norm": 15.24301528930664,
      "learning_rate": 1.4779418039081833e-05,
      "loss": 1.1661,
      "step": 8760
    },
    {
      "epoch": 2.1532040265160814,
      "grad_norm": 17.31894302368164,
      "learning_rate": 1.4736752282617972e-05,
      "loss": 1.1472,
      "step": 8770
    },
    {
      "epoch": 2.155659219248711,
      "grad_norm": 16.660869598388672,
      "learning_rate": 1.4694086526154111e-05,
      "loss": 1.0511,
      "step": 8780
    },
    {
      "epoch": 2.1581144119813405,
      "grad_norm": 15.913257598876953,
      "learning_rate": 1.4651420769690247e-05,
      "loss": 1.2323,
      "step": 8790
    },
    {
      "epoch": 2.16056960471397,
      "grad_norm": 21.237380981445312,
      "learning_rate": 1.4608755013226386e-05,
      "loss": 1.1445,
      "step": 8800
    },
    {
      "epoch": 2.1630247974465995,
      "grad_norm": 18.07123374938965,
      "learning_rate": 1.4566089256762521e-05,
      "loss": 1.079,
      "step": 8810
    },
    {
      "epoch": 2.165479990179229,
      "grad_norm": 18.349287033081055,
      "learning_rate": 1.452342350029866e-05,
      "loss": 1.0572,
      "step": 8820
    },
    {
      "epoch": 2.1679351829118585,
      "grad_norm": 16.67209815979004,
      "learning_rate": 1.44807577438348e-05,
      "loss": 1.0685,
      "step": 8830
    },
    {
      "epoch": 2.1703903756444882,
      "grad_norm": 22.757701873779297,
      "learning_rate": 1.4438091987370937e-05,
      "loss": 1.1132,
      "step": 8840
    },
    {
      "epoch": 2.1728455683771175,
      "grad_norm": 19.292972564697266,
      "learning_rate": 1.4395426230907074e-05,
      "loss": 1.0965,
      "step": 8850
    },
    {
      "epoch": 2.1753007611097472,
      "grad_norm": 22.037084579467773,
      "learning_rate": 1.4352760474443213e-05,
      "loss": 1.2606,
      "step": 8860
    },
    {
      "epoch": 2.1777559538423765,
      "grad_norm": 16.85980796813965,
      "learning_rate": 1.431009471797935e-05,
      "loss": 1.1436,
      "step": 8870
    },
    {
      "epoch": 2.1802111465750063,
      "grad_norm": 19.63576316833496,
      "learning_rate": 1.426742896151549e-05,
      "loss": 1.084,
      "step": 8880
    },
    {
      "epoch": 2.1826663393076355,
      "grad_norm": 19.262577056884766,
      "learning_rate": 1.4224763205051625e-05,
      "loss": 1.1689,
      "step": 8890
    },
    {
      "epoch": 2.1851215320402653,
      "grad_norm": 19.970802307128906,
      "learning_rate": 1.4182097448587764e-05,
      "loss": 1.1176,
      "step": 8900
    },
    {
      "epoch": 2.1875767247728946,
      "grad_norm": 16.135507583618164,
      "learning_rate": 1.4139431692123903e-05,
      "loss": 1.2397,
      "step": 8910
    },
    {
      "epoch": 2.1900319175055243,
      "grad_norm": 14.02221965789795,
      "learning_rate": 1.4096765935660039e-05,
      "loss": 1.0326,
      "step": 8920
    },
    {
      "epoch": 2.1924871102381536,
      "grad_norm": 20.41670036315918,
      "learning_rate": 1.4054100179196178e-05,
      "loss": 1.1645,
      "step": 8930
    },
    {
      "epoch": 2.1949423029707833,
      "grad_norm": 24.606266021728516,
      "learning_rate": 1.4011434422732317e-05,
      "loss": 1.1378,
      "step": 8940
    },
    {
      "epoch": 2.1973974957034126,
      "grad_norm": 16.780250549316406,
      "learning_rate": 1.3968768666268452e-05,
      "loss": 1.1071,
      "step": 8950
    },
    {
      "epoch": 2.1998526884360423,
      "grad_norm": 18.34297752380371,
      "learning_rate": 1.3926102909804591e-05,
      "loss": 1.023,
      "step": 8960
    },
    {
      "epoch": 2.2023078811686716,
      "grad_norm": 19.39040184020996,
      "learning_rate": 1.3883437153340729e-05,
      "loss": 0.9718,
      "step": 8970
    },
    {
      "epoch": 2.2047630739013013,
      "grad_norm": 18.300838470458984,
      "learning_rate": 1.3840771396876868e-05,
      "loss": 1.0097,
      "step": 8980
    },
    {
      "epoch": 2.2072182666339306,
      "grad_norm": 21.84113883972168,
      "learning_rate": 1.3798105640413007e-05,
      "loss": 1.0907,
      "step": 8990
    },
    {
      "epoch": 2.2096734593665603,
      "grad_norm": 18.452655792236328,
      "learning_rate": 1.3755439883949142e-05,
      "loss": 1.0398,
      "step": 9000
    },
    {
      "epoch": 2.2121286520991896,
      "grad_norm": 21.285856246948242,
      "learning_rate": 1.3712774127485281e-05,
      "loss": 1.137,
      "step": 9010
    },
    {
      "epoch": 2.2145838448318194,
      "grad_norm": 13.360007286071777,
      "learning_rate": 1.367010837102142e-05,
      "loss": 1.0987,
      "step": 9020
    },
    {
      "epoch": 2.2170390375644486,
      "grad_norm": 16.999271392822266,
      "learning_rate": 1.3627442614557556e-05,
      "loss": 1.0155,
      "step": 9030
    },
    {
      "epoch": 2.2194942302970784,
      "grad_norm": 27.196237564086914,
      "learning_rate": 1.3584776858093695e-05,
      "loss": 1.174,
      "step": 9040
    },
    {
      "epoch": 2.2219494230297077,
      "grad_norm": 24.817832946777344,
      "learning_rate": 1.354211110162983e-05,
      "loss": 1.2079,
      "step": 9050
    },
    {
      "epoch": 2.2244046157623374,
      "grad_norm": 15.447132110595703,
      "learning_rate": 1.349944534516597e-05,
      "loss": 1.0104,
      "step": 9060
    },
    {
      "epoch": 2.2268598084949667,
      "grad_norm": 16.564794540405273,
      "learning_rate": 1.3456779588702109e-05,
      "loss": 1.1198,
      "step": 9070
    },
    {
      "epoch": 2.2293150012275964,
      "grad_norm": 16.244123458862305,
      "learning_rate": 1.3414113832238246e-05,
      "loss": 1.1644,
      "step": 9080
    },
    {
      "epoch": 2.2317701939602257,
      "grad_norm": 21.54812240600586,
      "learning_rate": 1.3371448075774385e-05,
      "loss": 1.0794,
      "step": 9090
    },
    {
      "epoch": 2.2342253866928554,
      "grad_norm": 25.266372680664062,
      "learning_rate": 1.3328782319310524e-05,
      "loss": 1.2095,
      "step": 9100
    },
    {
      "epoch": 2.2366805794254847,
      "grad_norm": 19.00503158569336,
      "learning_rate": 1.328611656284666e-05,
      "loss": 1.1336,
      "step": 9110
    },
    {
      "epoch": 2.2391357721581144,
      "grad_norm": 17.925689697265625,
      "learning_rate": 1.3243450806382799e-05,
      "loss": 1.0886,
      "step": 9120
    },
    {
      "epoch": 2.2415909648907437,
      "grad_norm": 13.555427551269531,
      "learning_rate": 1.3200785049918934e-05,
      "loss": 1.1068,
      "step": 9130
    },
    {
      "epoch": 2.2440461576233734,
      "grad_norm": 14.979931831359863,
      "learning_rate": 1.3158119293455073e-05,
      "loss": 1.0535,
      "step": 9140
    },
    {
      "epoch": 2.246501350356003,
      "grad_norm": 21.33829116821289,
      "learning_rate": 1.3115453536991212e-05,
      "loss": 1.1861,
      "step": 9150
    },
    {
      "epoch": 2.2489565430886325,
      "grad_norm": 16.07343864440918,
      "learning_rate": 1.3072787780527348e-05,
      "loss": 0.9791,
      "step": 9160
    },
    {
      "epoch": 2.2514117358212618,
      "grad_norm": 17.45116424560547,
      "learning_rate": 1.3030122024063487e-05,
      "loss": 1.1282,
      "step": 9170
    },
    {
      "epoch": 2.2538669285538915,
      "grad_norm": 13.948600769042969,
      "learning_rate": 1.2987456267599626e-05,
      "loss": 1.0622,
      "step": 9180
    },
    {
      "epoch": 2.256322121286521,
      "grad_norm": 14.149149894714355,
      "learning_rate": 1.2944790511135763e-05,
      "loss": 1.064,
      "step": 9190
    },
    {
      "epoch": 2.2587773140191505,
      "grad_norm": 23.98715591430664,
      "learning_rate": 1.29021247546719e-05,
      "loss": 1.1679,
      "step": 9200
    },
    {
      "epoch": 2.26123250675178,
      "grad_norm": 19.339195251464844,
      "learning_rate": 1.2859458998208038e-05,
      "loss": 1.1368,
      "step": 9210
    },
    {
      "epoch": 2.2636876994844095,
      "grad_norm": 17.181062698364258,
      "learning_rate": 1.2816793241744177e-05,
      "loss": 1.0642,
      "step": 9220
    },
    {
      "epoch": 2.2661428922170392,
      "grad_norm": 18.2385196685791,
      "learning_rate": 1.2774127485280316e-05,
      "loss": 1.1072,
      "step": 9230
    },
    {
      "epoch": 2.2685980849496685,
      "grad_norm": 16.754404067993164,
      "learning_rate": 1.2731461728816452e-05,
      "loss": 1.1296,
      "step": 9240
    },
    {
      "epoch": 2.271053277682298,
      "grad_norm": 15.597738265991211,
      "learning_rate": 1.268879597235259e-05,
      "loss": 1.0794,
      "step": 9250
    },
    {
      "epoch": 2.2735084704149275,
      "grad_norm": 16.08374786376953,
      "learning_rate": 1.264613021588873e-05,
      "loss": 1.0202,
      "step": 9260
    },
    {
      "epoch": 2.2759636631475573,
      "grad_norm": 15.688918113708496,
      "learning_rate": 1.2603464459424865e-05,
      "loss": 0.9134,
      "step": 9270
    },
    {
      "epoch": 2.2784188558801866,
      "grad_norm": 21.4649600982666,
      "learning_rate": 1.2560798702961004e-05,
      "loss": 1.1026,
      "step": 9280
    },
    {
      "epoch": 2.2808740486128163,
      "grad_norm": 17.45024871826172,
      "learning_rate": 1.251813294649714e-05,
      "loss": 0.9791,
      "step": 9290
    },
    {
      "epoch": 2.2833292413454456,
      "grad_norm": 16.411863327026367,
      "learning_rate": 1.2475467190033279e-05,
      "loss": 1.1584,
      "step": 9300
    },
    {
      "epoch": 2.2857844340780753,
      "grad_norm": 14.091545104980469,
      "learning_rate": 1.2432801433569418e-05,
      "loss": 1.1289,
      "step": 9310
    },
    {
      "epoch": 2.2882396268107046,
      "grad_norm": 19.795665740966797,
      "learning_rate": 1.2390135677105555e-05,
      "loss": 0.9961,
      "step": 9320
    },
    {
      "epoch": 2.2906948195433343,
      "grad_norm": 19.474544525146484,
      "learning_rate": 1.2347469920641694e-05,
      "loss": 1.0835,
      "step": 9330
    },
    {
      "epoch": 2.2931500122759636,
      "grad_norm": 20.82025146484375,
      "learning_rate": 1.2304804164177832e-05,
      "loss": 1.16,
      "step": 9340
    },
    {
      "epoch": 2.2956052050085933,
      "grad_norm": 19.745105743408203,
      "learning_rate": 1.2262138407713969e-05,
      "loss": 1.1056,
      "step": 9350
    },
    {
      "epoch": 2.2980603977412226,
      "grad_norm": 18.279747009277344,
      "learning_rate": 1.2219472651250108e-05,
      "loss": 1.0146,
      "step": 9360
    },
    {
      "epoch": 2.3005155904738523,
      "grad_norm": 15.867898941040039,
      "learning_rate": 1.2176806894786245e-05,
      "loss": 1.0463,
      "step": 9370
    },
    {
      "epoch": 2.3029707832064816,
      "grad_norm": 29.078420639038086,
      "learning_rate": 1.2134141138322383e-05,
      "loss": 1.1099,
      "step": 9380
    },
    {
      "epoch": 2.3054259759391114,
      "grad_norm": 20.411958694458008,
      "learning_rate": 1.209147538185852e-05,
      "loss": 1.1507,
      "step": 9390
    },
    {
      "epoch": 2.3078811686717406,
      "grad_norm": 16.63461685180664,
      "learning_rate": 1.2048809625394659e-05,
      "loss": 1.0537,
      "step": 9400
    },
    {
      "epoch": 2.3103363614043704,
      "grad_norm": 19.317384719848633,
      "learning_rate": 1.2006143868930796e-05,
      "loss": 1.1298,
      "step": 9410
    },
    {
      "epoch": 2.3127915541369997,
      "grad_norm": 21.730953216552734,
      "learning_rate": 1.1963478112466934e-05,
      "loss": 1.085,
      "step": 9420
    },
    {
      "epoch": 2.3152467468696294,
      "grad_norm": 22.40767478942871,
      "learning_rate": 1.1920812356003073e-05,
      "loss": 1.0804,
      "step": 9430
    },
    {
      "epoch": 2.3177019396022587,
      "grad_norm": 13.176607131958008,
      "learning_rate": 1.1878146599539212e-05,
      "loss": 1.0194,
      "step": 9440
    },
    {
      "epoch": 2.3201571323348884,
      "grad_norm": 19.668107986450195,
      "learning_rate": 1.183548084307535e-05,
      "loss": 1.2601,
      "step": 9450
    },
    {
      "epoch": 2.3226123250675177,
      "grad_norm": 14.8628568649292,
      "learning_rate": 1.1792815086611487e-05,
      "loss": 1.0741,
      "step": 9460
    },
    {
      "epoch": 2.3250675178001474,
      "grad_norm": 14.512679100036621,
      "learning_rate": 1.1750149330147624e-05,
      "loss": 1.1863,
      "step": 9470
    },
    {
      "epoch": 2.3275227105327767,
      "grad_norm": 17.381954193115234,
      "learning_rate": 1.1707483573683763e-05,
      "loss": 1.2282,
      "step": 9480
    },
    {
      "epoch": 2.3299779032654064,
      "grad_norm": 18.249095916748047,
      "learning_rate": 1.16648178172199e-05,
      "loss": 1.1049,
      "step": 9490
    },
    {
      "epoch": 2.3324330959980357,
      "grad_norm": 17.215394973754883,
      "learning_rate": 1.1622152060756038e-05,
      "loss": 1.1587,
      "step": 9500
    },
    {
      "epoch": 2.3348882887306655,
      "grad_norm": 20.0782413482666,
      "learning_rate": 1.1579486304292175e-05,
      "loss": 1.1252,
      "step": 9510
    },
    {
      "epoch": 2.3373434814632947,
      "grad_norm": 18.968809127807617,
      "learning_rate": 1.1536820547828314e-05,
      "loss": 1.1309,
      "step": 9520
    },
    {
      "epoch": 2.3397986741959245,
      "grad_norm": 18.193836212158203,
      "learning_rate": 1.1494154791364451e-05,
      "loss": 1.0861,
      "step": 9530
    },
    {
      "epoch": 2.3422538669285538,
      "grad_norm": 18.610675811767578,
      "learning_rate": 1.1451489034900589e-05,
      "loss": 1.164,
      "step": 9540
    },
    {
      "epoch": 2.3447090596611835,
      "grad_norm": 21.605119705200195,
      "learning_rate": 1.1408823278436728e-05,
      "loss": 1.1424,
      "step": 9550
    },
    {
      "epoch": 2.3471642523938128,
      "grad_norm": 19.879846572875977,
      "learning_rate": 1.1366157521972865e-05,
      "loss": 1.0808,
      "step": 9560
    },
    {
      "epoch": 2.3496194451264425,
      "grad_norm": 16.75787353515625,
      "learning_rate": 1.1323491765509004e-05,
      "loss": 1.1863,
      "step": 9570
    },
    {
      "epoch": 2.352074637859072,
      "grad_norm": 16.523000717163086,
      "learning_rate": 1.1280826009045141e-05,
      "loss": 1.106,
      "step": 9580
    },
    {
      "epoch": 2.3545298305917015,
      "grad_norm": 21.74460220336914,
      "learning_rate": 1.1238160252581279e-05,
      "loss": 1.1765,
      "step": 9590
    },
    {
      "epoch": 2.356985023324331,
      "grad_norm": 22.154401779174805,
      "learning_rate": 1.1195494496117416e-05,
      "loss": 1.1278,
      "step": 9600
    },
    {
      "epoch": 2.3594402160569605,
      "grad_norm": 16.601442337036133,
      "learning_rate": 1.1152828739653555e-05,
      "loss": 1.1157,
      "step": 9610
    },
    {
      "epoch": 2.36189540878959,
      "grad_norm": 21.40376091003418,
      "learning_rate": 1.1110162983189692e-05,
      "loss": 1.1581,
      "step": 9620
    },
    {
      "epoch": 2.3643506015222195,
      "grad_norm": 15.618546485900879,
      "learning_rate": 1.106749722672583e-05,
      "loss": 0.9842,
      "step": 9630
    },
    {
      "epoch": 2.366805794254849,
      "grad_norm": 22.58897590637207,
      "learning_rate": 1.1024831470261967e-05,
      "loss": 1.2192,
      "step": 9640
    },
    {
      "epoch": 2.3692609869874786,
      "grad_norm": 19.123502731323242,
      "learning_rate": 1.0982165713798106e-05,
      "loss": 1.0818,
      "step": 9650
    },
    {
      "epoch": 2.371716179720108,
      "grad_norm": 21.173234939575195,
      "learning_rate": 1.0939499957334245e-05,
      "loss": 1.1144,
      "step": 9660
    },
    {
      "epoch": 2.3741713724527376,
      "grad_norm": 21.770994186401367,
      "learning_rate": 1.0896834200870382e-05,
      "loss": 1.0639,
      "step": 9670
    },
    {
      "epoch": 2.376626565185367,
      "grad_norm": 25.28310203552246,
      "learning_rate": 1.085416844440652e-05,
      "loss": 1.1504,
      "step": 9680
    },
    {
      "epoch": 2.3790817579179966,
      "grad_norm": 16.480798721313477,
      "learning_rate": 1.0811502687942659e-05,
      "loss": 1.0144,
      "step": 9690
    },
    {
      "epoch": 2.3815369506506263,
      "grad_norm": 15.570928573608398,
      "learning_rate": 1.0768836931478796e-05,
      "loss": 1.2264,
      "step": 9700
    },
    {
      "epoch": 2.3839921433832556,
      "grad_norm": 20.09842300415039,
      "learning_rate": 1.0726171175014933e-05,
      "loss": 1.2324,
      "step": 9710
    },
    {
      "epoch": 2.386447336115885,
      "grad_norm": 22.010820388793945,
      "learning_rate": 1.068350541855107e-05,
      "loss": 1.1007,
      "step": 9720
    },
    {
      "epoch": 2.3889025288485146,
      "grad_norm": 17.233793258666992,
      "learning_rate": 1.064083966208721e-05,
      "loss": 1.0342,
      "step": 9730
    },
    {
      "epoch": 2.3913577215811443,
      "grad_norm": 26.316696166992188,
      "learning_rate": 1.0598173905623347e-05,
      "loss": 1.2622,
      "step": 9740
    },
    {
      "epoch": 2.3938129143137736,
      "grad_norm": 30.639389038085938,
      "learning_rate": 1.0555508149159484e-05,
      "loss": 0.9347,
      "step": 9750
    },
    {
      "epoch": 2.396268107046403,
      "grad_norm": 20.91777992248535,
      "learning_rate": 1.0512842392695622e-05,
      "loss": 1.1329,
      "step": 9760
    },
    {
      "epoch": 2.3987232997790326,
      "grad_norm": 17.925710678100586,
      "learning_rate": 1.047017663623176e-05,
      "loss": 1.1227,
      "step": 9770
    },
    {
      "epoch": 2.4011784925116624,
      "grad_norm": 24.69709587097168,
      "learning_rate": 1.04275108797679e-05,
      "loss": 1.0184,
      "step": 9780
    },
    {
      "epoch": 2.4036336852442917,
      "grad_norm": 21.247314453125,
      "learning_rate": 1.0384845123304037e-05,
      "loss": 1.0392,
      "step": 9790
    },
    {
      "epoch": 2.406088877976921,
      "grad_norm": 25.906631469726562,
      "learning_rate": 1.0342179366840174e-05,
      "loss": 1.1788,
      "step": 9800
    },
    {
      "epoch": 2.4085440707095507,
      "grad_norm": 19.517513275146484,
      "learning_rate": 1.0299513610376313e-05,
      "loss": 1.2113,
      "step": 9810
    },
    {
      "epoch": 2.4109992634421804,
      "grad_norm": 14.722999572753906,
      "learning_rate": 1.025684785391245e-05,
      "loss": 1.2509,
      "step": 9820
    },
    {
      "epoch": 2.4134544561748097,
      "grad_norm": 27.130516052246094,
      "learning_rate": 1.0214182097448588e-05,
      "loss": 0.9892,
      "step": 9830
    },
    {
      "epoch": 2.4159096489074394,
      "grad_norm": 21.250513076782227,
      "learning_rate": 1.0171516340984725e-05,
      "loss": 1.1045,
      "step": 9840
    },
    {
      "epoch": 2.4183648416400687,
      "grad_norm": 29.143247604370117,
      "learning_rate": 1.0128850584520864e-05,
      "loss": 1.1239,
      "step": 9850
    },
    {
      "epoch": 2.4208200343726984,
      "grad_norm": 17.921056747436523,
      "learning_rate": 1.0086184828057002e-05,
      "loss": 1.1183,
      "step": 9860
    },
    {
      "epoch": 2.4232752271053277,
      "grad_norm": 22.35088348388672,
      "learning_rate": 1.0043519071593139e-05,
      "loss": 1.2047,
      "step": 9870
    },
    {
      "epoch": 2.4257304198379575,
      "grad_norm": 14.748705863952637,
      "learning_rate": 1.0000853315129278e-05,
      "loss": 1.0506,
      "step": 9880
    },
    {
      "epoch": 2.4281856125705867,
      "grad_norm": 17.165454864501953,
      "learning_rate": 9.958187558665415e-06,
      "loss": 1.131,
      "step": 9890
    },
    {
      "epoch": 2.4306408053032165,
      "grad_norm": 11.875280380249023,
      "learning_rate": 9.915521802201554e-06,
      "loss": 1.0834,
      "step": 9900
    },
    {
      "epoch": 2.4330959980358458,
      "grad_norm": 15.95248031616211,
      "learning_rate": 9.872856045737692e-06,
      "loss": 0.9465,
      "step": 9910
    },
    {
      "epoch": 2.4355511907684755,
      "grad_norm": 20.506786346435547,
      "learning_rate": 9.830190289273829e-06,
      "loss": 1.0432,
      "step": 9920
    },
    {
      "epoch": 2.4380063835011048,
      "grad_norm": 20.922691345214844,
      "learning_rate": 9.787524532809968e-06,
      "loss": 0.9807,
      "step": 9930
    },
    {
      "epoch": 2.4404615762337345,
      "grad_norm": 22.024152755737305,
      "learning_rate": 9.744858776346105e-06,
      "loss": 1.1831,
      "step": 9940
    },
    {
      "epoch": 2.442916768966364,
      "grad_norm": 17.44559669494629,
      "learning_rate": 9.702193019882243e-06,
      "loss": 1.1129,
      "step": 9950
    },
    {
      "epoch": 2.4453719616989935,
      "grad_norm": 16.75627899169922,
      "learning_rate": 9.65952726341838e-06,
      "loss": 1.0593,
      "step": 9960
    },
    {
      "epoch": 2.447827154431623,
      "grad_norm": 24.38159942626953,
      "learning_rate": 9.616861506954519e-06,
      "loss": 1.0498,
      "step": 9970
    },
    {
      "epoch": 2.4502823471642525,
      "grad_norm": 20.04468536376953,
      "learning_rate": 9.574195750490656e-06,
      "loss": 0.9875,
      "step": 9980
    },
    {
      "epoch": 2.452737539896882,
      "grad_norm": 23.425182342529297,
      "learning_rate": 9.531529994026794e-06,
      "loss": 1.0159,
      "step": 9990
    },
    {
      "epoch": 2.4551927326295115,
      "grad_norm": 16.631881713867188,
      "learning_rate": 9.488864237562933e-06,
      "loss": 1.1356,
      "step": 10000
    },
    {
      "epoch": 2.457647925362141,
      "grad_norm": 19.986553192138672,
      "learning_rate": 9.446198481099072e-06,
      "loss": 1.1838,
      "step": 10010
    },
    {
      "epoch": 2.4601031180947706,
      "grad_norm": 24.66484260559082,
      "learning_rate": 9.403532724635209e-06,
      "loss": 1.2131,
      "step": 10020
    },
    {
      "epoch": 2.4625583108274,
      "grad_norm": 14.605507850646973,
      "learning_rate": 9.360866968171346e-06,
      "loss": 1.0244,
      "step": 10030
    },
    {
      "epoch": 2.4650135035600296,
      "grad_norm": 17.090932846069336,
      "learning_rate": 9.318201211707484e-06,
      "loss": 1.0669,
      "step": 10040
    },
    {
      "epoch": 2.467468696292659,
      "grad_norm": 19.04582405090332,
      "learning_rate": 9.275535455243623e-06,
      "loss": 1.1278,
      "step": 10050
    },
    {
      "epoch": 2.4699238890252886,
      "grad_norm": 20.46800422668457,
      "learning_rate": 9.23286969877976e-06,
      "loss": 0.9989,
      "step": 10060
    },
    {
      "epoch": 2.472379081757918,
      "grad_norm": 15.990005493164062,
      "learning_rate": 9.190203942315897e-06,
      "loss": 1.0814,
      "step": 10070
    },
    {
      "epoch": 2.4748342744905476,
      "grad_norm": 18.157024383544922,
      "learning_rate": 9.147538185852035e-06,
      "loss": 1.0817,
      "step": 10080
    },
    {
      "epoch": 2.477289467223177,
      "grad_norm": 29.86260414123535,
      "learning_rate": 9.104872429388174e-06,
      "loss": 1.2085,
      "step": 10090
    },
    {
      "epoch": 2.4797446599558066,
      "grad_norm": 18.929109573364258,
      "learning_rate": 9.062206672924311e-06,
      "loss": 1.0909,
      "step": 10100
    },
    {
      "epoch": 2.482199852688436,
      "grad_norm": 19.329341888427734,
      "learning_rate": 9.019540916460448e-06,
      "loss": 1.133,
      "step": 10110
    },
    {
      "epoch": 2.4846550454210656,
      "grad_norm": 16.286571502685547,
      "learning_rate": 8.976875159996587e-06,
      "loss": 1.0609,
      "step": 10120
    },
    {
      "epoch": 2.487110238153695,
      "grad_norm": 16.117618560791016,
      "learning_rate": 8.934209403532726e-06,
      "loss": 0.9142,
      "step": 10130
    },
    {
      "epoch": 2.4895654308863246,
      "grad_norm": 22.395877838134766,
      "learning_rate": 8.891543647068864e-06,
      "loss": 1.0114,
      "step": 10140
    },
    {
      "epoch": 2.492020623618954,
      "grad_norm": 21.8512020111084,
      "learning_rate": 8.848877890605001e-06,
      "loss": 1.0687,
      "step": 10150
    },
    {
      "epoch": 2.4944758163515837,
      "grad_norm": 22.21523666381836,
      "learning_rate": 8.806212134141138e-06,
      "loss": 1.1252,
      "step": 10160
    },
    {
      "epoch": 2.496931009084213,
      "grad_norm": 16.28200340270996,
      "learning_rate": 8.763546377677277e-06,
      "loss": 1.0999,
      "step": 10170
    },
    {
      "epoch": 2.4993862018168427,
      "grad_norm": 21.927230834960938,
      "learning_rate": 8.720880621213415e-06,
      "loss": 1.1409,
      "step": 10180
    },
    {
      "epoch": 2.501841394549472,
      "grad_norm": 24.147472381591797,
      "learning_rate": 8.678214864749552e-06,
      "loss": 1.0904,
      "step": 10190
    },
    {
      "epoch": 2.5042965872821017,
      "grad_norm": 24.60660171508789,
      "learning_rate": 8.63554910828569e-06,
      "loss": 1.3013,
      "step": 10200
    },
    {
      "epoch": 2.5067517800147314,
      "grad_norm": 18.041587829589844,
      "learning_rate": 8.597149927468214e-06,
      "loss": 1.1672,
      "step": 10210
    },
    {
      "epoch": 2.5092069727473607,
      "grad_norm": 18.02228355407715,
      "learning_rate": 8.554484171004353e-06,
      "loss": 1.1829,
      "step": 10220
    },
    {
      "epoch": 2.51166216547999,
      "grad_norm": 15.744004249572754,
      "learning_rate": 8.51181841454049e-06,
      "loss": 1.0875,
      "step": 10230
    },
    {
      "epoch": 2.5141173582126197,
      "grad_norm": 20.81978416442871,
      "learning_rate": 8.469152658076628e-06,
      "loss": 1.051,
      "step": 10240
    },
    {
      "epoch": 2.5165725509452495,
      "grad_norm": 16.36644172668457,
      "learning_rate": 8.426486901612767e-06,
      "loss": 0.9155,
      "step": 10250
    },
    {
      "epoch": 2.5190277436778787,
      "grad_norm": 20.088598251342773,
      "learning_rate": 8.383821145148904e-06,
      "loss": 1.213,
      "step": 10260
    },
    {
      "epoch": 2.521482936410508,
      "grad_norm": 19.853660583496094,
      "learning_rate": 8.341155388685042e-06,
      "loss": 1.1685,
      "step": 10270
    },
    {
      "epoch": 2.5239381291431378,
      "grad_norm": 19.194026947021484,
      "learning_rate": 8.298489632221179e-06,
      "loss": 1.2266,
      "step": 10280
    },
    {
      "epoch": 2.5263933218757675,
      "grad_norm": 16.620634078979492,
      "learning_rate": 8.255823875757318e-06,
      "loss": 1.0976,
      "step": 10290
    },
    {
      "epoch": 2.5288485146083968,
      "grad_norm": 21.07605743408203,
      "learning_rate": 8.213158119293455e-06,
      "loss": 1.2125,
      "step": 10300
    },
    {
      "epoch": 2.531303707341026,
      "grad_norm": 19.368391036987305,
      "learning_rate": 8.170492362829593e-06,
      "loss": 1.1365,
      "step": 10310
    },
    {
      "epoch": 2.533758900073656,
      "grad_norm": 21.08159065246582,
      "learning_rate": 8.127826606365732e-06,
      "loss": 1.056,
      "step": 10320
    },
    {
      "epoch": 2.5362140928062855,
      "grad_norm": 14.191234588623047,
      "learning_rate": 8.085160849901869e-06,
      "loss": 1.0784,
      "step": 10330
    },
    {
      "epoch": 2.538669285538915,
      "grad_norm": 13.580154418945312,
      "learning_rate": 8.042495093438008e-06,
      "loss": 0.9595,
      "step": 10340
    },
    {
      "epoch": 2.541124478271544,
      "grad_norm": 19.795982360839844,
      "learning_rate": 7.999829336974145e-06,
      "loss": 1.1239,
      "step": 10350
    },
    {
      "epoch": 2.543579671004174,
      "grad_norm": 17.18595314025879,
      "learning_rate": 7.957163580510283e-06,
      "loss": 1.219,
      "step": 10360
    },
    {
      "epoch": 2.5460348637368035,
      "grad_norm": 13.824850082397461,
      "learning_rate": 7.914497824046422e-06,
      "loss": 1.1801,
      "step": 10370
    },
    {
      "epoch": 2.548490056469433,
      "grad_norm": 16.07614517211914,
      "learning_rate": 7.871832067582559e-06,
      "loss": 1.1903,
      "step": 10380
    },
    {
      "epoch": 2.550945249202062,
      "grad_norm": 17.86272621154785,
      "learning_rate": 7.829166311118696e-06,
      "loss": 1.1115,
      "step": 10390
    },
    {
      "epoch": 2.553400441934692,
      "grad_norm": 12.833165168762207,
      "learning_rate": 7.786500554654834e-06,
      "loss": 1.0143,
      "step": 10400
    },
    {
      "epoch": 2.5558556346673216,
      "grad_norm": 18.319725036621094,
      "learning_rate": 7.743834798190973e-06,
      "loss": 1.0538,
      "step": 10410
    },
    {
      "epoch": 2.558310827399951,
      "grad_norm": 12.839611053466797,
      "learning_rate": 7.70116904172711e-06,
      "loss": 1.077,
      "step": 10420
    },
    {
      "epoch": 2.56076602013258,
      "grad_norm": 27.52109146118164,
      "learning_rate": 7.658503285263247e-06,
      "loss": 0.993,
      "step": 10430
    },
    {
      "epoch": 2.56322121286521,
      "grad_norm": 28.990299224853516,
      "learning_rate": 7.615837528799385e-06,
      "loss": 1.195,
      "step": 10440
    },
    {
      "epoch": 2.5656764055978396,
      "grad_norm": 19.619426727294922,
      "learning_rate": 7.5731717723355244e-06,
      "loss": 1.3174,
      "step": 10450
    },
    {
      "epoch": 2.568131598330469,
      "grad_norm": 18.133085250854492,
      "learning_rate": 7.530506015871662e-06,
      "loss": 1.0669,
      "step": 10460
    },
    {
      "epoch": 2.5705867910630986,
      "grad_norm": 16.477413177490234,
      "learning_rate": 7.4878402594078e-06,
      "loss": 1.0079,
      "step": 10470
    },
    {
      "epoch": 2.573041983795728,
      "grad_norm": 24.226272583007812,
      "learning_rate": 7.445174502943937e-06,
      "loss": 1.2413,
      "step": 10480
    },
    {
      "epoch": 2.5754971765283576,
      "grad_norm": 20.38350486755371,
      "learning_rate": 7.402508746480076e-06,
      "loss": 1.1016,
      "step": 10490
    },
    {
      "epoch": 2.577952369260987,
      "grad_norm": 22.793563842773438,
      "learning_rate": 7.359842990016214e-06,
      "loss": 1.1076,
      "step": 10500
    },
    {
      "epoch": 2.5804075619936166,
      "grad_norm": 22.51718521118164,
      "learning_rate": 7.317177233552351e-06,
      "loss": 1.1677,
      "step": 10510
    },
    {
      "epoch": 2.582862754726246,
      "grad_norm": 13.39942455291748,
      "learning_rate": 7.274511477088488e-06,
      "loss": 1.0906,
      "step": 10520
    },
    {
      "epoch": 2.5853179474588757,
      "grad_norm": 16.60188865661621,
      "learning_rate": 7.231845720624627e-06,
      "loss": 1.0905,
      "step": 10530
    },
    {
      "epoch": 2.587773140191505,
      "grad_norm": 16.600839614868164,
      "learning_rate": 7.1891799641607655e-06,
      "loss": 1.0061,
      "step": 10540
    },
    {
      "epoch": 2.5902283329241347,
      "grad_norm": 18.708602905273438,
      "learning_rate": 7.146514207696903e-06,
      "loss": 1.0144,
      "step": 10550
    },
    {
      "epoch": 2.592683525656764,
      "grad_norm": 27.429868698120117,
      "learning_rate": 7.10384845123304e-06,
      "loss": 1.1671,
      "step": 10560
    },
    {
      "epoch": 2.5951387183893937,
      "grad_norm": 23.018688201904297,
      "learning_rate": 7.061182694769179e-06,
      "loss": 1.1225,
      "step": 10570
    },
    {
      "epoch": 2.597593911122023,
      "grad_norm": 14.432558059692383,
      "learning_rate": 7.0185169383053165e-06,
      "loss": 1.0147,
      "step": 10580
    },
    {
      "epoch": 2.6000491038546527,
      "grad_norm": 16.140169143676758,
      "learning_rate": 6.975851181841455e-06,
      "loss": 1.2301,
      "step": 10590
    },
    {
      "epoch": 2.602504296587282,
      "grad_norm": 22.114316940307617,
      "learning_rate": 6.933185425377592e-06,
      "loss": 1.1244,
      "step": 10600
    },
    {
      "epoch": 2.6049594893199117,
      "grad_norm": 18.897705078125,
      "learning_rate": 6.890519668913731e-06,
      "loss": 1.0544,
      "step": 10610
    },
    {
      "epoch": 2.607414682052541,
      "grad_norm": 18.32457733154297,
      "learning_rate": 6.847853912449868e-06,
      "loss": 1.0707,
      "step": 10620
    },
    {
      "epoch": 2.6098698747851707,
      "grad_norm": 21.355478286743164,
      "learning_rate": 6.805188155986006e-06,
      "loss": 1.19,
      "step": 10630
    },
    {
      "epoch": 2.6123250675178,
      "grad_norm": 14.636256217956543,
      "learning_rate": 6.762522399522144e-06,
      "loss": 1.1398,
      "step": 10640
    },
    {
      "epoch": 2.6147802602504298,
      "grad_norm": 13.082428932189941,
      "learning_rate": 6.719856643058282e-06,
      "loss": 1.1392,
      "step": 10650
    },
    {
      "epoch": 2.617235452983059,
      "grad_norm": 20.23357582092285,
      "learning_rate": 6.67719088659442e-06,
      "loss": 1.1337,
      "step": 10660
    },
    {
      "epoch": 2.6196906457156888,
      "grad_norm": 18.43614387512207,
      "learning_rate": 6.6345251301305575e-06,
      "loss": 1.1666,
      "step": 10670
    },
    {
      "epoch": 2.622145838448318,
      "grad_norm": 19.238801956176758,
      "learning_rate": 6.591859373666695e-06,
      "loss": 1.2193,
      "step": 10680
    },
    {
      "epoch": 2.624601031180948,
      "grad_norm": 25.19541358947754,
      "learning_rate": 6.549193617202834e-06,
      "loss": 1.0228,
      "step": 10690
    },
    {
      "epoch": 2.627056223913577,
      "grad_norm": 18.00684356689453,
      "learning_rate": 6.506527860738971e-06,
      "loss": 1.1005,
      "step": 10700
    },
    {
      "epoch": 2.629511416646207,
      "grad_norm": 19.14787483215332,
      "learning_rate": 6.463862104275109e-06,
      "loss": 1.0075,
      "step": 10710
    },
    {
      "epoch": 2.631966609378836,
      "grad_norm": 18.51967430114746,
      "learning_rate": 6.421196347811247e-06,
      "loss": 1.0999,
      "step": 10720
    },
    {
      "epoch": 2.634421802111466,
      "grad_norm": 23.233903884887695,
      "learning_rate": 6.378530591347386e-06,
      "loss": 1.1159,
      "step": 10730
    },
    {
      "epoch": 2.636876994844095,
      "grad_norm": 20.450531005859375,
      "learning_rate": 6.335864834883523e-06,
      "loss": 1.1992,
      "step": 10740
    },
    {
      "epoch": 2.639332187576725,
      "grad_norm": 15.68912124633789,
      "learning_rate": 6.29319907841966e-06,
      "loss": 1.0665,
      "step": 10750
    },
    {
      "epoch": 2.6417873803093546,
      "grad_norm": 23.707534790039062,
      "learning_rate": 6.2505333219557985e-06,
      "loss": 1.0993,
      "step": 10760
    },
    {
      "epoch": 2.644242573041984,
      "grad_norm": 20.10622787475586,
      "learning_rate": 6.207867565491937e-06,
      "loss": 1.2429,
      "step": 10770
    },
    {
      "epoch": 2.646697765774613,
      "grad_norm": 20.68317222595215,
      "learning_rate": 6.165201809028075e-06,
      "loss": 1.1452,
      "step": 10780
    },
    {
      "epoch": 2.649152958507243,
      "grad_norm": 23.056949615478516,
      "learning_rate": 6.122536052564212e-06,
      "loss": 1.2537,
      "step": 10790
    },
    {
      "epoch": 2.6516081512398726,
      "grad_norm": 17.324907302856445,
      "learning_rate": 6.0798702961003495e-06,
      "loss": 1.1412,
      "step": 10800
    },
    {
      "epoch": 2.654063343972502,
      "grad_norm": 15.678855895996094,
      "learning_rate": 6.037204539636488e-06,
      "loss": 1.0712,
      "step": 10810
    },
    {
      "epoch": 2.656518536705131,
      "grad_norm": 13.569371223449707,
      "learning_rate": 5.994538783172626e-06,
      "loss": 1.1922,
      "step": 10820
    },
    {
      "epoch": 2.658973729437761,
      "grad_norm": 17.417898178100586,
      "learning_rate": 5.951873026708764e-06,
      "loss": 1.1545,
      "step": 10830
    },
    {
      "epoch": 2.6614289221703906,
      "grad_norm": 19.763521194458008,
      "learning_rate": 5.909207270244901e-06,
      "loss": 1.0475,
      "step": 10840
    },
    {
      "epoch": 2.66388411490302,
      "grad_norm": 23.766216278076172,
      "learning_rate": 5.8665415137810395e-06,
      "loss": 1.1279,
      "step": 10850
    },
    {
      "epoch": 2.666339307635649,
      "grad_norm": 21.235376358032227,
      "learning_rate": 5.823875757317177e-06,
      "loss": 1.0771,
      "step": 10860
    },
    {
      "epoch": 2.668794500368279,
      "grad_norm": 16.423538208007812,
      "learning_rate": 5.781210000853315e-06,
      "loss": 1.0929,
      "step": 10870
    },
    {
      "epoch": 2.6712496931009087,
      "grad_norm": 14.06312084197998,
      "learning_rate": 5.738544244389453e-06,
      "loss": 1.0426,
      "step": 10880
    },
    {
      "epoch": 2.673704885833538,
      "grad_norm": 22.513593673706055,
      "learning_rate": 5.695878487925591e-06,
      "loss": 0.9996,
      "step": 10890
    },
    {
      "epoch": 2.676160078566167,
      "grad_norm": 15.004314422607422,
      "learning_rate": 5.653212731461729e-06,
      "loss": 1.0848,
      "step": 10900
    },
    {
      "epoch": 2.678615271298797,
      "grad_norm": 20.580524444580078,
      "learning_rate": 5.610546974997867e-06,
      "loss": 1.1507,
      "step": 10910
    },
    {
      "epoch": 2.6810704640314267,
      "grad_norm": 22.698137283325195,
      "learning_rate": 5.567881218534004e-06,
      "loss": 1.1351,
      "step": 10920
    },
    {
      "epoch": 2.683525656764056,
      "grad_norm": 15.053692817687988,
      "learning_rate": 5.525215462070142e-06,
      "loss": 1.0718,
      "step": 10930
    },
    {
      "epoch": 2.6859808494966853,
      "grad_norm": 26.46512222290039,
      "learning_rate": 5.482549705606281e-06,
      "loss": 1.0631,
      "step": 10940
    },
    {
      "epoch": 2.688436042229315,
      "grad_norm": 20.104171752929688,
      "learning_rate": 5.439883949142419e-06,
      "loss": 1.0334,
      "step": 10950
    },
    {
      "epoch": 2.6908912349619447,
      "grad_norm": 22.447587966918945,
      "learning_rate": 5.397218192678556e-06,
      "loss": 1.0916,
      "step": 10960
    },
    {
      "epoch": 2.693346427694574,
      "grad_norm": 21.89776039123535,
      "learning_rate": 5.354552436214694e-06,
      "loss": 1.0306,
      "step": 10970
    },
    {
      "epoch": 2.6958016204272033,
      "grad_norm": 17.832674026489258,
      "learning_rate": 5.311886679750832e-06,
      "loss": 1.2126,
      "step": 10980
    },
    {
      "epoch": 2.698256813159833,
      "grad_norm": 22.931520462036133,
      "learning_rate": 5.269220923286971e-06,
      "loss": 1.1031,
      "step": 10990
    },
    {
      "epoch": 2.7007120058924627,
      "grad_norm": 15.96107006072998,
      "learning_rate": 5.226555166823108e-06,
      "loss": 1.0598,
      "step": 11000
    },
    {
      "epoch": 2.703167198625092,
      "grad_norm": 19.809566497802734,
      "learning_rate": 5.183889410359246e-06,
      "loss": 1.0381,
      "step": 11010
    },
    {
      "epoch": 2.7056223913577213,
      "grad_norm": 22.260047912597656,
      "learning_rate": 5.1412236538953834e-06,
      "loss": 1.1233,
      "step": 11020
    },
    {
      "epoch": 2.708077584090351,
      "grad_norm": 14.912850379943848,
      "learning_rate": 5.098557897431522e-06,
      "loss": 1.0892,
      "step": 11030
    },
    {
      "epoch": 2.7105327768229808,
      "grad_norm": 19.225772857666016,
      "learning_rate": 5.055892140967659e-06,
      "loss": 1.2231,
      "step": 11040
    },
    {
      "epoch": 2.71298796955561,
      "grad_norm": 20.705368041992188,
      "learning_rate": 5.013226384503798e-06,
      "loss": 1.1589,
      "step": 11050
    },
    {
      "epoch": 2.71544316228824,
      "grad_norm": 21.24703025817871,
      "learning_rate": 4.970560628039935e-06,
      "loss": 1.176,
      "step": 11060
    },
    {
      "epoch": 2.717898355020869,
      "grad_norm": 31.71480941772461,
      "learning_rate": 4.9278948715760735e-06,
      "loss": 1.0949,
      "step": 11070
    },
    {
      "epoch": 2.720353547753499,
      "grad_norm": 14.42390251159668,
      "learning_rate": 4.885229115112211e-06,
      "loss": 1.0548,
      "step": 11080
    },
    {
      "epoch": 2.722808740486128,
      "grad_norm": 17.240713119506836,
      "learning_rate": 4.842563358648349e-06,
      "loss": 1.0674,
      "step": 11090
    },
    {
      "epoch": 2.725263933218758,
      "grad_norm": 21.04183578491211,
      "learning_rate": 4.799897602184487e-06,
      "loss": 1.1491,
      "step": 11100
    },
    {
      "epoch": 2.727719125951387,
      "grad_norm": 21.367624282836914,
      "learning_rate": 4.757231845720625e-06,
      "loss": 1.0942,
      "step": 11110
    },
    {
      "epoch": 2.730174318684017,
      "grad_norm": 16.6458740234375,
      "learning_rate": 4.714566089256763e-06,
      "loss": 1.144,
      "step": 11120
    },
    {
      "epoch": 2.732629511416646,
      "grad_norm": 16.8698673248291,
      "learning_rate": 4.671900332792901e-06,
      "loss": 1.0804,
      "step": 11130
    },
    {
      "epoch": 2.735084704149276,
      "grad_norm": 24.643524169921875,
      "learning_rate": 4.629234576329038e-06,
      "loss": 1.2704,
      "step": 11140
    },
    {
      "epoch": 2.737539896881905,
      "grad_norm": 29.173423767089844,
      "learning_rate": 4.586568819865176e-06,
      "loss": 1.1773,
      "step": 11150
    },
    {
      "epoch": 2.739995089614535,
      "grad_norm": 26.024906158447266,
      "learning_rate": 4.5439030634013145e-06,
      "loss": 1.1224,
      "step": 11160
    },
    {
      "epoch": 2.742450282347164,
      "grad_norm": 16.023746490478516,
      "learning_rate": 4.501237306937453e-06,
      "loss": 0.9842,
      "step": 11170
    },
    {
      "epoch": 2.744905475079794,
      "grad_norm": 11.606660842895508,
      "learning_rate": 4.45857155047359e-06,
      "loss": 1.1178,
      "step": 11180
    },
    {
      "epoch": 2.747360667812423,
      "grad_norm": 17.346038818359375,
      "learning_rate": 4.415905794009728e-06,
      "loss": 1.0542,
      "step": 11190
    },
    {
      "epoch": 2.749815860545053,
      "grad_norm": 18.603315353393555,
      "learning_rate": 4.3732400375458655e-06,
      "loss": 1.097,
      "step": 11200
    },
    {
      "epoch": 2.752271053277682,
      "grad_norm": 13.45842456817627,
      "learning_rate": 4.330574281082004e-06,
      "loss": 0.9295,
      "step": 11210
    },
    {
      "epoch": 2.754726246010312,
      "grad_norm": 18.640565872192383,
      "learning_rate": 4.287908524618142e-06,
      "loss": 0.9845,
      "step": 11220
    },
    {
      "epoch": 2.757181438742941,
      "grad_norm": 22.448331832885742,
      "learning_rate": 4.24524276815428e-06,
      "loss": 1.0838,
      "step": 11230
    },
    {
      "epoch": 2.759636631475571,
      "grad_norm": 18.597923278808594,
      "learning_rate": 4.202577011690417e-06,
      "loss": 1.0488,
      "step": 11240
    },
    {
      "epoch": 2.7620918242082,
      "grad_norm": 28.119970321655273,
      "learning_rate": 4.1599112552265555e-06,
      "loss": 1.1544,
      "step": 11250
    },
    {
      "epoch": 2.76454701694083,
      "grad_norm": 19.56290626525879,
      "learning_rate": 4.117245498762693e-06,
      "loss": 1.1484,
      "step": 11260
    },
    {
      "epoch": 2.7670022096734592,
      "grad_norm": 20.906646728515625,
      "learning_rate": 4.074579742298831e-06,
      "loss": 1.0725,
      "step": 11270
    },
    {
      "epoch": 2.769457402406089,
      "grad_norm": 18.580686569213867,
      "learning_rate": 4.031913985834969e-06,
      "loss": 1.1748,
      "step": 11280
    },
    {
      "epoch": 2.7719125951387182,
      "grad_norm": 22.047286987304688,
      "learning_rate": 3.989248229371107e-06,
      "loss": 1.175,
      "step": 11290
    },
    {
      "epoch": 2.774367787871348,
      "grad_norm": 14.96498966217041,
      "learning_rate": 3.946582472907245e-06,
      "loss": 1.1701,
      "step": 11300
    },
    {
      "epoch": 2.7768229806039773,
      "grad_norm": 14.71438217163086,
      "learning_rate": 3.903916716443383e-06,
      "loss": 1.1431,
      "step": 11310
    },
    {
      "epoch": 2.779278173336607,
      "grad_norm": 12.544726371765137,
      "learning_rate": 3.86125095997952e-06,
      "loss": 1.1073,
      "step": 11320
    },
    {
      "epoch": 2.7817333660692363,
      "grad_norm": 14.293770790100098,
      "learning_rate": 3.818585203515658e-06,
      "loss": 1.1153,
      "step": 11330
    },
    {
      "epoch": 2.784188558801866,
      "grad_norm": 19.111804962158203,
      "learning_rate": 3.775919447051796e-06,
      "loss": 1.0828,
      "step": 11340
    },
    {
      "epoch": 2.7866437515344957,
      "grad_norm": 18.210725784301758,
      "learning_rate": 3.7332536905879343e-06,
      "loss": 1.1085,
      "step": 11350
    },
    {
      "epoch": 2.789098944267125,
      "grad_norm": 16.64327621459961,
      "learning_rate": 3.690587934124072e-06,
      "loss": 1.0295,
      "step": 11360
    },
    {
      "epoch": 2.7915541369997543,
      "grad_norm": 17.044246673583984,
      "learning_rate": 3.6479221776602102e-06,
      "loss": 1.1311,
      "step": 11370
    },
    {
      "epoch": 2.794009329732384,
      "grad_norm": 22.267562866210938,
      "learning_rate": 3.605256421196348e-06,
      "loss": 1.0696,
      "step": 11380
    },
    {
      "epoch": 2.7964645224650138,
      "grad_norm": 23.18438720703125,
      "learning_rate": 3.562590664732486e-06,
      "loss": 1.093,
      "step": 11390
    },
    {
      "epoch": 2.798919715197643,
      "grad_norm": 20.21013832092285,
      "learning_rate": 3.5199249082686235e-06,
      "loss": 1.0249,
      "step": 11400
    },
    {
      "epoch": 2.8013749079302723,
      "grad_norm": 18.4291934967041,
      "learning_rate": 3.477259151804762e-06,
      "loss": 1.1145,
      "step": 11410
    },
    {
      "epoch": 2.803830100662902,
      "grad_norm": 16.65570640563965,
      "learning_rate": 3.4345933953408994e-06,
      "loss": 1.0437,
      "step": 11420
    },
    {
      "epoch": 2.806285293395532,
      "grad_norm": 24.80402374267578,
      "learning_rate": 3.3919276388770376e-06,
      "loss": 1.2995,
      "step": 11430
    },
    {
      "epoch": 2.808740486128161,
      "grad_norm": 13.507034301757812,
      "learning_rate": 3.3492618824131753e-06,
      "loss": 1.0014,
      "step": 11440
    },
    {
      "epoch": 2.8111956788607904,
      "grad_norm": 19.60127830505371,
      "learning_rate": 3.3065961259493135e-06,
      "loss": 1.0905,
      "step": 11450
    },
    {
      "epoch": 2.81365087159342,
      "grad_norm": 18.799470901489258,
      "learning_rate": 3.263930369485451e-06,
      "loss": 1.1851,
      "step": 11460
    },
    {
      "epoch": 2.81610606432605,
      "grad_norm": 21.96060562133789,
      "learning_rate": 3.2212646130215894e-06,
      "loss": 1.1847,
      "step": 11470
    },
    {
      "epoch": 2.818561257058679,
      "grad_norm": 24.118297576904297,
      "learning_rate": 3.1785988565577268e-06,
      "loss": 1.1663,
      "step": 11480
    },
    {
      "epoch": 2.8210164497913084,
      "grad_norm": 24.622764587402344,
      "learning_rate": 3.135933100093865e-06,
      "loss": 1.2558,
      "step": 11490
    },
    {
      "epoch": 2.823471642523938,
      "grad_norm": 18.797176361083984,
      "learning_rate": 3.0932673436300027e-06,
      "loss": 1.1415,
      "step": 11500
    },
    {
      "epoch": 2.825926835256568,
      "grad_norm": 14.243401527404785,
      "learning_rate": 3.0506015871661404e-06,
      "loss": 1.1312,
      "step": 11510
    },
    {
      "epoch": 2.828382027989197,
      "grad_norm": 23.165142059326172,
      "learning_rate": 3.0079358307022786e-06,
      "loss": 1.1326,
      "step": 11520
    },
    {
      "epoch": 2.8308372207218264,
      "grad_norm": 22.009807586669922,
      "learning_rate": 2.9652700742384164e-06,
      "loss": 1.0726,
      "step": 11530
    },
    {
      "epoch": 2.833292413454456,
      "grad_norm": 17.191225051879883,
      "learning_rate": 2.922604317774554e-06,
      "loss": 1.101,
      "step": 11540
    },
    {
      "epoch": 2.835747606187086,
      "grad_norm": 16.55961036682129,
      "learning_rate": 2.8799385613106923e-06,
      "loss": 1.0307,
      "step": 11550
    },
    {
      "epoch": 2.838202798919715,
      "grad_norm": 16.87339210510254,
      "learning_rate": 2.83727280484683e-06,
      "loss": 1.2014,
      "step": 11560
    },
    {
      "epoch": 2.8406579916523444,
      "grad_norm": 21.342803955078125,
      "learning_rate": 2.7946070483829678e-06,
      "loss": 1.0764,
      "step": 11570
    },
    {
      "epoch": 2.843113184384974,
      "grad_norm": 16.553335189819336,
      "learning_rate": 2.751941291919106e-06,
      "loss": 0.9736,
      "step": 11580
    },
    {
      "epoch": 2.845568377117604,
      "grad_norm": 23.861560821533203,
      "learning_rate": 2.7092755354552437e-06,
      "loss": 1.006,
      "step": 11590
    },
    {
      "epoch": 2.848023569850233,
      "grad_norm": 17.17330551147461,
      "learning_rate": 2.6666097789913815e-06,
      "loss": 1.2057,
      "step": 11600
    },
    {
      "epoch": 2.8504787625828625,
      "grad_norm": 18.650781631469727,
      "learning_rate": 2.6239440225275196e-06,
      "loss": 1.0709,
      "step": 11610
    },
    {
      "epoch": 2.852933955315492,
      "grad_norm": 18.70904541015625,
      "learning_rate": 2.5812782660636574e-06,
      "loss": 1.1343,
      "step": 11620
    },
    {
      "epoch": 2.855389148048122,
      "grad_norm": 14.435473442077637,
      "learning_rate": 2.538612509599795e-06,
      "loss": 1.144,
      "step": 11630
    },
    {
      "epoch": 2.8578443407807512,
      "grad_norm": 17.29938316345215,
      "learning_rate": 2.4959467531359333e-06,
      "loss": 1.205,
      "step": 11640
    },
    {
      "epoch": 2.860299533513381,
      "grad_norm": 20.946060180664062,
      "learning_rate": 2.453280996672071e-06,
      "loss": 1.1592,
      "step": 11650
    },
    {
      "epoch": 2.8627547262460102,
      "grad_norm": 20.55058479309082,
      "learning_rate": 2.410615240208209e-06,
      "loss": 1.197,
      "step": 11660
    },
    {
      "epoch": 2.86520991897864,
      "grad_norm": 14.024412155151367,
      "learning_rate": 2.367949483744347e-06,
      "loss": 1.0469,
      "step": 11670
    },
    {
      "epoch": 2.8676651117112693,
      "grad_norm": 19.08846092224121,
      "learning_rate": 2.3252837272804847e-06,
      "loss": 1.1503,
      "step": 11680
    },
    {
      "epoch": 2.870120304443899,
      "grad_norm": 18.6326904296875,
      "learning_rate": 2.2826179708166225e-06,
      "loss": 1.1177,
      "step": 11690
    },
    {
      "epoch": 2.8725754971765283,
      "grad_norm": 16.469959259033203,
      "learning_rate": 2.2399522143527607e-06,
      "loss": 1.1391,
      "step": 11700
    },
    {
      "epoch": 2.875030689909158,
      "grad_norm": 22.18998908996582,
      "learning_rate": 2.1972864578888984e-06,
      "loss": 1.1136,
      "step": 11710
    },
    {
      "epoch": 2.8774858826417873,
      "grad_norm": 18.417068481445312,
      "learning_rate": 2.154620701425036e-06,
      "loss": 1.1645,
      "step": 11720
    },
    {
      "epoch": 2.879941075374417,
      "grad_norm": 18.556377410888672,
      "learning_rate": 2.1119549449611743e-06,
      "loss": 1.1646,
      "step": 11730
    },
    {
      "epoch": 2.8823962681070463,
      "grad_norm": 18.233428955078125,
      "learning_rate": 2.069289188497312e-06,
      "loss": 1.0335,
      "step": 11740
    },
    {
      "epoch": 2.884851460839676,
      "grad_norm": 15.307674407958984,
      "learning_rate": 2.02662343203345e-06,
      "loss": 1.1545,
      "step": 11750
    },
    {
      "epoch": 2.8873066535723053,
      "grad_norm": 19.857913970947266,
      "learning_rate": 1.983957675569588e-06,
      "loss": 1.1104,
      "step": 11760
    },
    {
      "epoch": 2.889761846304935,
      "grad_norm": 16.251449584960938,
      "learning_rate": 1.9412919191057258e-06,
      "loss": 1.0858,
      "step": 11770
    },
    {
      "epoch": 2.8922170390375643,
      "grad_norm": 29.896873474121094,
      "learning_rate": 1.8986261626418637e-06,
      "loss": 1.1117,
      "step": 11780
    },
    {
      "epoch": 2.894672231770194,
      "grad_norm": 17.60646629333496,
      "learning_rate": 1.8559604061780017e-06,
      "loss": 1.0452,
      "step": 11790
    },
    {
      "epoch": 2.8971274245028233,
      "grad_norm": 22.303741455078125,
      "learning_rate": 1.8132946497141394e-06,
      "loss": 1.2354,
      "step": 11800
    },
    {
      "epoch": 2.899582617235453,
      "grad_norm": 22.055253982543945,
      "learning_rate": 1.7706288932502774e-06,
      "loss": 1.1573,
      "step": 11810
    },
    {
      "epoch": 2.9020378099680824,
      "grad_norm": 31.771984100341797,
      "learning_rate": 1.7279631367864154e-06,
      "loss": 1.1725,
      "step": 11820
    },
    {
      "epoch": 2.904493002700712,
      "grad_norm": 24.27933692932129,
      "learning_rate": 1.6852973803225533e-06,
      "loss": 1.0467,
      "step": 11830
    },
    {
      "epoch": 2.9069481954333414,
      "grad_norm": 19.93353843688965,
      "learning_rate": 1.642631623858691e-06,
      "loss": 1.1526,
      "step": 11840
    },
    {
      "epoch": 2.909403388165971,
      "grad_norm": 14.768071174621582,
      "learning_rate": 1.599965867394829e-06,
      "loss": 1.1117,
      "step": 11850
    },
    {
      "epoch": 2.9118585808986004,
      "grad_norm": 15.571175575256348,
      "learning_rate": 1.557300110930967e-06,
      "loss": 1.1881,
      "step": 11860
    },
    {
      "epoch": 2.91431377363123,
      "grad_norm": 24.31025505065918,
      "learning_rate": 1.5146343544671048e-06,
      "loss": 1.1276,
      "step": 11870
    },
    {
      "epoch": 2.9167689663638594,
      "grad_norm": 18.24443244934082,
      "learning_rate": 1.4719685980032427e-06,
      "loss": 1.1786,
      "step": 11880
    },
    {
      "epoch": 2.919224159096489,
      "grad_norm": 13.978984832763672,
      "learning_rate": 1.4293028415393807e-06,
      "loss": 1.1726,
      "step": 11890
    },
    {
      "epoch": 2.9216793518291184,
      "grad_norm": 15.980413436889648,
      "learning_rate": 1.3866370850755184e-06,
      "loss": 1.1088,
      "step": 11900
    },
    {
      "epoch": 2.924134544561748,
      "grad_norm": 18.608009338378906,
      "learning_rate": 1.3439713286116564e-06,
      "loss": 1.0645,
      "step": 11910
    },
    {
      "epoch": 2.9265897372943774,
      "grad_norm": 27.054367065429688,
      "learning_rate": 1.3013055721477944e-06,
      "loss": 1.0717,
      "step": 11920
    },
    {
      "epoch": 2.929044930027007,
      "grad_norm": 19.696802139282227,
      "learning_rate": 1.2586398156839321e-06,
      "loss": 1.0122,
      "step": 11930
    },
    {
      "epoch": 2.931500122759637,
      "grad_norm": 15.72180461883545,
      "learning_rate": 1.21597405922007e-06,
      "loss": 1.1529,
      "step": 11940
    },
    {
      "epoch": 2.933955315492266,
      "grad_norm": 22.798778533935547,
      "learning_rate": 1.173308302756208e-06,
      "loss": 1.1843,
      "step": 11950
    },
    {
      "epoch": 2.9364105082248955,
      "grad_norm": 18.98419952392578,
      "learning_rate": 1.130642546292346e-06,
      "loss": 1.2881,
      "step": 11960
    },
    {
      "epoch": 2.938865700957525,
      "grad_norm": 15.096883773803711,
      "learning_rate": 1.0879767898284838e-06,
      "loss": 1.1159,
      "step": 11970
    },
    {
      "epoch": 2.941320893690155,
      "grad_norm": 19.649314880371094,
      "learning_rate": 1.0453110333646217e-06,
      "loss": 1.3321,
      "step": 11980
    },
    {
      "epoch": 2.943776086422784,
      "grad_norm": 21.520732879638672,
      "learning_rate": 1.0026452769007595e-06,
      "loss": 1.1835,
      "step": 11990
    },
    {
      "epoch": 2.9462312791554135,
      "grad_norm": 17.205488204956055,
      "learning_rate": 9.599795204368972e-07,
      "loss": 1.1733,
      "step": 12000
    },
    {
      "epoch": 2.9486864718880432,
      "grad_norm": 20.54789924621582,
      "learning_rate": 9.173137639730352e-07,
      "loss": 1.1152,
      "step": 12010
    },
    {
      "epoch": 2.951141664620673,
      "grad_norm": 17.4530086517334,
      "learning_rate": 8.746480075091731e-07,
      "loss": 0.9769,
      "step": 12020
    },
    {
      "epoch": 2.9535968573533022,
      "grad_norm": 11.548623085021973,
      "learning_rate": 8.31982251045311e-07,
      "loss": 0.907,
      "step": 12030
    },
    {
      "epoch": 2.9560520500859315,
      "grad_norm": 17.49899673461914,
      "learning_rate": 7.893164945814489e-07,
      "loss": 1.1923,
      "step": 12040
    },
    {
      "epoch": 2.9585072428185613,
      "grad_norm": 19.772464752197266,
      "learning_rate": 7.466507381175869e-07,
      "loss": 1.1415,
      "step": 12050
    },
    {
      "epoch": 2.960962435551191,
      "grad_norm": 18.027339935302734,
      "learning_rate": 7.039849816537248e-07,
      "loss": 0.9894,
      "step": 12060
    },
    {
      "epoch": 2.9634176282838203,
      "grad_norm": 19.61406898498535,
      "learning_rate": 6.613192251898626e-07,
      "loss": 1.1632,
      "step": 12070
    },
    {
      "epoch": 2.9658728210164496,
      "grad_norm": 17.973405838012695,
      "learning_rate": 6.186534687260006e-07,
      "loss": 1.1847,
      "step": 12080
    },
    {
      "epoch": 2.9683280137490793,
      "grad_norm": 18.279932022094727,
      "learning_rate": 5.759877122621385e-07,
      "loss": 1.1545,
      "step": 12090
    },
    {
      "epoch": 2.970783206481709,
      "grad_norm": 26.44321632385254,
      "learning_rate": 5.333219557982764e-07,
      "loss": 1.167,
      "step": 12100
    },
    {
      "epoch": 2.9732383992143383,
      "grad_norm": 16.395416259765625,
      "learning_rate": 4.906561993344142e-07,
      "loss": 0.9254,
      "step": 12110
    },
    {
      "epoch": 2.9756935919469676,
      "grad_norm": 15.247856140136719,
      "learning_rate": 4.479904428705521e-07,
      "loss": 1.1666,
      "step": 12120
    },
    {
      "epoch": 2.9781487846795973,
      "grad_norm": 18.057886123657227,
      "learning_rate": 4.0532468640669e-07,
      "loss": 1.128,
      "step": 12130
    },
    {
      "epoch": 2.980603977412227,
      "grad_norm": 18.003408432006836,
      "learning_rate": 3.626589299428279e-07,
      "loss": 1.0892,
      "step": 12140
    },
    {
      "epoch": 2.9830591701448563,
      "grad_norm": 20.804607391357422,
      "learning_rate": 3.199931734789658e-07,
      "loss": 1.2149,
      "step": 12150
    },
    {
      "epoch": 2.9855143628774856,
      "grad_norm": 12.838508605957031,
      "learning_rate": 2.773274170151037e-07,
      "loss": 1.0625,
      "step": 12160
    },
    {
      "epoch": 2.9879695556101153,
      "grad_norm": 19.425634384155273,
      "learning_rate": 2.3466166055124158e-07,
      "loss": 1.098,
      "step": 12170
    },
    {
      "epoch": 2.990424748342745,
      "grad_norm": 16.39198875427246,
      "learning_rate": 1.9199590408737949e-07,
      "loss": 1.1096,
      "step": 12180
    },
    {
      "epoch": 2.9928799410753744,
      "grad_norm": 16.134645462036133,
      "learning_rate": 1.4933014762351737e-07,
      "loss": 1.2368,
      "step": 12190
    },
    {
      "epoch": 2.995335133808004,
      "grad_norm": 18.71588897705078,
      "learning_rate": 1.0666439115965525e-07,
      "loss": 1.2236,
      "step": 12200
    },
    {
      "epoch": 2.9977903265406334,
      "grad_norm": 20.0461368560791,
      "learning_rate": 6.399863469579316e-08,
      "loss": 0.9912,
      "step": 12210
    },
    {
      "epoch": 3.0,
      "eval_exact": 56.506358965720544,
      "eval_f1": 61.22141012913486,
      "eval_hasAns_f1": 55.39504090135215,
      "eval_loss": 1.6421538591384888,
      "eval_runtime": 43.3757,
      "eval_samples_per_second": 273.725,
      "eval_steps_per_second": 4.288,
      "step": 12219
    }
  ],
  "logging_steps": 10,
  "max_steps": 12219,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4116483110251008.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
